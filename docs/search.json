[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "챗GPT SQL",
    "section": "",
    "text": "감사의 글\n이 책이 탄생할 수 있도록 도움을 주신 여러분께 깊은 감사의 마음을 표합니다.\n기술적인 부분에서 깊은 통찰력을 제공해주신 한국 R 사용자회 유충현 회장님, 챗GPT가 국내에 소개된 후 가장 먼저 챗GPT를 세밀하게 조명할 수 있도록 교육 기회를 주신 경기도의회 문승호, 이자형, 장한별 의원님, 광명시 박승원 시장님, 김종업 센터장님, 김포시 김규식 부시장님, 지방행정연구원 최인수, 전대욱, 김필, 주희진 박사님, 경기도청 AI빅데이터 산업과 이수재 과장님, 원금동, 최정환, 윤여찬 팀장님, 나이스 디앤알 박정우 대표님, 남영민 본부장님, 우민호 실장님, 서울교육청 조희연 교육감님, 양신호 원장님, 김선자 장학관님, 디플래닉스 김범진 전대표님, 장석호 대표님, 이용빈님, 경기도 경제과학진흥원 임문영 상임이사님, 건국대 미래지식교육원 이영범 원장님, 세종대 곽창규, 김현정 교수님께 감사드립니다.\n이 책의 공개와 출판이 가능했던 것은 한국 R 사용자회의 지원 덕분입니다. 행정사법인 광화문 최순영 대표님, 한채민 과장님, 법무법인 평안 김형주 변호사님, 법률사무소 하우림 정병운 변호사님, 홍성학 감사님, 김호성님, 김현철님, 형환희님, 명지대 박순만 교수님, Macao Polytechnic University 김송규 교수님, 한국텍학회 김강수님, 배달의 민족 이봉호님, 홍익대 이현진 교수님, 경상국립대 백원희 교수님께 진심으로 감사드립니다.\n한국 R 사용자회 활동에서 소프트웨어 카펜트리의 영향은 결코 무시할 수 없습니다. 소프트웨어 카펜트리를 설립한 Greg Wilson 박사님, 카펜트리 재단의 Kari Jordan 박사님, AsiaR 커뮤니티를 이끌고 계신 Janani Ravi 박사님, 서울 R 미트업에서 발표해주신 제빈 웨스트 교수님, 그리고 곽수영, 장연훈, 나성호, 안영찬, 박남호, 공병규, 김용우님께 감사의 말씀을 드립니다.\n지적 자극을 주시고 더 넓은 세상을 보게 해주신 국가교육위원회 이배용 위원장님, 정대화 상임위원님, 김수환 전문위원님, 삼정 KPMG 장지수 부대표님, 박문구 전무님, 가톨릭 의대 문건웅 교수님, 성균관대 최재성 교수님, 국무조정실 장명헌 사무관님, 제주대 안도현 교수님, 명지대 박순만 교수님, 엘릭스 이희정 전대표님, 서울대 고길곤 교수님, 한밭대 이원일 교수님에게 깊은 감사의 말씀을 전합니다.\n공공정책분야의 빅데이터 분석 활성화를 격려해주신 노웅래 의원님, 김병욱 의원님, 성남시의회 조정식 의원님, 환경보전협회 남광우 전 상근부회장님, 공공의창 최정묵 박사님, 조원씨앤아이 김대진 대표님, 그리고 이 책 출판에 관심이 많으신 오마이뉴스의 김지현 기자님, 나눔국민운동본부 위정희 이사님에게 감사의 인사를 전합니다.\n이 책이 출간되는데 있어 이들 모든 분들의 도움 없이는 어려웠을 것입니다. 그동안의 관심과 지원에 깊은 감사를 드리며, 이 책이 데이터 과학의 발전과 독자들에게 도움이 될 수 있기를 바라는 마음으로 마무리하겠습니다.",
    "crumbs": [
      "감사의 글"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  들어가며",
    "section": "",
    "text": "엑셀로 대표되는 스프레드시트를 사용하는 것은 많은 사람들에게 익숙한 경험이다. 엑셀은 데이터를 쉽게 조작하고 분석할 수 있는 기본적인 도구로 활용되지만, 데이터셋이 복잡해지면서 한계에 부딪힌다. 이때 스프레드시트 한계를 넘어서 다양한 선택지를 살펴보게 되고, 첫번째 선택지로 데이터베이스가 주저없이 선택된다.\n간단한 수치 연산에는 스프레드시트로 충분할 수 있지만, 복잡하고 큰 데이터셋을 빠르고 효율적으로 처리하려면 데이터베이스가 필요하다. SQL(Structured Query Language)은 데이터베이스를 관리하고 조작하는 데 있어 중요한 역할을 한다. 데이터베이스가 어떻게 작동하는지 이해하는 것은 단순한 데이터 조작 능력을 넘어서, 현재 사용되는 다양한 시스템들이 왜 이와 같은 방식으로 작동하는지, 왜 데이터를 특정한 방식으로 구조화하는지에 대한 깊은 통찰도 제공한다.\n이 책은 데이터베이스의 기본 원리와 SQL의 사용 방법으로 시작하지만, 데이터베이스 프로그래밍과 데이터 과학자가 알아야 할 다양한 데이터베이스 시스템을 아우르며, 챗GPT로 상징되는 생성형 AI SQL LLM에 대해서도 다룬다. 데이터베이스 기본 개념을 이해하고 SQL을 사용하는 방법을 익히면, 데이터베이스가 활용되는 다양한 시스템을 쉽게 이해할 수 있다. 이러한 시스템들을 이해함으로써, 데이터베이스 기본 개념과 SQL에 대해 더 깊이 이해할 수 있다. 추가적으로, 소프트웨어 카펜트리 콘텐츠에 기반한 “챗GPT 유닉스 쉘”에 이어 “챗GPT SQL”(이광춘 와/과 신종화 2023)을 출판함으로써, 2023년에 큰 인기를 끈 챗GPT를 SQL과 어떻게 접목되는지 살펴보는 것도 큰 의미가 있다고 할 수 있다.\n이 책에서 제시하는 데이터베이스와 SQL의 중요성, 기본 개념, 언어, 다양한 시스템, 활용 사례, 그리고 최근 주목받는 챗GPT와 같은 생성형 AI에 대한 내용은 방대하다. 각자 책을 읽는 나만의 방식이 있겠지만, 책을 효과적으로 읽는 방법을 제시하면 다음과 같다.\n\nSQLite3 설치 및 실습: SQLite3를 다운로드하고 설치한 후, 소프트웨어 카펜트리의 SQL 교과목을 따라가며 실습을 시작한다. 이는 데이터베이스 기본 개념을 실제로 적용해보는 첫걸음이다.\n데이터베이스 기본개념 공부: 책에 정리된 데이터베이스 기본 개념을 참고하며 학습을 병행한다. 이 과정에서 SQL 실습을 병행하여 이론과 실습을 연결시킨다.\n현장 데이터베이스 시스템 실습: 시카고 범죄 데이터를 사용해 분석형 데이터베이스 DuckDB, DVD 렌털 데이터를 활용해 오픈소스 PostgreSQL을 설치하고 실습함으로써 실제 현장에서 사용되는 데이터베이스 시스템 작동 원리를 실습을 통해 이해한다.\n생성형 AI 챗GPT와의 연계: 마지막으로, OpenAI API와 오픈소스 SQL LLM을 활용하여 생성형 AI 챗GPT를 경험함으로써, 데이터베이스 기술과 AI의 결합을 통해 현재 진행되고 있는 기술의 미래를 탐색한다.\n\n\n\n\n\n이광춘, 와/과 신종화. 2023. 챗GPT 유닉스쉘. \"한국 R 사용자회\".",
    "crumbs": [
      "들어가며",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>들어가며</span>"
    ]
  },
  {
    "objectID": "theory.html",
    "href": "theory.html",
    "title": "2  기본 개념",
    "section": "",
    "text": "3 데이터 통합\n원천데이터가 서로 다른 형태로 다양하게 존재하는 상황에서 데이터를 통합한다는 것은 시스템을 맞추는 것을 넘어 개념적인 데이터 모델로 정립하여야 하고 관련하여 파생된느 다양한 문제를 조화롭게 해결하는 것으로 정의할 수 있다.\n먼저 데이터를 한곳에 모은다고 하면 어떤 데이터를 모을 것인지 정의하고 클라우드 서비스를 사용한다고 하면 AWS Redshift 혹은 S3를 상정하고 혹시나 포함될 수 있는 개인정보도 사전에 식별하여 마스킹 등을 통해 익명화시켜야 되며 데이터 혈통(Data Lineage)도 구축하여 투명성과 가시성도 확보한다.\n지금은 빅데이터 시대라 데이터가 커지게 되면 테이블에 담을 수 없는 상황이 온다. 이런 문제를 해결하기 위해 도입된 개념이 분할(Partition) 이다. 테이블 크기가 예를 들어 100GB 혹은 1TB가 되면 인덱스도 커져서 메모리에 적재가 되지 않아 쿼리 속도와 업데이트 속도가 현격히 늦어지게 된다. 이런 경우 테이블을 더 작은 단위로 쪼개는데 이를 분할(Partition)이라고 한다.\n테이블을 분할하게 될 경우 개념과 논리 데이터 모형은 동일하지만 물리 데이터 모형이 분할에 영향을 받게 된다.\n테이블을 분할하는 방법은 수평 분할 (Horizontal Partitioning)과 수직 분할 (Vertical Partitioning) 두가지 방법이 있다.\n데이터베이스 샤딩(databae sharding)은 테이블이 동일한 데이터베이스에 있지 않고 다른 기계에 있다는 점에서 차이가 난다. 3\n데이터베이스 설계(Database Design)는 데이터를 논리적으로 저장하는 방식으로 데이터베이스 모델(Database Model)을 사용한다. 데이터베이스 모델(Database Model)은 데이터베이스 구조에 대한 최상위 사양서의 역할을 한다. 일반적으로 관계형 데이터베이스 모형(Relational Database Model)을 사용하지만 NoSQL, 객체지향 DB 모형, 네트워크 DB 모델 등이 있다. 데이터베이스의 청사진으로 스키마를 사용해서 테이블, 필드, 관계, 인덱스, 뷰로 구성하여 작성한다.\n즉, 데이터를 체계적으로 구조화하는 논리모형을 먼저 구상하고 나서 사양서를 작성하고 실제 데이터베이스 설계로 들어간다.",
    "crumbs": [
      "들어가며",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>기본 개념</span>"
    ]
  },
  {
    "objectID": "theory.html#oltp-vs-olap",
    "href": "theory.html#oltp-vs-olap",
    "title": "2  기본 개념",
    "section": "2.1 OLTP vs. OLAP",
    "text": "2.1 OLTP vs. OLAP\nOLTP (OnLine Transaction Processing)는 데이터 자체 처리에 중점을 둔 개념인데 반해, OLAP (OnLine Analytical Processing)은 저장된 데이터를 분석하여 꿰뚤어 볼 수 있는 능력(Insight)를 도출하는데 중심을 두고 있다.\nOLAP의 대표적인 예로 편의점 판매시점 정보관리(Point-Of-Sales, POS) 기계를 들어보자. 편의점에서 물건을 구매한 경우 다음과 같은 거래가 발생된다.\n\n고객 카드에서 현금 10,000원 인출\n편의점 통장에 10,000 지급\n명세표 출력\n\n상기 3건의 작업 프로세스가 하나의 트랜잭션(transaction) 묶어 모두 성공적으로 처리가 되어야 편의점 물건구매가 완료되도록 개발한다.\n반면에 OLAP은 데이터를 체계적으로 저장하여 데이터에 기반한 의사결정지원을 할 수 있도록 주안점을 두고 있다.\n\n\n\n\n\n\n\n\nOLAP과 OLTP 비교\n\n\n구분\nOLAP\nOLTP\n\n\n\n\n목적\n트랜젝션 처리\n데이터 분석과 보고서 작성, 대쉬보드 시각화\n\n\n설계\n앱 기능 지향\n비즈니스 주제 지향\n\n\n데이터\n운영계, 실시간 최신 데이터\n정보계, 통합/이력 데이터\n\n\n크기\n기가 바이트, 스냅샷\n테라데이터, 아카이브\n\n\nSQL 쿼리\n단순 트랜잭션, 빈번한 갱신\n복잡한 집계 쿼리\n\n\n사용자\n아주 많음\n분석가 포함 일부\n\n\n\n출처: https://www.guru99.com/oltp-vs-olap.html\n\n\n\n\n\n\n\n\n따라서, OLTP는 운영 데이터베이스(Operational Database)로 적합하여 쓰기 업무(Write-intensive)가 많은 경우 빠르고 안전하게 레코드를 삽입(insert)하는데 특화된 반면, OLAP은 데이터 창고(Data Warehouse) 업무에 적합한데 다양한 분석업무를 수행할 때 쿼리 작업을 속도감있게 진행할 수 있어 읽기 업무(Read-intensive)에 특화되어 있다.",
    "crumbs": [
      "들어가며",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>기본 개념</span>"
    ]
  },
  {
    "objectID": "theory.html#database-warehouse-lake",
    "href": "theory.html#database-warehouse-lake",
    "title": "2  기본 개념",
    "section": "2.2 DW, Data Lake 1",
    "text": "2.2 DW, Data Lake 1\n전통적인 데이터베이스(Database)는 관계형 데이터베이스를 통해서 실시간 정형데이터를 빠르고 신뢰성있게 처리하는데 운영계를 지탱하는 주된 쓰임새가 있으며, 데이터 창고(Data Warehouse)는 이력 데이터를 통합하여 꿰뚤어 볼 수 있는 능력(Insight)을 제공함은 물론 보고서와 전체적인 현황을 대쉬보드를 통해 제공하는데 큰 의미가 있다. 데이터 호수(Data Lake)는 정형, 반정형, 비정형 데이터를 모두 저장하고 관리한다는 측면에서 유연성과 확장성을 내재하고 있으며 빅데이터를 분석하여 OLAP에서 추구하는 바를 한단계 더 넓혔다는 점에서 의의를 둘 수 있다.\n\n데이터 호수(Data Lake)는 특정한 구조가 없기 때문에 접근하기 용이하고 쉽게 수정하기도 용이한 반면에 데이터 창고(Data Warehouse)는 상대적으로 유연성이 떨어진다. 뿐만 아니라 데이터 과학자는 아직 결정되지 않는 비즈니스 활용 사례를 데이터 문제로 바꿔 모형을 만들고 시각화를 하는데 데이터 호수를 적합한 데 반해 비즈니스 현업전문가는 일단 전처리가 된 데이터를 데이터 창고에 넣어 특정 목적을 달성하는데 활용된다는 점에서 비교가 된다.\n\n\n\n\n\n\n\n\n데이터 호수와 창고(DW) 비교\n\n\n구분\n데이터 호수\n데이터 창고\n\n\n\n\n자료구조\n원천 데이터 (Raw Data)\n전처리 된 데이터\n\n\n데이터 활용 목적\n미결정 상태\n현재 사용 중\n\n\n사용자\n데이터 과학자\n비즈니스 현업전문가\n\n\n접근성\n접근성 높고 신속한 업데이트\n변경하기 쉽지 않고 비용도 많이 소요됨.\n\n\n\n출처: https://www.qlik.com/us/data-lake/data-lake-vs-data-warehouse\n\n\n\n\n\n\n\n\n클라우드 서비스도 데이터 창고(Data Warehouse)를 기능으로 제공하고 있는데 상품명은 다음과 같다.\n\nAWS: 아마존 Redshift\nMS Azure: Azure SQL Data Warehouse\n구글: 구글 빅쿼리(Big Query)\n\n데이터 호수도 클라우드 서비스에서 제공된다. Object Storage와 함께 하둡/스파크 빅데이터 소프트웨어와 함께 검토된다.\n\nAWS: AWS S3\nMS Azure: Blob Storage / Azure Data Lake Storage\n구글: Cloud Storage\n네이버: Object Storage",
    "crumbs": [
      "들어가며",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>기본 개념</span>"
    ]
  },
  {
    "objectID": "theory.html#etl-vs-elt",
    "href": "theory.html#etl-vs-elt",
    "title": "2  기본 개념",
    "section": "2.3 ETL과 ELT 2",
    "text": "2.3 ETL과 ELT 2\nETL은 추출, 변환, 적재(Extract, Transform, Load)의 약자로 동일 기종 혹은 이기종의 원천데이터로부터 데이터 웨어하우스에서 쌓는 과정을 뜻하는데 변환(Transform) 과정이 무척 많은 노력이 투여된다. 반면에 ELT는 데이터를 먼저 적재한 후에 필요에 따라 변환과정을 거쳐 후속 작업에 사용한다. 데이터 호수 ELT 프로세스가 매력적으로 보이지만 데이터 카탈로그가 잘 관리되지 않는다면 데이터 늪(Data Swamp)가 될 수 있다.\n\n데이터 호수를 잘 관리하지 않는다면 데이터 늪에 빠질 수 있는데 메타데이터를 잘 관리하고 거버넌스를 확립해야 되고 비정형데이터도 많이 다루기 때문에 데이터 전문가 과학자를 확보하여 효율성을 높인다.",
    "crumbs": [
      "들어가며",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>기본 개념</span>"
    ]
  },
  {
    "objectID": "theory.html#db-design-in-practice",
    "href": "theory.html#db-design-in-practice",
    "title": "2  기본 개념",
    "section": "5.1 데이터 모형(Data Modeling)",
    "text": "5.1 데이터 모형(Data Modeling)\n데이터를 저장하는 방식에 대해 데이터 모형(Data Model)을 제작하는 단계는 다음과 같다.\n\n개념 데이터 모형(Concept Data Model): Entity, Relationship, Attributes\n\nERD(Entity Relational Diagram), UML 다이어그램\n\n논리 데이터 모형(Logical Data Model): 테이블, 칼럼, 관계\n\n데이터베이스 스키마와 모형: 관계형 모형, 스타 스키마(Star Sceman)\n\n물리 데이터 모형(Physical Data Model): 물리적 저장장치\n\n백업 시스템, 파티션, CPU, 저장공간 등",
    "crumbs": [
      "들어가며",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>기본 개념</span>"
    ]
  },
  {
    "objectID": "theory.html#data-normalization",
    "href": "theory.html#data-normalization",
    "title": "2  기본 개념",
    "section": "5.2 데이터 정규화 4 5",
    "text": "5.2 데이터 정규화 4 5\n관계형 데이터베이스의 설계에서 정규화(normalization)는 중복을 최소화하도록 데이터를 구조화하는 프로세스를 지칭한다. 관계형 모델의 발견자인 에드거 F. 커드는 1970년에 제 1 정규형(1NF)로 알려진 정규화의 개념을 도입하였고, 에드거 F. 커드는 이어서 제 2 정규형(2NF)과 제 3 정규형(3NF)을 1971년에 정의하였으며, 1974년에는 레이먼드 F. 보이스와 함께 보이스-코드 정규화(BCNF)를 정의하였다. 통상 관계형 데이터베이스 테이블이 제 3 정규(3NF)형이 되었으면 정규화(Normalization) 되었다고 한다. 따라서, 데이터 정규형(Normal Forms)은 1NF 부터 3NF까지가 많이 회자된다.\n\n제 1 정규형 (1 NF)\n\n각 레코드는 유일무이(unique)해야 한다. 즉, 중복(duplication)이 없어야 함.\n각 셀은 하나의 값만 가져야 함.\n\n제 2 정규형 (2 NF)\n\n제 1 정규형을 만족한다.\n기본키(primary key)가 한 칼럼이면 자동으로 제 2 정규형을 만족한다.\n기본키가 아닌 모든 속성이 기본키에 완전 함수 종속되어야 한다.\n\n제 3 정규형 (3 NF)\n\n제 2 정규형을 만족한다.\n기본키가 아닌 모든 속성이 기본키에 이행적 함수 종속이 되지 않아야 한다.\n즉, 이행(移行)적 함수 종속 (Transitive Functional Dependency)이 없어야 한다.\n함수 종속 사례로 X, Y, Z 에 대해 X → Y 이고 Y → Z 이면 X → Z 가 성립한다. 이를 Z 가 X 에 이행적으로 함수 종속되었다고 함.",
    "crumbs": [
      "들어가며",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>기본 개념</span>"
    ]
  },
  {
    "objectID": "theory.html#r-db-what-it-is",
    "href": "theory.html#r-db-what-it-is",
    "title": "2  기본 개념",
    "section": "5.3 데이터베이스가 뭔가요?",
    "text": "5.3 데이터베이스가 뭔가요?\n데이터베이스(database)는 데이터를 저장하기 위한 목적으로 조직된 파일이다. 대부분의 데이터베이스는 키(key)와 값(value)를 매핑한다는 의미에서 딕셔너리처럼 조직되었다. 가장 큰 차이점은 데이터베이스는 디스크(혹은 다른 영구 저장소)에 위치하고 있어서, 프로그램 종료 후에도 정보가 계속 저장된다. 데이터베이스가 영구 저장소에 저장되어서, 컴퓨터 주기억장치(memory) 크기에 제한받는 딕셔너리보다 훨씬 더 많은 정보를 저장할 수 있다.\n딕셔너리처럼, 데이터베이스 소프트웨어는 엄청난 양의 데이터 조차도 매우 빠르게 삽입하고 접근하도록 설계되었다. 컴퓨터가 특정 항목으로 빠르게 찾아갈 수 있도록 데이터베이스에 인덱스(indexes)를 추가한다. 데이터베이스 소프트웨어는 인덱스를 구축하여 성능을 보장한다.\n다양한 목적에 맞춰 서로 다른 많은 데이터베이스 시스템이 개발되어 사용되고 있다. Oracle, MySQL, Microsoft SQL Server, PostgreSQL, SQLite이 여기에 포함된다. 이 책에서는 SQLite를 집중해서 살펴볼 것이다. 왜냐하면 매우 일반적인 데이터베이스이며 파이썬에 이미 내장되어 있기 때문이다. 응용프로그램 내부에서 데이터베이스 기능을 제공하도록 SQLite가 다른 응용프로그램 내부에 내장(embedded)되도록 설계되었다. 예를 들어, 다른 많은 소프트웨어 제품이 그렇듯이, 파이어폭스 브라우져도 SQLite를 사용한다.\n\nhttp://sqlite.org/\n\n이번 장에서 기술하는 트위터 스파이더링 응용프로그램처럼 정보과학(Informatics)에서 마주치는 몇몇 데이터 조작 문제에 SQLite가 적합하다.",
    "crumbs": [
      "들어가며",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>기본 개념</span>"
    ]
  },
  {
    "objectID": "theory.html#r-db-concept",
    "href": "theory.html#r-db-concept",
    "title": "2  기본 개념",
    "section": "5.4 데이터베이스 개념",
    "text": "5.4 데이터베이스 개념\n처음 데이터베이스를 볼때 드는 생각은 마치 엑셀같은 다중 시트를 지닌 스프레드쉬트(spreadsheet)같다는 것이다. 데이터베이스에서 주요 데이터 구조물은 테이블(tables), 행(rows), and 열(columns)이 된다.\n\n\n\n데이터베이스 개념\n\n\n관계형 데이터베이스의 기술적인 면을 설명하면 테이블, 행, 열의 개념은 관계(relation), 튜플(tuple), 속성(attribute) 각각 형식적으로 매칭된다. 이번 장에서는 조금 덜 형식 용어를 사용한다.",
    "crumbs": [
      "들어가며",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>기본 개념</span>"
    ]
  },
  {
    "objectID": "theory.html#r-db-create-table",
    "href": "theory.html#r-db-create-table",
    "title": "2  기본 개념",
    "section": "5.5 데이터베이스 테이블 생성",
    "text": "5.5 데이터베이스 테이블 생성\n데이터베이스는 R 리스트 혹은 딕셔너리보다 좀더 명확히 정의된 구조를 요구한다. 6\n데이터베이스에 테이블(table)을 생성할 때, 열(column)의 명칭과 각 열(column)에 저장하는 테이터 형식을 사전에 정의해야 한다. 데이터베이스 소프트웨어가 각 열의 데이터 형식을 인식하게 되면, 데이터 형식에 따라 데이터를 저장하고 찾아오는 방법을 가장 효율적인 방식을 선택할 수 있다.\n다음 url에서 SQLite에서 지원되는 다양한 데이터 형식을 살펴볼 수 있다.\n\nhttp://www.sqlite.org/datatypes.html\n\n처음에는 데이터 구조를 사전에 정의하는 것이 불편하게 보이지만, 대용량의 데이터가 데이터베이스에 포함되더라도 데이터의 빠른 접근을 보장하는 잇점이 있다.\n데이터베이스 파일과 데이터베이스에 두개의 열을 가진 Tracks 이름의 테이블을 생성하는 코드는 다음과 같다.\n\nlibrary(RSQLite)\n\nmusic_db  &lt;- \"data/music.sqlite\"\nconn &lt;- dbConnect(drv = SQLite(), dbname= music_db)\n\ndbSendQuery(conn, \"INSERT INTO Tracks (title, plays) VALUES ( ?, ? )\", c('Thunderstruck', 20))\ndbSendQuery(conn, \"INSERT INTO Tracks (title, plays) VALUES ( ?, ? )\", c('My Way', 15))\n\ndbDisconnect(conn)\n\n연결 (connect) 연산은 현재 디렉토리 data/music.sqlite3 파일에 저장된 데이터베이스에 “연결(connection)”한다. 파일이 존재하지 않으면, 자동 생성된다. “연결(connection)”이라고 부르는 이유는 때때로 데이터베이스가 응용프로그램이 실행되는 서버로부터 분리된 “데이터베이스 서버(database server)”에 저장되기 때문이다. 지금 간단한 예제 파일의 경우에 데이터베이스가 로컬 파일 형태로 R 코드 마찬가지로 동일한 디렉토리에 있다.\n파일을 다루는 파일 핸들(file handle)처럼 데이터베이스에 저장된 파일에 연산을 수행하기 위해서 커서(cursor)를 사용한다. cursor()를 호출하는 것은 개념적으로 텍스트 파일을 다룰 때 readLines()을 호출하는 것과 개념적으로 매우 유사하다.\n\n\n\n데이터베이스 커서\n\n\n커서가 생성되면, dbGetQuery() 함수를 사용하여 데이터베이스 콘텐츠에 명령어 실행을 할 수 있다.\n데이터베이스 명령어는 특별한 언어로 표현된다. 단일 데이터베이스 언어를 학습하도록 서로 다른 많은 데이터베이스 업체 사이에서 표준화되었다.\n데이터베이스 언어를 SQL(Structured Query Language 구조적 질의 언어)로 부른다.\n\nhttp://en.wikipedia.org/wiki/SQL\n\n상기 예제에서, 데이터베이스에 두개의 SQL 명령어를 실행했다. 관습적으로 데이터베이스 키워드는 대문자로 표기한다. 테이블명이나 열의 명칭처럼 사용자가 추가한 명령어 부분은 소문자로 표기한다.\n첫 SQL 명령어는 만약 존재한다면 데이터베이스에서 Tracks 테이블을 삭제한다. 동일한 프로그램을 실행해서 오류 없이 반복적으로 Tracks 테이블을 생성하도록하는 패턴이다. DROP TABLE 명령어는 데이터베이스 테이블 및 테이블 콘텐츠 전부를 삭제하니 주의한다. (즉, “실행취소(undo)”가 없다.)\n`dbGetQuery(conn, 'DROP TABLE IF EXISTS Tracks ') `\n두번째 명령어는 title 문자형 열과 plays 정수형 열을 가진 Tracks으로 명명된 테이블을 생성한다.\n`dbGetQuery(conn, 'CREATE TABLE Tracks (title TEXT, plays INTEGER)')`\n이제 Tracks으로 명명된 테이블을 생성했으니, SQL INSERT 연산을 통해 테이블에 데이터를 넣을 수 있다. 다시 한번, 데이터베이스에 연결하여 커서(cursor)를 얻어 작업을 시작한다. 그리고 나서 커서를 사용해서 SQL 명령어를 수행한다.\nSQL INSERT 명령어는 어느 테이블을 사용할지 특정한다. 그리고 나서 (title, plays) 포함할 필드 목록과 테이블 새로운 행에 저장될 VALUES 나열해서 신규 행을 정의를 마친다. 실제 값이 execute() 호출의 두번째 매개변수로 튜플 ('My Way', 15) 로 넘겨는 것을 표기하기 위해서 값을 물음표 (?, ?)로 명기한다.\n\nlibrary(RSQLite)\n\nmusic_db  &lt;- \"data/music.sqlite\"\nconn &lt;- dbConnect(drv = SQLite(), dbname= music_db)\n\ndbSendQuery(conn, \"INSERT INTO Tracks (title, plays) VALUES ( ?, ? )\", \n            c('Thunderstruck', 20))\ndbSendQuery(conn, \"INSERT INTO Tracks (title, plays) VALUES ( ?, ? )\", \n            c('My Way', 15))\n\nprint('Tracks:')\n\ndbGetQuery(conn, 'SELECT title, plays FROM Tracks')\n\ndbSendQuery(conn, \"DELETE FROM Tracks WHERE plays &lt; 100\")\n\ndbDisconnect(conn)\n\n먼저 테이블에 두개 열을 삽입(INSERT)하여 데이터를 데이터베이스에 저장되도록 했다. 그리고 나서, SELECT 명령어를 사용하여 테이블에 방금 전에 삽입한 행을 불러왔다. SELECT 명령어에서 데이터를 어느 열(title, plays)에서, 어느 테이블Tracks에서 가져올지 명세한다. 프로그램 실행결과는 다음과 같다.\n\n&gt; dbGetQuery(conn, 'SELECT title, plays FROM Tracks')\n          title plays\n1 Thunderstruck    20\n2        My Way    15\n\n프로그램 마지막에 SQL 명령어를 실행 사용해서 방금전에 생성한 행을 모두 삭제(DELETE)했기 때문에 프로그램을 반복해서 실행할 수 있다. 삭제(DELETE) 명령어는 WHERE 문을 사용하여 선택 조건을 표현할 수 있다. 따라서 명령문에 조건을 충족하는 행에만 명령문을 적용한다. 이번 예제에서 기준이 모든 행에 적용되어 테이블에 아무 것도 없게 된다. 따라서 프로그램을 반복적으로 실행할 수 있다. 삭제(DELETE)를 실행한 후에 데이터베이스에서 데이터를 완전히 제거했다.",
    "crumbs": [
      "들어가며",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>기본 개념</span>"
    ]
  },
  {
    "objectID": "theory.html#r-db-sql",
    "href": "theory.html#r-db-sql",
    "title": "2  기본 개념",
    "section": "5.6 SQL 요약",
    "text": "5.6 SQL 요약\n지금까지, R 예제를 통해서 SQL(Structured Query Language)을 사용했고, SQL 명령어에 대한 기본을 다루었다. 이번 장에서는 SQL 언어를 보고 SQL 구문 개요를 살펴본다.\n대단히 많은 데이터베이스 업체가 존재하기 때문에 호환성의 문제로 SQL(Structured Query Language)이 표준화되었다. 그래서, 여러 업체가 개발한 데이터베이스 시스템 사이에 호환하는 방식으로 커뮤니케이션 가능하다.\n관계형 데이터베이스는 테이블, 행과 열로 구성된다. 열(column)은 일반적으로 텍스트, 숫자, 혹은 날짜 자료형을 갖는다. 테이블을 생성할 때, 열의 명칭과 자료형을 지정한다.\nCREATE TABLE Tracks (title TEXT, plays INTEGER)\n테이블에 행을 삽입하기 위해서 SQL INSERT 명령어를 사용한다.\nINSERT INTO Tracks (title, plays) VALUES ('My Way', 15)\nINSERT 문장은 테이블 이름을 명기한다. 그리고 나서 새로운 행에 놓고자 하는 열/필드 리스트를 명시한다. 그리고 나서 키워드 VALUES와 각 필드 별로 해당하는 값을 넣는다.\nSQL SELECT 명령어는 데이터베이스에서 행과 열을 가져오기 위해 사용된다. SELECT 명령문은 가져오고자 하는 행과 WHERE절을 사용하여 어느 행을 가져올지 지정한다. 선택 사항으로 ORDER BY 절을 이용하여 반환되는 행을 정렬할 수도 있다.\nSELECT * FROM Tracks WHERE title = 'My Way'\n* 을 사용하여 WHERE 절에 매칭되는 각 행의 모든 열을 데이터베이스에서 가져온다.\n주목할 점은 R과 달리 SQL WHERE 절은 등식을 시험하기 위해서 두개의 등치 기호 대신에 단일 등치 기호를 사용한다. WHERE에서 인정되는 다른 논리 연산자는 &lt;,&gt;,&lt;=,&gt;=,!= 이고, 논리 표현식을 생성하는데 AND, OR, 괄호를 사용한다.\n다음과 같이 반환되는 행이 필드값 중 하나에 따라 정렬할 수도 있다.\nSELECT title,plays FROM Tracks ORDER BY title\n행을 제거하기 위해서, SQL DELETE 문장에 WHERE 절이 필요하다. WHERE 절이 어느 행을 삭제할지 결정한다.\nSELECT title,plays FROM Tracks ORDER BY title\n다음과 같이 SQL UPDATE 문장을 사용해서 테이블에 하나 이상의 행 내에 있는 하나 이상의 열을 갱신(UPDATE)할 수 있다.\nUPDATE Tracks SET plays = 16 WHERE title = 'My Way'\nUPDATE 문장은 먼저 테이블을 명시한다. 그리고 나서, SET 키워드 다음에 변경할 필드 리스트 와 값을 명시한다. 그리고 선택사항으로 갱신될 행을 WHERE절에 지정한다. 단일 UPDATE 문장은 WHERE절에서 매칭되는 모든 행을 갱신한다. 혹은 만약 WHERE절이 지정되지 않으면,테이블 모든 행에 대해서 갱신(UPDATE)을 한다.\n네가지 기본 SQL 명령문(INSERT, SELECT, UPDATE, DELETE)은 데이터를 생성하고 유지 관리하는데 필요한 기본적인 4가지 작업을 가능케 한다.",
    "crumbs": [
      "들어가며",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>기본 개념</span>"
    ]
  },
  {
    "objectID": "setup.html#sqlite-설치",
    "href": "setup.html#sqlite-설치",
    "title": "3  실습환경 설정",
    "section": "3.1 SQLite 설치",
    "text": "3.1 SQLite 설치\n명령-라인(command-line)을 통해 디렉토리를 이동하고 명령문을 실행하는 방법을 알고 있어야만 이후 학습내용을 빠르게 습득할 수 있다. 명령-라인, 콘솔, 쉘 등 주제가 낯설다면 챗GPT 유닉스 쉘(Unix Shell) 학습 자료를 확인해보길 권한다.\n데이터베이스와 SQL을 본격적으로 학습하기 전에, SQLite3를 설치해야 한다. SQLite3를 설치하는 방법은 운영 체제에 따라 차이가 난다.\n\n3.1.1 윈도우즈 SQLite3 설치\n\nSQLite 다운로드: SQLite 공식 웹사이트(https://www.sqlite.org/download.html)에서 Windows용 바이너리 파일을 다운로드한다.\n압축 해제: 다운로드한 파일을 압축 해제한다.\n시스템 환경 변수 설정: 압축 해제한 SQLite 실행 파일의 경로를 시스템 환경 변수에 추가하여 명령 프롬프트에서 어디서나 SQLite를 실행할 수 있도록 설정한다.\n\n\n\n3.1.2 맥OS SQLite3 설치\n맥OS에는 기본적으로 SQLite3가 설치되어 있지만, 최신 버전을 사용하고 싶다면, 다음 방법을 실행한다.\n\nHomebrew 설치: Homebrew가 설치되어 있지 않다면, 터미널에서 /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\" 명령을 실행하여 설치한다.\nSQLite 설치: 터미널에서 brew install sqlite3 명령을 실행하여 SQLite를 설치한다.\n\n\n\n3.1.3 리눅스 SQLite3 설치\n리눅스 배포판 대부분에는 SQLite3가 이미 설치되어 있다. 만약 설치되어 있지 않거나 최신 버전을 설치하고 싶다면, 다음 방법을 사용하여 설치한다.\n\n패키지 관리자 설치: 리눅스 배포판 대부분에는 패키지 관지라를 제공하기 때문에, 우분투(Ubuntu)와 같은 데비안(Debian) 기반 시스템에서는 sudo apt-get install sqlite3 명령을 사용하고, 페도라(Fedora)와 같은 RPM 기반 시스템에서는 sudo yum install sqlite 명령으로 설치한다.\n\n\n\n3.1.4 설치 확인\n설치가 완료되면, 명령 프롬프트나 터미널에서 sqlite3 --version 명령을 실행하여 설치된 SQLite의 버전을 확인할 수 있다.\n$ sqlite3 --version\n3.40.1 2022-12-28 14:03:47 df5c253c0b3dd24916e4ec7cf77d3db5294cc9fd45ae7b9c5e82ad8197f38a24\n윈도우 환경에서 SQLite 다운로드한 후에, 압축을 풀면 몇개 .exe 파일이 존재하는데 설치가 완료되었다. 뭔가 복잡하고 대단한 것을 기대했을 수도 있지만 이러한 단순함이 SQLite의 장점이다.\n$ ls\ngen-survey-database.sql  sqlite3.exe           survey.db\nsqldiff.exe              sqlite3_analyzer.exe\n\nsqlite3.exe: sqlite 실행파일\ngen-survey-database.sql: survey.db sqlite 데이터베이스를 생성시키는데 사용되는 스크립트\nsurvey.db: sqlite3.exe 명령어를 실행해서 gen-survey-database.sql 스크립트를 통해 생성된 데이터베이스",
    "crumbs": [
      "들어가며",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>실습환경 설정</span>"
    ]
  },
  {
    "objectID": "setup.html#데이터베이스-설치",
    "href": "setup.html#데이터베이스-설치",
    "title": "3  실습환경 설정",
    "section": "3.2 데이터베이스 설치",
    "text": "3.2 데이터베이스 설치\nSQLite3를 설치했다면, 이제 데이터베이스(database)를 설치해야 한다. 데이터베이스는 테이블(table)과 뷰(view) 등의 객체가 포함된 파일이다. SQLite3는 데이터베이스를 생성할 때, 확장자를 지정하지 않는다. 데이터베이스 파일의 이름만 지정해도 되지만, 일반적으로 .db, .sqlite, .sqlite3 등 확장자를 사용하여 데이터베이스 파일임을 명확히 하는 것을 권장한다.\n\n3.2.1 survey.db 생성\n소프트웨어 카펜트리 “데이터베이스와 SQL” 학습 자료에서 사용할 데이터베이스 파일은 웹사이트(https://swcarpentry.github.io/sql-novice-survey/index.html)에서 다음과 같이 로컬 컴퓨터에 다운로드 받아 넣어두면 된다. 데이터베이스 파일 이름은 survey.db로 되어 있다.\n\n\n\nsurvey.db 다운로드\n\n\n또 다른 방법은 명령 프롬프트나 터미널에서 다음과 같이 직접 데이터베이스 파일을 생성하는 것이다.\n\n명령 라인 터미널 윈도우를 연다.\n다음과 같이 타이핑한다.\n\n$ mkdir ~/swc/sql\n\n생성한 디렉토리로 현재 작업 디렉토리를 변경한다.\n\n$ cd ~/swc/sql\n\nGitHub에서 “gen-survey-database.sql” 파일을 다운로드 한다.\n\n\n~/swc/sql 디렉토리로 이동한 후에 그 디렉토리에서 github 사이트 (https://github.com/swcarpentry/bc/blob/master/novice/sql/gen-survey-database.sql) 에 위치한 SQL 파일(“gen-survey-database.sql”)을 다운로드한다.\n파일이 GitHub 저장소 내에 위치하고 있어서, 전체 Git 저장소(git repo)를 복제(cloning)하지 않고 단일 파일만 로컬로 가져온다.\n이 목적을 달성하기 위해서, HTTP, HTTPS, FTP 프로토콜을 지원하는 명령-라인 웹크롤러(web-crawler) 소프트웨어 GNU Wget 혹은, 다양한 프로토콜을 사용하여 데이터를 전송하는데 사용되는 라이브러리이며 명령-라인 도구인 cURL을 사용한다. 두가지 도구 모두 크로스 플랫폼(cross platform)으로 다양한 운영체제를 지원한다.\n\nWget 혹은 cURL을 로컬에 설치한 후에, 터미널에서 다음 명령어를 실행한다.\n\n만약 cURL을 선호한다면, 다음 명령문에서 “wget”을 “curl -O”로 대체한다.\n\n$ wget https://raw.githubusercontent.com/swcarpentry/bc/master/novice/sql/gen-survey-database.sql\n상기 명령문으로 Wget은 HTTP 요청을 생성해서 github 저장소의 “gen-survey-database.sql” 파일만 현재 작업 디렉토리로 가져온다. 성공적으로 완료되면 터미널은 다음 출력결과를 화면에 표시한다.\n\n--2024-01-01 11:49:13--  https://raw.githubusercontent.com/swcarpentry/bc/master/novice/sql/gen-survey-database.sql\n\nLoaded CA certificate '/usr/ssl/certs/ca-bundle.crt'\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.108.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 3297 (3.2K) [text/plain]\nSaving to: ‘gen-survey-database.sql’\n\n     0K ...                                                   100% 4.15M=0.001s\n\n2024-01-01 11:49:13 (4.15 MB/s) - ‘gen-survey-database.sql’ saved [3297/3297]\nsqlite3 survey.db &lt; gen-survey-database.sql 명령어는 SQLite3 데이터베이스 엔진을 사용하여 survey.db라는 데이터베이스 파일에 gen-survey-database.sql 파일에 포함된 SQL 명령들을 실행한다. 참고로, gen-survey-database.sql 파일에는 데이터베이스 구조를 정의하고 데이터를 삽입하는 SQL 명령들을 포함하고 있다. 예를 들어, Person 테이블 에는 ‘ident’, ‘personal’, ‘family’라는 세 개의 열이 정의되어 있고, 여러 사람의 정보를 테이블에 삽입하는 명령이 포함되어 있으며, ’dyer’, ‘William’, ’Dyer’와 같은 사람이 각각의 열에 맞추어 삽입된다.\n$ sqlite3 survey.db &lt; gen-survey-database.sql",
    "crumbs": [
      "들어가며",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>실습환경 설정</span>"
    ]
  },
  {
    "objectID": "setup.html#db-브라우저",
    "href": "setup.html#db-브라우저",
    "title": "3  실습환경 설정",
    "section": "3.3 DB 브라우저",
    "text": "3.3 DB 브라우저\n\n3.3.1 SQLite DB 브라우저\nSQLite용 DB 브라우저를 여러분의 운영 체제에 맞게 https://sqlitebrowser.org/dl/ 에서 다운로드한다. SQLite용 DB 브라우저는 SQLite 데이터베이스를 생성, 편집 및 쿼리하는 시각적 도구다. SQLite는 SQLite용 DB 브라우저에 포함되어 있으므로 별도로 설치할 필요가 없습니다.\nWindows를 위한 몇 가지 옵션이 있지만, 대부분의 현대 컴퓨터는 64비트 Windows 버전을 위한 표준 설치 프로그램을 사용할 수 있다. .zip(설치 프로그램 없음) 버전은 zip 파일의 내용을 추출한 후 폴더에서 직접 실행할 수 있지만 이렇게 설치할 경우 시작 메뉴에 표시되지 않는다.\nSQLite용 DB 브라우저를 실행하여 설치가 완료되었는지 확인한다.",
    "crumbs": [
      "들어가며",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>실습환경 설정</span>"
    ]
  },
  {
    "objectID": "setup.html#r-db-firefox",
    "href": "setup.html#r-db-firefox",
    "title": "3  실습환경 설정",
    "section": "3.4 파이어폭스 SQLite 관리자",
    "text": "3.4 파이어폭스 SQLite 관리자\nSQLite 데이터베이스 파일에 있는 데이터를 다루기 위해서 이번장에서 주로 R 사용에 집중을 하지만, 다음 웹사이트에서 무료로 이용 가능한 SQLite 데이터베이스 매니저(SQLite Database Manager)로 불리는 파이어폭스 애드온(add-on)을 사용해서 좀더 쉽게 많은 작업을 수행할 수 있다. 파이어폭스 애드온은 크롬 확장 프로그램과 유사한 개념으로 파이어폭스는 개발자들이 많이 사용하는 웹브라우져 중 하나다.\n\nhttps://addons.mozilla.org/en-us/firefox/addon/sqlite-manager/\n\n브라우져를 사용해서 쉽게 테이블을 생성하고, 데이터를 삽입, 편집하고 데이터베이스 데이터에 대해 간단한 SQL 질의를 실행할 수 있다.\n이러한 점에서 데이터베이스 매니저는 텍스트 파일을 작업할 때 사용하는 텍스트 편집기와 유사하다. 텍스트 파일에 하나 혹은 몇개 작업만 수행하고자 하면, 텍스트 편집기에서 파일을 열어 필요한 수정작업을 하고 닫으면 된다. 텍스트 파일에 작업할 사항이 많은 경우는 종종 간단한 R 프로그램을 작성하여 수행한다. 데이터베이스로 작업할 때도 동일한 패턴이 발견된다. 간단한 작업은 데이터베이스 매니저를 통해서 수행하고, 좀더 복잡한 작업은 R로 수행하는 것이 더 편리하다.",
    "crumbs": [
      "들어가며",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>실습환경 설정</span>"
    ]
  },
  {
    "objectID": "setup.html#sqlite-db-연결",
    "href": "setup.html#sqlite-db-연결",
    "title": "3  실습환경 설정",
    "section": "3.5 SQLite DB 연결",
    "text": "3.5 SQLite DB 연결\n생성된 데이터베이스에 연결하기 위해서, 데이터베이스를 생성한 디렉토리 안에서 SQLite를 시작한다. 그래서 ~/swc/sql 디렉토리에서 다음과 같이 타이핑한다.\n$ sqlite3 survey.db\n“sqlite3 survey.db” 명령문이 데이터베이스를 열고 데이터베이스 명령-라인 프롬프트로 안내한다. SQLite에서 데이터베이스는 플랫 파일(flat file)로 명시적으로 열 필요가 있다. 그리고 나서 SQLite 시작되고 “sqlite”로 명령-라인 프롬프트로 다음과 같이 변경되어 표시된다.\n$ sqlite3 survey.db \nSQLite version 3.34.1 2021-01-20 14:10:07\nEnter \".help\" for usage hints.\nsqlite&gt;\n다음 출력결과가 보여주듯이 .databases 명령문으로 소속된 데이터베이스 이름과 파일 목록을 확인한다.\nsqlite&gt; .databases\nseq  name             file                                                      \n---  ---------------  ----------------------------------------------------------\n0    main             ~/swc/sql/survey.db &lt;/code&gt;&lt;/pre&gt;\n다음과 같이 타이핑해서 필요한 “Person”, “Survey”, “Site” “Visited” 테이블이 존재하는 것을 확인한다.\nsqlite&gt; .tables\n그리고 “.table”의 출력결과는 다음과 같다.\nsqlite&gt; .tables\nPerson   Site     Survey   Visited\n이제, 설치를 완료해서 다음 학습으로 진행할 수 있다. 현재 명령-라인 SQLite 세션에서 다음 연습을 수행할 수 있다.\n\n3.5.1 SQLite3 빠져 나오기\nSQLite3를 빠져나오기 위해서, 다음과 같이 타이핑한다.\nsqlite&gt; .quit\n\n\n\n\n\n\nSQLite3 CLI 대신 IPython notebook 사용방법\n\n\n\n만약 실습으로 IPython notebook 사용을 선호한다면, IPython이 로컬 컴퓨터에 설치되었는지 점검하라. 만약 설치되어 있지 않다면, 설치 방법을 참고하여 설치한다. 만약 IPython이 이미 로컬 컴퓨터에 설치되어 있다면 notebook을 열기 위해서 작업 폴더 ~/swc/sql으로 이동해서 “ipython notebook”을 타이핑한다.\n$ ipython notebook\n상기 명령어가 IPython 커널을 구동해서 디폴트 브라우져에 인터랙티브 노트북을 화면에 표시해서 편집할 수 있게 된다. 작업이 종료되면 변경사항을 간직하기 위해 노트북 저장을 기억하다.",
    "crumbs": [
      "들어가며",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>실습환경 설정</span>"
    ]
  },
  {
    "objectID": "select.html#기본-지식",
    "href": "select.html#기본-지식",
    "title": "4  데이터 선택하기",
    "section": "4.1 기본 지식",
    "text": "4.1 기본 지식\n관계형 데이터베이스(relational database)는 테이블(tables)로 정렬된 정보를 저장하고 다루는 방식이다. 각 테이블은 데이터를 기술하는 필드(fields)로도 알려진 열(column)과 데이터를 담고 있는 레코드(records)로 알려진 행(row)으로 구성된다.\n스프레드쉬트를 사용할 때, 이전 값에 기초하여 새로운 값을 계산할 때 공식을 셀(cell)에 넣어서 구한다. 데이터베이스를 사용할 때는 쿼리(queries, 질의)로 불리는 명령문을 데이터베이스 관리자(database manager)에게 보낸다. 데이터베이스 관리자는 사용자를 대신해서 데이터베이스를 조작하는 프로그램이다. 데이터베이스 관리자는 쿼리가 명기하는 임의의 조회작업과 계산작업을 수행하고 다음 쿼리작업 시작점으로 사용될 수 있는 테이블 형식으로 결과값을 반환한다.\n\n\n\n\n\n\n데이터베이스 상호호환\n\n\n\n모든 데이터베이스 관리자(IBM DB2, PostgreSQL, MySQL, Microsoft Access, SQLite)는 서로 다른 고유한 방식으로 데이터를 저장하기 때문에, 특정 데이터베이스로 저장된 데이터를 다른 곳의 데이터베이스에서 직접적으로 사용할 수는 없다. 하지만, 모든 데이터베이스 관리자에는 데이터를 다양한 형식으로 가져오기(import)와 내보내기(export) 기능을 제공한다. 그래서 한 곳에서 다른 곳으로 정보를 이동하는 것이 가능하다.\n\n\n쿼리는 SQL로 불리는 언어로 작성된다. SQL 은 “Structured Query Language”(구조적 질의 언어)의 약자다. SQL은 데이터를 분석하고 다시 조합할 수 있는 수백개의 다른 방식을 제공한다. 본서에서 일부를 살펴볼 것이지만, 일부작업이 데이터 과학자가 수행하는 일의 대부분을 처리할 것이다.",
    "crumbs": [
      "SQL 쿼리",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>데이터 선택하기</span>"
    ]
  },
  {
    "objectID": "select.html#조사-데이터베이스",
    "href": "select.html#조사-데이터베이스",
    "title": "4  데이터 선택하기",
    "section": "4.2 조사 데이터베이스",
    "text": "4.2 조사 데이터베이스\nSQLite를 사용하여 본격적인 쿼리문 작성에 앞서 실습에 사용될 데이터베이스 구조를 살펴보자.\n\n\nPerson: 측정값을 기록한 사람, id는 해당 인물을 위한 고유 식별자를 나타낸다.\n\n\n\nid\npersonal\nfamily\n\n\n\n\ndyer\nWilliam\nDyer\n\n\npb\nFrank\nPabodie\n\n\nlake\nAnderson\nLake\n\n\nroe\nValentina\nRoerich\n\n\ndanforth\nFrank\nDanforth\n\n\n\nSite: 측정값이 기록된 sites의 위치를 나타낸다.\n\n\n\nname\nlat\nlong\n\n\n\n\nDR-1\n-49.85\n-128.57\n\n\nDR-3\n-47.15\n-126.72\n\n\nMSK-4\n-48.87\n-123.4\n\n\n\nVisited: 측정 위지에서 측정값이 기록된 구체적인 위치와 날짜에 대한 특정 식별 id를 나타낸다.\n\n\n\nid\nsite\ndated\n\n\n\n\n619\nDR-1\n1927-02-08\n\n\n622\nDR-1\n1927-02-10\n\n\n734\nDR-3\n1930-01-07\n\n\n735\nDR-3\n1930-01-12\n\n\n751\nDR-3\n1930-02-26\n\n\n752\nDR-3\n-null-\n\n\n837\nMSK-4\n1932-01-14\n\n\n844\nDR-1\n1932-03-22\n\n\n\n\n\nSurvey: 각 측정지의 정확한 위치에서 취한 측정값들로, taken으로 식별된다. quant 필드는 ‘양’을 의미하는 줄임말로 측정 대상을 나타낸다. 값은 각각 ’방사능’, ‘염도’, ’온도’를 의미하는 rad, sal, temp로 표시된다.\n\n\n\ntaken\nperson\nquant\nreading\n\n\n\n\n619\ndyer\nrad\n9.82\n\n\n619\ndyer\nsal\n0.13\n\n\n622\ndyer\nrad\n7.8\n\n\n622\ndyer\nsal\n0.09\n\n\n734\npb\nrad\n8.41\n\n\n734\nlake\nsal\n0.05\n\n\n734\npb\ntemp\n-21.5\n\n\n735\npb\nrad\n7.22\n\n\n735\n-null-\nsal\n0.06\n\n\n735\n-null-\ntemp\n-26.0\n\n\n751\npb\nrad\n4.35\n\n\n751\npb\ntemp\n-18.5\n\n\n751\nlake\nsal\n0.1\n\n\n752\nlake\nrad\n2.19\n\n\n752\nlake\nsal\n0.09\n\n\n752\nlake\ntemp\n-16.0\n\n\n752\nroe\nsal\n41.6\n\n\n837\nlake\nrad\n1.46\n\n\n837\nlake\nsal\n0.21\n\n\n837\nroe\nsal\n22.5\n\n\n844\nroe\nrad\n11.25\n\n\n\n\n\n3개 항목 (Visited 테이블에서 1개, Survey 테이블에서 2개)은 실제 데이터가 아닌 특별한 -null- 항목을 가지고 있다. 왜냐하면 어떠한 값도 담고 있지 않아서 그렇다. 뒤에서 결측값(missing)을 다시 다룰 것이다.",
    "crumbs": [
      "SQL 쿼리",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>데이터 선택하기</span>"
    ]
  },
  {
    "objectID": "select.html#데이터-유무-확인하기",
    "href": "select.html#데이터-유무-확인하기",
    "title": "4  데이터 선택하기",
    "section": "4.3 데이터 유무 확인하기",
    "text": "4.3 데이터 유무 확인하기\n쉘 명령으로, survey.db를 저장한 디렉토리로 작업 디렉토리를 변경한다. 만약 바탕화면에 저장했다면 다음 명령어를 사용해야 한다.\n$ cd Desktop\n$ ls | grep survey.db\nsurvey.db\n동일한 출력이 나오면 다음 명령어를 실행할 수 있다.\n$ sqlite3 survey.db\nSQLite version 3.8.8 2015-01-16 12:08:06\nEnter \".help\" for usage hints.\nsqlite&gt;\nSQLite에 survey.db 파일에 있는 데이터베이스를 로드하라는 명령어다.\n유용한 시스템 명령어 목록을 보려면 .help를 입력한다.\nSQLite 시스템 명령어들은 SQL 명령어와 구별하기 위해 .로 시작한다.\n데이터베이스의 테이블 목록을 보려면 .tables를 입력한다.\n.tables\nPerson   Site     Survey   Visited\nIf you had the above tables, you might be curious what information was stored in each table. To get more information on the tables, type .schema to see the SQL statements used to create the tables in the database. The statements will have a list of the columns and the data types each column stores.\n위와 같은 테이블이 있다면 각 테이블에 어떤 정보가 저장되어 있는지 궁금할 수 있다. 테이블에 대한 자세한 정보를 얻으려면 .schema를 입력해 데이터베이스의 테이블을 생성하는 데 사용된 SQL 문을 확인한다. 테이블명과 함께 테이블을 구성하는 칼럼과 칼럼 자료형이 목록으로 나열된다.\n.schema\nCREATE TABLE Person(\n        ident    text,\n        personal text,\n        family   text\n);\nCREATE TABLE Site(\n        name text,\n        lat  real,\n        long real\n);\nCREATE TABLE Visited(\n        ident integer,\n        site  text,\n        dated text\n);\nCREATE TABLE Survey(\n        taken   integer,\n        person  text,\n        quant   text,\n        reading real\n);\n출력 형식은 &lt;columnName dataType&gt;로 되어 있다. 따라서 첫 번째 줄에서 Person 테이블에 세 개의 열이 있음을 알 수 있다:\n\nid는 text 자료형\npersonal는 text 자료형\nfamily는 text 자료형\n\n참고: 사용 가능한 자료형은 데이터베이스 관리자에 따라 다르며, 지원되는 자료형은 온라인에서 검색할 수 있다.\nSQLite 설정을 변경하여 출력 가독성을 높일 수 있다. 먼저, 왼쪽 정렬 출력 모드를 설정한다. 그런 다음 칼럼명 헤더 표시를 켠다.\n.mode column\n.header on\n또 다른 방법으로, .sqliterc 파일을 생성하여 설정사항을 자동으로 가져올 수 있다. 위의 명령어를 추가하고 SQLite를 다시 연다. 윈도우에서는 C:\\Users\\&lt;yourusername&gt;.sqliterc를 사용한다. 리눅스와 맥(Linux/MacOS)에서는 /Users/&lt;yourusername&gt;/.sqliterc를 사용한다.\nSQLite를 종료하고 쉘 명령줄로 돌아가려면, .quit 또는 .exit를 사용하여 종료한다.",
    "crumbs": [
      "SQL 쿼리",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>데이터 선택하기</span>"
    ]
  },
  {
    "objectID": "select.html#sql-헬로월드",
    "href": "select.html#sql-헬로월드",
    "title": "4  데이터 선택하기",
    "section": "4.4 SQL 헬로월드",
    "text": "4.4 SQL 헬로월드\n과학자의 이름을 화면에 표시하는 SQL 쿼리문을 작성해 보자. SQL SELECT 문을 사용해서 원하는 칼럼의 이름과 해당 열이 있는 테이블을 지정한다. 쿼리와 결과는 다음과 같다.\nSELECT family, personal FROM Person;\n\n\n\nfamily\npersonal\n\n\n\n\nDyer\nWilliam\n\n\nPabodie\nFrank\n\n\nLake\nAnderson\n\n\nRoerich\nValentina\n\n\nDanforth\nFrank\n\n\n\n쿼리 끝에 세미콜론(;)은 쿼리가 완료되어 실행준비 되었다고 데이터베이스 관리자에게 알린다. 명령문과 칼럼 이름을 모두 소문자로 작성했고, 테이블 이름은 타이틀 케이스(Title Case, 단어의 첫 문자를 대문자로 표기)로 작성했다. 하지만 그렇게 반듯이 할 필요는 없다. 아래 예제가 보여주듯이, SQL은 대소문자 구분하지 않는다(case insensitive).\nSeLeCt FaMiLy, PeRsOnAl FrOm PeRsOn;\n\n\n\nfamily\npersonal\n\n\n\n\nDyer\nWilliam\n\n\nPabodie\nFrank\n\n\nLake\nAnderson\n\n\nRoerich\nValentina\n\n\nDanforth\nFrank\n\n\n\n모두 소문자, 타이틀 케이스, 소문자 낙타 대문자(Lower Camel Case)를 선택하든지 관계없이 일관성을 가져라. 무작위 대문자를 추가적으로 인지하지 않더라고 복잡한 쿼리는 충분히 그 자체로 이해하기 어렵다.\nSQL의 대소문자 구분이 없는 특성을 이용해 SQL 문의 다른 부분을 구분할 수 있다. 일반적인 SQL 문 작성사례로, SQL 키워드(예: SELECT, FROM)는 대문자로, 테이블 이름은 타이틀 케이스로, 필드 이름은 소문자로 사용하는 관례를 따른다. 어떤 대소문자 사용 관례를 선택하든 일관성을 유지하는 것이 중요하다.\n\n\n\n\n\n\n노트\n\n\n\nSQL 구문의 한 측면인 ; (세미콜론)으로 명령을 마치지 않는 것은 초보자와 전문가 모두에게 혼란을 주고 있다. ;를 추가하지 않고 명령어를 입력한 후 엔터를 누르면 다음과 같은 상황이 발생한다.\nSELECT id FROM Person\n...&gt;\n...&gt;\n이것은 SQL이 추가 명령을 기다리거나 ;를 통해 SQL에게 명령을 마칠 준비가 되었음을 알리는 프롬프트다. 이 문제는 쉽게 해결할 수 있다! 단순히 ;를 입력하고 엔터를 누르면 된다!\n\n\n쿼리로 돌아가서, 데이터베이스 테이블의 행과 칼럼이 특정한 순서로 저장되어 있지 않는다는 것을 이해하는 것이 중요하다. 어떤 순서로 항상 표시되지만, 다양한 방식으로 출력을 제어할 수 있다. 예를 들어, 쿼리를 다음과 같이 작성해서 칼럼을 교환할 수 있다.\nSELECT personal, family FROM Person;\n\n\n\npersonal\nfamily\n\n\n\n\nWilliam\nDyer\n\n\nFrank\nPabodie\n\n\nAnderson\nLake\n\n\nValentina\nRoerich\n\n\nFrank\nDanforth\n\n\n\n혹은 심지어 칼럼을 반복할 수도 있다.\nSELECT id, id, id FROM Person;\n\n\n\nid\nid\nid\n\n\n\n\ndyer\ndyer\ndyer\n\n\npb\npb\npb\n\n\nlake\nlake\nlake\n\n\nroe\nroe\nroe\n\n\ndanforth\ndanforth\ndanforth\n\n\n\n손쉬운 방법으로, *을 사용해서 테이블의 모든 칼럼을 선택할 수도 있다.\nSELECT * FROM Person;\n\n\n\nid\npersonal\nfamily\n\n\n\n\ndyer\nWilliam\nDyer\n\n\npb\nFrank\nPabodie\n\n\nlake\nAnderson\nLake\n\n\nroe\nValentina\nRoerich\n\n\ndanforth\nFrank\nDanforth",
    "crumbs": [
      "SQL 쿼리",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>데이터 선택하기</span>"
    ]
  },
  {
    "objectID": "select.html#연습문제",
    "href": "select.html#연습문제",
    "title": "4  데이터 선택하기",
    "section": "연습문제",
    "text": "연습문제\n\nCREATE 쿼리문\n데이터베이스 SQL 문을 작성할 때 테이블 칼럼 자료형(문자, 숫자, 날짜 등)을 파악하고 있어야 버그없는 깔끔한 쿼리를 작성할 수 있다.\n정수를 포함하는 칼럼을 확인하기 위해, .schema 명령어를 사용하면 확인할 수 있다.\n.schema\nCREATE TABLE Person (id text, personal text, family text);\nCREATE TABLE Site (name text, lat real, long real);\nCREATE TABLE Survey (taken integer, person text, quant text, reading real);\nCREATE TABLE Visited (id integer, site text, dated text);\n출력 결과에서, Survey 테이블(3번째 줄)의 taken 칼럼이 실수 자료형임을 확인할 수 있다.\n\n\n사이트 이름 선택하기\nSite 테이블에서 name 칼럼만 선택하는 쿼리를 작성한다.\nSELECT name FROM Site;\n\n\n\nname\n\n\n\n\nDR-1\n\n\nDR-3\n\n\nMSK-4\n\n\n\n\n\n쿼리 스타일\n많은 사람들은 쿼리를 다음과 같이 작성한다.\nSELECT personal, family FROM person;\n또는,\nselect Personal, Family from PERSON;\n어떤 스타일이 가장 읽기 쉽다고 생각하며, 그 이유는 무엇인가?\n읽기 쉬운 쿼리 스타일은 주로 개인적인 선호와 경험에 따라 달라질 수 있다. 그러나 일반적으로 첫 번째 예시처럼 SQL 키워드를 대문자로, 테이블과 칼럼 이름을 소문자로 쓰는 방식이 가독성이 높다고 여겨진다. SQL 키워드와 테이블, 칼럼 이름 사이의 명확한 구분을 제공하여 쿼리를 더 쉽게 읽고 이해할 수 있게 한다. 반면, 두 번째 예시처럼 모든 것을 소문자나 대문자로 쓰는 방식은 이러한 구분을 덜 명확하게 만들어 쿼리의 구조를 파악하기 어렵게 할 수 있다.",
    "crumbs": [
      "SQL 쿼리",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>데이터 선택하기</span>"
    ]
  },
  {
    "objectID": "sort-filter.html#정렬",
    "href": "sort-filter.html#정렬",
    "title": "5  정렬과 필터",
    "section": "5.1 정렬",
    "text": "5.1 정렬\n두 경우 모두에서, 중복된 값이 제거되었음을 알 수 있다. 데이터베이스 테이블에서 해당 행들이 인접해 있지 않아도 마찬가지다.\n다음 과제로 Person 테이블에서 탐사에 참여한 과학자들을 식별하는 것이다. 앞서 언급했듯이, 데이터베이스 레코드는 일반적으로 특정한 순서로 저장되지 않는다. 쿼리 결과가 반드시 정렬되어 있지 않으며, 설사 정렬되어 있다 해도, 원하는 다른 방식(예를 들어, 개인 이름 대신 식별자 등)으로 정렬 결과를 보고 싶을 때가 많다는 의미기도 하다. SQL에서는 쿼리에 ORDER BY 절을 추가함으로써 간단하게 구현할 수 있다.\nSELECT * FROM Person ORDER BY id;\n\n\n\nid\npersonal\nfamily\n\n\n\n\ndanfort\nFrank\nDanforth\n\n\ndyer\nWilliam\nDyer\n\n\nlake\nAnderson\nLake\n\n\npb\nFrank\nPabodie\n\n\nroe\nValentina\nRoerich\n\n\n\n기본설정으로 ORDER BY를 사용할 때, 결과는 지정한 칼럼 오름차순으로 정렬된다 (즉, 가장 작은 값에서 가장 큰 값으로).\nDESC(내림차순을 의미하는 “descending”의 약자)를 사용하여 반대 순서로 정렬할 수 있다.\n\n\n\n\n\n\n정렬 참고 사항\n\n\n\n데이터베이스에 쿼리문을 전송할 때마다 레코드가 일관되게 보이는 이유는 지금까지 아무도 데이터를 변경하거나 수정하지 않았기 때문이다. 행이 일관성을 갖고 예측 가능한 순서로 반환되기를 원한다면 ORDER BY를 사용하는 것을 기억하라.\n\n\n(오름차순 정렬을 명확히 하고 싶다면, DESC 대신 ASC를 사용한다.)\n각 사이트 방문 때 어떤 과학자가 양을 측정했는지 살펴보려면, 다시 Survey 테이블을 봐야 한다. 여러 칼럼을 한 번에 정렬할 수도 있다. 예를 들어, 다음 쿼리는 결과를 먼저 taken에 따라 오름차순으로 정렬한 다음, 각각의 동일한 taken 값 그룹 내에서 person에 따라 내림차순으로 정렬한다.\nSELECT taken, person, quant FROM Survey ORDER BY taken ASC, person DESC;\n\n\n\ntaken\nperson\nquant\n\n\n\n\n619\ndyer\nrad\n\n\n619\ndyer\nsal\n\n\n622\ndyer\nrad\n\n\n622\ndyer\nsal\n\n\n734\npb\nrad\n\n\n734\npb\ntemp\n\n\n734\nlake\nsal\n\n\n735\npb\nrad\n\n\n735\n-null-\nsal\n\n\n735\n-null-\ntemp\n\n\n751\npb\nrad\n\n\n751\npb\ntemp\n\n\n751\nlake\nsal\n\n\n752\nroe\nsal\n\n\n752\nlake\nrad\n\n\n752\nlake\nsal\n\n\n752\nlake\ntemp\n\n\n837\nroe\nsal\n\n\n837\nlake\nrad\n\n\n837\nlake\nsal\n\n\n844\nroe\nrad",
    "crumbs": [
      "SQL 쿼리",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>정렬과 필터</span>"
    ]
  },
  {
    "objectID": "sort-filter.html#중복제거",
    "href": "sort-filter.html#중복제거",
    "title": "5  정렬과 필터",
    "section": "5.2 중복제거",
    "text": "5.2 중복제거\n데이터베이스에서 중복 데이터의 존재는 정보의 해석을 복잡하게 만들 수 있다. 같은 데이터가 반복되면, 결과의 해석이 어려워지고, 필요한 정보를 찾는 데 시간이 더 걸릴 수 있다. DISTINCT 키워드는 이러한 중복을 제거하여 결과를 더 명확하고 간결하게 만드는 역할을 한다. 예를 들어, 여러 번의 측정에서 동일한 과학자가 나타날 수 있는데, DISTINCT를 사용하면 각 과학자가 수행한 고유한 측정 유형만을 표시하여 데이터의 중복을 최소화하고 결과를 더 쉽게 해석할 수 있다.\n다음 쿼리를 통해 어떤 과학자가 방문에 관여했으며, 방문 동안 어떤 측정을 수행했는지를 잘 파악할 수 있다.\n테이블을 살펴보면, 일부 과학자들이 특정 종류의 측정에 전문화되어 있는 것처럼 보인다. 적절한 칼럼을 선택하고 중복을 제거함으로써 어떤 과학자가 어떤 측정을 수행했는지 선명히 드러난다.\nSELECT DISTINCT quant, person FROM Survey ORDER BY quant ASC;\n\n\n\nquant\nperson\n\n\n\n\nrad\ndyer\n\n\nrad\npb\n\n\nrad\nlake\n\n\nrad\nroe\n\n\nsal\ndyer\n\n\nsal\nlake\n\n\nsal\n-null-\n\n\nsal\nroe\n\n\ntemp\npb\n\n\ntemp\n-null-\n\n\ntemp\nlake\n\n\n\n데이터베이스 테이블의 레코드는 본질적으로 정렬되어 있지 않기 때문에, 특정 순서대로 표시하고 싶다면, ORDER BY를 명시적으로 사용하여 그 순서를 지정해야 한다. 데이터베이스 저장된 값은 고유함이 보장되지 않기 때문에, 중복을 제거하고 싶다면, DISTINCT를 사용하여 명시적으로 지정하여 처리해야만 된다.",
    "crumbs": [
      "SQL 쿼리",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>정렬과 필터</span>"
    ]
  },
  {
    "objectID": "sort-filter.html#필터",
    "href": "sort-filter.html#필터",
    "title": "5  정렬과 필터",
    "section": "5.3 필터",
    "text": "5.3 필터\n데이터베이스의 가장 강력한 기능 중 하나는 데이터를 필터(filter)하는 능력이다. 즉, 특정 기준에 맞는 레코드만 선택한다. 예를 들어, 특정 사이트를 언제 방문했는지 확인한다고 가정하자. 쿼리에 where 절을 사용해서 Visited 테이블로부터 조건에 맞는 레코드만 뽑아낼 수 있다.\nSELECT * FROM Visited WHERE site = 'DR-1';\n\n\n\nid\nsite\ndated\n\n\n\n\n619\nDR-1\n1927-02-08\n\n\n622\nDR-1\n1927-02-10\n\n\n844\nDR-1\n1932-03-22\n\n\n\n데이터베이스 관리자는 이 쿼리를 두 단계로 실행한다. 먼저, Visited 테이블 각 행을 확인하여 WHERE 조건을 만족하는 행을 찾는다. 그 다음, SELECT 키워드 뒤에 따라오는 칼럼 이름을 사용하여 표시할 칼럼을 결정한다.\n이러한 처리 순서가 의미하는 바는 화면에 표시되지 않는 칼럼 값에 기반해서도 WHERE 절을 사용해서 레코드를 필터링할 수 있다는 것이다.\nSELECT id FROM Visited WHERE site = 'DR-1';\n\n\n\nid\n\n\n\n\n619\n\n\n622\n\n\n844\n\n\n\n\n\n\nSQL 필터링 동작방식",
    "crumbs": [
      "SQL 쿼리",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>정렬과 필터</span>"
    ]
  },
  {
    "objectID": "sort-filter.html#부울-연산자",
    "href": "sort-filter.html#부울-연산자",
    "title": "5  정렬과 필터",
    "section": "5.4 부울 연산자",
    "text": "5.4 부울 연산자\n데이터를 필터링할 때 부울 연산자(Boolean Operators)를 사용한다. 이는 특정 조건을 만족하는 데이터를 선택하거나 제외하는 데 유용하다. 예를 들어, DR-1 사이트에서 1930년 이후로 수집된 모든 정보를 요청하는 경우, 부울 연산자를 사용하여 ‘사이트 이름이 DR-1이고, 수집 연도가 1930년 이후인’ 데이터를 필터링할 수 있다. 이렇게 SQL 쿼리에서 부울 연산자를 사용하면, 복잡한 데이터 집합에서 필요한 정보를 정확하고 효율적으로 추출할 수 있다.\nSELECT * FROM Visited WHERE site = 'DR-1' AND dated &lt; '1930-01-01';\n\n\n\nid\nsite\ndated\n\n\n\n\n619\nDR-1\n1927-02-08\n\n\n622\nDR-1\n1927-02-10\n\n\n\n\n\n\n\n\n\n날짜 자료형\n\n\n\n거의 모든 데이터베이스 관리자는 날짜 자료 처리를 위한 특별한 자료형(Data Type)을 가지고 있다. 실제로 많은 데이터베이스는 크게 두개로 갈린다. 하나는 “1971년 5월 31일”과 같은 날짜용이고, 다른 하나는 “31일”과 같은 기간용(duration)이다. 하지만, SQLite는 그렇지 않다. 대신, SQLite는 날짜를 텍스트(ISO-8601 표준 형식 “YYYY-MM-DD HH:MM:SS.SSSS”), 실수(율리우스 일자, 기원전 4714년 11월 24일부터 일수), 또는 정수(유닉스 시간, 1970년 1월 1일 자정 이후의 초 수)로 저장한다. 만약 복잡하게 들린다면, 그럴수도 있다 하지만 옛날 스웨덴 날짜를 파악하는 것만큼 복잡하지는 않다.\n\n\nLake나 Roerich에 의해 수행된 측정이 무엇인지 알아보려면, 그들의 이름에 대한 검사를 OR를 사용하여 결합한다.\nSELECT * FROM Survey WHERE person = 'lake' OR person = 'roe';\n\n\n\ntaken\nperson\nquant\nreading\n\n\n\n\n734\nlake\nsal\n0.05\n\n\n751\nlake\nsal\n0.1\n\n\n752\nlake\nrad\n2.19\n\n\n752\nlake\nsal\n0.09\n\n\n752\nlake\ntemp\n-16.0\n\n\n752\nroe\nsal\n41.6\n\n\n837\nlake\nrad\n1.46\n\n\n837\nlake\nsal\n0.21\n\n\n837\nroe\nsal\n22.5\n\n\n844\nroe\nrad\n11.25\n\n\n\n다른 방식으로, IN을 사용하여 특정 집합에 값이 있는지 확인할 수 있다.\nSELECT * FROM Survey WHERE person IN ('lake', 'roe');\n\n\n\ntaken\nperson\nquant\nreading\n\n\n\n\n734\nlake\nsal\n0.05\n\n\n751\nlake\nsal\n0.1\n\n\n752\nlake\nrad\n2.19\n\n\n752\nlake\nsal\n0.09\n\n\n752\nlake\ntemp\n-16.0\n\n\n752\nroe\nsal\n41.6\n\n\n837\nlake\nrad\n1.46\n\n\n837\nlake\nsal\n0.21\n\n\n837\nroe\nsal\n22.5\n\n\n844\nroe\nrad\n11.25\n\n\n\nAND와 OR을 결합할 수 있지만, 어떤 연산자가 먼저 실행되는지 주의해야 한다. 괄호를 사용하지 않으면, 다음과 같은 결과를 얻게 된다:\nSELECT * FROM Survey WHERE quant = 'sal' AND person = 'lake' OR person = 'roe';\n\n\n\ntaken\nperson\nquant\nreading\n\n\n\n\n734\nlake\nsal\n0.05\n\n\n751\nlake\nsal\n0.1\n\n\n752\nlake\nsal\n0.09\n\n\n752\nroe\nsal\n41.6\n\n\n837\nlake\nsal\n0.21\n\n\n837\nroe\nsal\n22.5\n\n\n844\nroe\nrad\n11.25\n\n\n\n이 쿼리는 Lake에 의한 염도(salinity) 측정과 Roerich에 의한 모든 측정을 포함한다. 대신에 아마도 다음과 같은 결과를 얻고자 했을 것이다.\nSELECT * FROM Survey WHERE quant = 'sal' AND (person = 'lake' OR person = 'roe');\n\n\n\ntaken\nperson\nquant\nreading\n\n\n\n\n734\nlake\nsal\n0.05\n\n\n751\nlake\nsal\n0.1\n\n\n752\nlake\nsal\n0.09\n\n\n752\nroe\nsal\n41.6\n\n\n837\nlake\nsal\n0.21\n\n\n837\nroe\nsal\n22.5",
    "crumbs": [
      "SQL 쿼리",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>정렬과 필터</span>"
    ]
  },
  {
    "objectID": "sort-filter.html#like-키워드",
    "href": "sort-filter.html#like-키워드",
    "title": "5  정렬과 필터",
    "section": "5.5 LIKE 키워드",
    "text": "5.5 LIKE 키워드\nSQL 필터링에서 LIKE 키워드의 중요성은 부분 일치를 통해 데이터를 필터링할 수 있게 해준다는 데에 있다. 예를 들어, ’DR’로 시작하는 사이트 이름과 같이 특정 패턴이나 문자열을 포함하는 레코드를 찾고 싶을 때 LIKE 키워드를 사용한다. 퍼센트 기호(%)는 와일드카드로서, 그 위치에 어떤 문자열이든 일치할 수 있게 한다. 이를 통해 문자열의 시작, 중간, 끝 부분에서 특정 패턴을 검색할 수 있다. LIKE와 와일드카드의 조합은 SQL 쿼리에서 매우 유연한 문자열 검색을 가능하게 하며, 복잡하거나 정확하지 않은 데이터에서 원하는 정보를 효과적으로 추출하는 데 중요한 역할을 한다.\nSELECT * FROM Visited WHERE site LIKE 'DR%';\n\n\n\nid\nsite\ndated\n\n\n\n\n619\nDR-1\n1927-02-08\n\n\n622\nDR-1\n1927-02-10\n\n\n734\nDR-3\n1930-01-07\n\n\n735\nDR-3\n1930-01-12\n\n\n751\nDR-3\n1930-02-26\n\n\n752\nDR-3\n\n\n\n844\nDR-1\n1932-03-22\n\n\n\n마지막으로, DISTINCT를 WHERE와 함께 사용하여, 두 번째 수준 필터링 작업을 수행한다.\nSELECT DISTINCT person, quant FROM Survey WHERE person = 'lake' OR person = 'roe';\n\n\n\nperson\nquant\n\n\n\n\nlake\nsal\n\n\nlake\nrad\n\n\nlake\ntemp\n\n\nroe\nsal\n\n\nroe\nrad\n\n\n\n하지만, 기억하라. DISTINCT는 처리될 때 선택된 칼럼에 표시되는 값에만 적용되고 전체 행에는 적용되지 않는다.\n\n\n\n\n\n\n쿼리작성 방법\n\n\n\n방금 수행한 방식이 대부분의 사람들이 SQL 쿼리를 “발전시키는” 방식이기도 하다. 의도한 것의 일부를 수행하는 단순한 것에서부터 시작했다. 그리고 절을 하나씩 하나씩 추가하면서 효과를 테스트했다. 좋은 전략이다. 사실 복잡한 쿼리를 작성할 때, 거의 유일한 전략이다. 하지만 이런 전략은 빠른 결과 확인과 더불어, 올바른 결과를 얻었을 때 빠른 인식에도 상당히 의존한다.\n빠른 결과 확인을 이루는 가장 좋은 방법은 데이터의 일부를 임시 데이터베이스에 저장하고 그 위에서 쿼리를 실행하는 것이거나, 혹은 합리적으로 구성된 레코드로 소규모 데이터베이스를 채워두고 실험하는 것이다. 예를 들어, 실제 2000만 호주 인구의 데이터베이스에서 쿼리를 실행하기보다는 1만 명의 샘플을 추출하여 실험을 하거나, 무작위 또는 그럴듯한 1만 명의 레코드를 생성할 수 있는 작은 프로그램을 작성해 사용하는 것이다.",
    "crumbs": [
      "SQL 쿼리",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>정렬과 필터</span>"
    ]
  },
  {
    "objectID": "sort-filter.html#연습문제",
    "href": "sort-filter.html#연습문제",
    "title": "5  정렬과 필터",
    "section": "연습문제",
    "text": "연습문제\n\n중복 날짜\nVisited 테이블에서 별개로 구별되는 고유한(distinct) 날짜들을 선택하는 쿼리를 작성하시오.\n다음 쿼리는 Visited 테이블에서 중복 없이 모든 고유한 ‘dated’ 칼럼의 값을 반환한다.\nSELECT DISTINCT dated FROM Visited;\n\n\n\ndated\n\n\n\n\n1927-02-08\n\n\n1927-02-10\n\n\n1930-01-07\n\n\n1930-01-12\n\n\n1930-02-26\n\n\n \n\n\n1932-01-14\n\n\n1932-03-22\n\n\n\n\n\n조사자명\nPerson 테이블에 있는 과학자들 전체 이름을 표시하고, 가족 이름(family name)으로 정렬하는 쿼리문을 작성하시오.\n다음 쿼리는 Person 테이블에서 개인 이름(personal)과 가족 이름(family)을 결합하여 전체 이름(fullname)을 생성하고, 그 결과를 가족 이름으로 정렬해 반환한다. 여기서 ||는 문자열을 연결하는 SQL 연산자다.\nSELECT personal, family FROM Person ORDER BY family ASC;\n\n\n\npersonal\nfamily\n\n\n\n\nFrank\nDanforth\n\n\nWilliam\nDyer\n\n\nAnderson\nLake\n\n\nFrank\nPabodie\n\n\nValentina\nRoerich\n\n\n\n\n\n5.5.1 쿼리 디버깅\n극에서 48&deg보다 고위도에 위치한 모든 사이트를 선택하고자 한다고 가정하자. 작성한 첫번째 쿼리는 다음과 같다.\nSELECT * FROM Site WHERE (lat &gt; -48) OR (lat &lt; 48);\n왜 이 쿼리가 잘못된 것인지 설명하세요. 그리고 쿼리를 다시 작성해서 올바르게 동작하게 만드세요.\n\nOR를 사용했기 때문에, 예를 들어 남극에 있는 사이트도 두 번째 기준을 만족하여 포함된다. 대신에, 두 가지 기준 모두를 충족하는 사이트만을 포함시키고 싶다면 다음과 같이 쿼리를 수정한다.\nSELECT * FROM Site WHERE (lat &gt; -48) AND (lat &lt; 48);\n작성된 쿼리는 위도가 -48보다 크고 48보다 작은 Site 테이블의 모든 레코드를 선택한다. 이는 사실상 남극과 북극 사이의 사이트들만을 대상으로 한다.\n\n\n5.5.2 이상치 탐지\n정규화된 염분 수치는 0.0에서 1.0 사이에 있어야 한다. 상기 범위 밖에 있는 염분수치를 가진 모든 레코드를 Survey 테이블에서 선택하는 쿼리를 작성하세요.\nSELECT * FROM Survey WHERE quant = 'sal' AND ((reading &gt; 1.0) OR (reading &lt; 0.0));\n\n\n\ntaken\nperson\nquant\nreading\n\n\n\n\n752\nroe\nsal\n41.6\n\n\n837\nroe\nsal\n22.5\n\n\n\n\n\n5.5.3 패턴 매칭\n다음 표현식 중 참은 무엇인가?\n\n'a' LIKE 'a'\n'a' LIKE '%a'\n'beta' LIKE '%a'\n'alpha' LIKE 'a%%'\n'alpha' LIKE 'a%p%'\n\n\n표현식들이 참인 이유는 다음과 같습니다:\n\nTrue - ’a’와 ’a’는 동일한 문자이기 때문이다.\nTrue - 와일드카드는 제로 또는 그 이상의 문자와 일치할 수 있기 때문이다.\nTrue - ’%’가 ’bet’과 일치하고, ’a’가 마지막 ’a’와 일치한다.\nTrue - 첫 번째 와일드카드가 ’lpha’와 일치하고, 두 번째 와일드카드는 제로 문자(또는 그 반대)와 일치한다.\nTrue - 첫 번째 와일드카드가 ’l’과 일치하고, 두 번째 와일드카드가 ’ha’와 일치한다.\n\n만약 명명된 칼럼의 값이 주어진 패턴과 일치한다면 SQL 테스트 *column-name* like *pattern*은 참이다. “0 혹은 그 이상의 문자와 매칭”된다는 것을 의미하기 위해서 ’%’문자를 패턴에 임의 숫자 횟수에 사용한다. 반면, 표현식 *column-name* not like *pattern*은 매칭을 거꾸로 한다.",
    "crumbs": [
      "SQL 쿼리",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>정렬과 필터</span>"
    ]
  },
  {
    "objectID": "calc.html#합집합",
    "href": "calc.html#합집합",
    "title": "6  새로운 값 계산",
    "section": "6.1 합집합",
    "text": "6.1 합집합\nUNION 연산자는 두 개의 쿼리 결과를 결합한다.\nSELECT * FROM Person WHERE id = 'dyer' UNION SELECT * FROM Person WHERE id = 'roe';\n\n\n\nid\npersonal\nfamily\n\n\n\n\ndyer\nWilliam\nDyer\n\n\nroe\nValentina\nRoerich\n\n\n\nUNION ALL 명령은 UNION 연산자와 동일하지만, UNION ALL은 모든 값을 선택한다는 점에서 차이가 있다. 차이점은 UNION ALL이 중복 행을 제거하지 않는다는 것이다. 대신, UNION ALL은 쿼리의 모든 행을 가져와서 하나의 테이블로 결합한다. UNION 명령은 결과 세트에 대해 SELECT DISTINCT를 수행한다. 만약 합병할 모든 레코드가 고유하다면, DISTINCT 단계를 건너뛰므로 더 빠른 결과를 얻기 위해 UNION ALL을 사용한다.",
    "crumbs": [
      "SQL 쿼리",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>새로운 값 계산</span>"
    ]
  },
  {
    "objectID": "calc.html#연습문제",
    "href": "calc.html#연습문제",
    "title": "6  새로운 값 계산",
    "section": "6.2 연습문제",
    "text": "6.2 연습문제\n\n6.2.1 염도 측정치 수정\n추가로 정보를 살펴본 결과, 발렌티나 로에리히(Valentina Roerich)가 염도를 백분율로 보고했다는 것을 알게 되었다. Survey 테이블에서 모든 염도 측정치를 100으로 나눈 값으로 반환하는 쿼리를 작성하시오.\nSELECT taken, reading / 100 FROM Survey WHERE person = 'roe' AND quant = 'sal';\n\n\n\ntaken\nreading / 100\n\n\n\n\n752\n0.416\n\n\n837\n0.225\n\n\n\n\n\n6.2.2 통합 측정목록\nUNION을 사용하여 발렌티나 로에리히(Roerich가)의 염도 측정치를 앞선 도전과제에서 설명한 대로 수정하고, 발렌티나 로에리히만의 측정치로 통합된 염도 측정치 목록을 만든다. 출력 결과는 다음과 같아야 한다.\n\n\n\ntaken\nreading\n\n\n\n\n619\n0.13\n\n\n622\n0.09\n\n\n734\n0.05\n\n\n751\n0.1\n\n\n752\n0.09\n\n\n752\n0.416\n\n\n837\n0.21\n\n\n837\n0.225\n\n\n\nSELECT taken, reading FROM Survey WHERE person != 'roe' AND quant = 'sal' UNION SELECT taken, reading / 100 FROM Survey WHERE person = 'roe' AND quant = 'sal' ORDER BY taken ASC;\n\n\n6.2.3 주요 사이트 식별자\nVisited 테이블의 사이트 식별자는 ’-’로 구분된 두 부분으로 이루어져 있다.\nSELECT DISTINCT site FROM Visited;\n\n\n\nsite\n\n\n\n\nDR-1\n\n\nDR-3\n\n\nMSK-4\n\n\n\n일부 주요 사이트 식별자(즉, 문자 코드)는 두 글자 길이이고 일부는 세 글자 길이이다. “문자열 내” 함수인 instr(X, Y)는 문자열 X 내에서 문자열 Y가 처음 나타나는 1-기반 인덱스를 반환하며, X 안에 Y가 존재하지 않으면 0을 반환한다. 부분 문자열 함수 substr(X, I, [L])는 X의 I 인덱스에서 시작하는 부분 문자열을 반환하며, 선택적으로 길이 L을 지정할 수 있다. 이 두 함수를 사용하여 고유한 주요 사이트 식별자 목록을 생성한다. (이 데이터의 경우, 목록은 “DR”과 “MSK”만을 포함해야 한다).\nSELECT DISTINCT substr(site, 1, instr(site, '-') - 1) AS MajorSite FROM Visited;",
    "crumbs": [
      "SQL 쿼리",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>새로운 값 계산</span>"
    ]
  },
  {
    "objectID": "agg.html#연습-문제",
    "href": "agg.html#연습-문제",
    "title": "7  집계",
    "section": "7.1 연습 문제",
    "text": "7.1 연습 문제\n\n7.1.1 온도 측정횟수 세기\n프랭크 파보디(Frank Pabodie)가 기록한 온도 측정횟수는 몇 번이며, 그 평균 값은 얼마인가?\nSELECT count(reading), avg(reading) FROM Survey WHERE quant = 'temp' AND person = 'pb';\n\n\n\ncount(reading)\navg(reading)\n\n\n\n\n2\n-20.0\n\n\n\n\n\n7.1.2 NULL 포함 평균 계산\n집합 값의 평균은 값들의 합을 값들의 개수로 나눈 것이다. 이는 avg 함수가 1.0, null, 5.0이라는 값들이 주어졌을 때 2.0 또는 3.0을 반환한다는 것을 의미하는가?\n\n정답은 3.0이다. NULL은 값이 아니라 값이 없음을 나타낸다. 따라서 계산에 포함되지 않는다. SQL avg 함수는 null 값을 무시하고, null이 아닌 값들만을 사용하여 평균을 계산한다. 따라서 주어진 값이 1.0, null, 5.0일 때, avg 함수는 null을 제외한 1.0과 5.0의 평균을 계산한다. 이는 (1.0 + 5.0) / 2 = 3.0 이므로, 함수는 3.0을 반환한다.\n다음 코드를 실행하여 이를 확인할 수 있다:\nSELECT AVG(a) FROM (\n    SELECT 1 AS a\n    UNION ALL SELECT NULL\n    UNION ALL SELECT 5);\n\n\n7.1.3 쿼리가 의미하는 바는?\n각 개별 방사능 측정값과 모든 방사능 측정값의 평균 사이의 차이를 계산하고자 한다. 이를 위해 다음과 같은 쿼리를 작성했다.\nSELECT avg(reading) FROM Survey WHERE quant='rad';\n쿼리가 실제로 어떤 결과를 생성하며, 그 이유는 무엇일까?\n\n쿼리는 각 측정값에 대한 결과 대신 단 하나의 결과 행만을 생성한다. avg() 함수는 단일 값을 생성하며, 먼저 실행되기 때문에 테이블이 단일 행으로 축소된다. reading 값은 단순히 임의의 값일 뿐이다.\n원하는 결과를 얻기 위해서는 두 개의 쿼리를 실행해야 한다:\nSELECT avg(reading) FROM Survey WHERE quant='rad';\n이는 평균값(6.5625)을 생성하는데, 이 값을 다음과 같은 두 번째 쿼리에 삽입할 수 있다.\nSELECT reading - 6.5625 FROM Survey WHERE quant = 'rad';\n이 쿼리는 우리가 원하는 결과를 생성하지만, 하위 쿼리(subquery)를 사용하여 이를 단일 쿼리로 결합할 수 있다.\nSELECT reading - (SELECT avg(reading) FROM Survey WHERE quant='rad') FROM Survey WHERE quant = 'rad';\n이 방법을 사용하면 두 개의 쿼리를 실행할 필요가 없다.\n요약하자면, 원래 쿼리에서 avg(reading)을 (SELECT avg(reading) FROM Survey WHERE quant='rad')로 대체한 것이다.\n\n\n7.1.4 group_concat 함수 사용\ngroup_concat(field, separator) 함수는 지정된 구분자 문자(또는 구분자가 지정되지 않은 경우 ‘,’)를 사용하여 필드의 모든 값을 연결한다. 이를 사용하여 과학자들의 이름을 한 줄 목록으로 생성하면 출력결과는 다음과 같다.\nWilliam Dyer, Frank Pabodie, Anderson Lake, Valentina Roerich, Frank Danforth\n쉼표로 구분된 모든 과학자들의 성을 나열하는 쿼리를 작성하세요. 쉼표로 구분된 모든 과학자들의 개인 이름과 성을 나열하는 쿼리를 작성하세요.\n\n쉼표로 구분된 모든 성을 나열하는 쿼리는 다음과 같다.\nSELECT group_concat(family, ',') FROM Person;\n쉼표로 구분된 모든 전체 이름을 나열하는 쿼리는 다음과 같다.\nSELECT group_concat(personal || ' ' || family, ',') FROM Person;",
    "crumbs": [
      "SQL 쿼리",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>집계</span>"
    ]
  },
  {
    "objectID": "join.html#sqlite3-survey-model",
    "href": "join.html#sqlite3-survey-model",
    "title": "7  조인(Join)",
    "section": "7.1 데이터모델 - datamodelr",
    "text": "7.1 데이터모델 - datamodelr\nsurvey.db 데이터베이스의 정보를 이용해 데이터프레임을 추출하고, 이를 datamodelr에 넣어 데이터베이스 모델을 시각화한다. datamodelr 패키지 nycflights13 사례를 참조하여 유사한 방식으로 제작한다. datamodelr는 R에서 데이터 모델 다이어그램을 그리는 방법을 제공한다. 먼저 데이터프레임만 시각화하는 작업을 진행한다.\n\n# 0. 환경설정 -----------------------\n\nlibrary(dbplyr)\nlibrary(tidyverse)\nlibrary(datamodelr)\nlibrary(DBI)\n\n# 1. 데이터 연결 -----------------------\nsurvey_con &lt;- DBI::dbConnect(RSQLite::SQLite(), \"data/survey.db\")\n\nsurvey &lt;- tbl(survey_con, \"Survey\") %&gt;% collect()\nperson &lt;- tbl(survey_con, \"Person\") %&gt;% collect()\nsite &lt;- tbl(survey_con, \"Site\") %&gt;% collect()\nvisited &lt;- tbl(survey_con, \"Visited\") %&gt;% collect()\n\nsurvey_model &lt;- dm_from_data_frames(survey, person, site, visited)\nsurvey_graph &lt;- dm_create_graph(survey_model, rankdir = \"LR\", col_attr = c(\"column\", \"type\"))\n\ndm_render_graph(survey_graph)\n\n\n\n\n\n다음으로 테이블의 키를 찾아 이를 연결시켜서 관계도 함께 표현되도록 작업한다.\n\nsurvey_model &lt;- dm_add_references(\n  survey_model,\n  \n  person$id == survey$person,\n  survey$person == visited$id,\n  visited$site == site$name,\n  survey$taken == visited$id\n)\n\nsurvey_graph &lt;- dm_create_graph(survey_model, rankdir = \"LR\", col_attr = c(\"column\", \"type\"))\ndm_render_graph(survey_graph)\n\n\n\n\n\ndbDisconnect(survey_con)\n\n이러한 작업을 하는 SQL 명령어가 JOIN이다. 어떻게 동작하는지 확인하기 위해서, Site와 Visited 테이블을 조인하면서 살펴보자.\nSELECT * FROM Site JOIN Visited;\n\n\n\nname\nlat\nlong\nid\nsite\ndated\n\n\n\n\nDR-1\n-49.85\n-128.57\n619\nDR-1\n1927-02-08\n\n\nDR-1\n-49.85\n-128.57\n622\nDR-1\n1927-02-10\n\n\nDR-1\n-49.85\n-128.57\n734\nDR-3\n1930-01-07\n\n\nDR-1\n-49.85\n-128.57\n735\nDR-3\n1930-01-12\n\n\nDR-1\n-49.85\n-128.57\n751\nDR-3\n1930-02-26\n\n\nDR-1\n-49.85\n-128.57\n752\nDR-3\n-null-\n\n\nDR-1\n-49.85\n-128.57\n837\nMSK-4\n1932-01-14\n\n\nDR-1\n-49.85\n-128.57\n844\nDR-1\n1932-03-22\n\n\nDR-3\n-47.15\n-126.72\n619\nDR-1\n1927-02-08\n\n\nDR-3\n-47.15\n-126.72\n622\nDR-1\n1927-02-10\n\n\nDR-3\n-47.15\n-126.72\n734\nDR-3\n1930-01-07\n\n\nDR-3\n-47.15\n-126.72\n735\nDR-3\n1930-01-12\n\n\nDR-3\n-47.15\n-126.72\n751\nDR-3\n1930-02-26\n\n\nDR-3\n-47.15\n-126.72\n752\nDR-3\n-null-\n\n\nDR-3\n-47.15\n-126.72\n837\nMSK-4\n1932-01-14\n\n\nDR-3\n-47.15\n-126.72\n844\nDR-1\n1932-03-22\n\n\nMSK-4\n-48.87\n-123.4\n619\nDR-1\n1927-02-08\n\n\nMSK-4\n-48.87\n-123.4\n622\nDR-1\n1927-02-10\n\n\nMSK-4\n-48.87\n-123.4\n734\nDR-3\n1930-01-07\n\n\nMSK-4\n-48.87\n-123.4\n735\nDR-3\n1930-01-12\n\n\nMSK-4\n-48.87\n-123.4\n751\nDR-3\n1930-02-26\n\n\nMSK-4\n-48.87\n-123.4\n752\nDR-3\n-null-\n\n\nMSK-4\n-48.87\n-123.4\n837\nMSK-4\n1932-01-14\n\n\nMSK-4\n-48.87\n-123.4\n844\nDR-1\n1932-03-22\n\n\n\nJOIN은 두 테이블의 교차 곱(cross product)을 생성한다. 즉, 한 테이블의 각 레코드를 다른 테이블의 각 레코드와 결합하여 가능한 모든 조합을 만든다. Site에는 세 개의 레코드가 있고 Visited에는 여덟 개가 있으므로, 조인의 출력에는 24개의 레코드가 있다(3 * 8 = 24). 그리고 각 테이블에는 세 개의 필드가 있으므로, 출력에는 여섯 개의 필드가 있다(3 + 3 = 6).\n조인이 수행하지 않은 것은 조인되는 레코드가 서로 관계가 있는지를 파악하는 것이다. 어떻게 조인할지 명시할 때까지 레코드가 서로 관계가 있는지 없는지 알 수 있는 방법은 없다. 이를 위해서 동일한 사이트 이름을 가진 조합에만 관심있다는 것을 명시하는 절(clause)을 추가한다.\nSELECT\n  Site.lat,\n  Site.long,\n  Visited.dated\nFROM\n  Site\n  JOIN Visited ON Site.name = Visited.site;\n\n\n\nname\nlat\nlong\nid\nsite\ndated\n\n\n\n\nDR-1\n-49.85\n-128.57\n619\nDR-1\n1927-02-08\n\n\nDR-1\n-49.85\n-128.57\n622\nDR-1\n1927-02-10\n\n\nDR-1\n-49.85\n-128.57\n844\nDR-1\n1932-03-22\n\n\nDR-3\n-47.15\n-126.72\n734\nDR-3\n1930-01-07\n\n\nDR-3\n-47.15\n-126.72\n735\nDR-3\n1930-01-12\n\n\nDR-3\n-47.15\n-126.72\n751\nDR-3\n1930-02-26\n\n\nDR-3\n-47.15\n-126.72\n752\nDR-3\n-null-\n\n\nMSK-4\n-48.87\n-123.4\n837\nMSK-4\n1932-01-14\n\n\n\nON은 WHERE와 매우 유사하며, SQL 기본과정에서 이 둘을 서로 바꿔가며 사용할 수 있다. [외부 조인(outer joins)이 미치는 영향에 대해 차이점이 있지만, SQL 기본과정 범위를 벗어난다. 쿼리에 ON을 추가하면, 데이터베이스 관리자는 두 다른 사이트에 대한 정보를 결합한 레코드를 제거하고 원하는 것만 남긴다.\n조인의 출력에서 필드 이름을 지정하기 위해 Table.field를 사용했음을 주목하자. 테이블에 동일한 이름의 필드가 있을 수 있고, 어떤 것에 대해 이야기하고 있는지 구체적으로 명시할 필요가 있기 때문이다. 예를 들어, Person과 Visited 테이블을 조인하면, 결과는 각 원래 테이블에서 id라는 필드를 상속받는다.\n이제 같은 점 표기법(dotted notation)을 사용하여 조인에서 실제로 원하는 칼럼 3개를 선택할 수 있다.\nSELECT\n  Site.lat,\n  Site.long,\n  Visited.dated\nFROM\n  Site\n  JOIN Visited ON Site.name = Visited.site;\n\n\n\nlat\nlong\ndated\n\n\n\n\n-49.85\n-128.57\n1927-02-08\n\n\n-49.85\n-128.57\n1927-02-10\n\n\n-49.85\n-128.57\n1932-03-22\n\n\n-47.15\n-126.72\n-null-\n\n\n-47.15\n-126.72\n1930-01-12\n\n\n-47.15\n-126.72\n1930-02-26\n\n\n-47.15\n-126.72\n1930-01-07\n\n\n-48.87\n-123.4\n1932-01-14\n\n\n\n두 테이블을 조인하는 것이 좋다면, 여러 테이블을 조인하는 것이 더 좋을 것이다. 실제로, 쿼리에 더 많은 JOIN 절을 추가하고 논리적으로 맞지 않는 레코드 조합을 걸러내기 위해 더 많은 ON 테스트를 추가함으로써 어떤 수의 테이블이든 조인할 수 있다.\nSELECT\n  Site.lat,\n  Site.long,\n  Visited.dated,\n  Survey.quant,\n  Survey.reading\nFROM \n  Site\n  JOIN Visited\n  JOIN Survey ON Site.name = Visited.site\n  AND Visited.id = Survey.taken\n  AND Visited.dated IS NOT NULL;\n\n\n\nlat\nlong\ndated\nquant\nreading\n\n\n\n\n-49.85\n-128.57\n1927-02-08\nrad\n9.82\n\n\n-49.85\n-128.57\n1927-02-08\nsal\n0.13\n\n\n-49.85\n-128.57\n1927-02-10\nrad\n7.8\n\n\n-49.85\n-128.57\n1927-02-10\nsal\n0.09\n\n\n-47.15\n-126.72\n1930-01-07\nrad\n8.41\n\n\n-47.15\n-126.72\n1930-01-07\nsal\n0.05\n\n\n-47.15\n-126.72\n1930-01-07\ntemp\n-21.5\n\n\n-47.15\n-126.72\n1930-01-12\nrad\n7.22\n\n\n-47.15\n-126.72\n1930-01-12\nsal\n0.06\n\n\n-47.15\n-126.72\n1930-01-12\ntemp\n-26.0\n\n\n-47.15\n-126.72\n1930-02-26\nrad\n4.35\n\n\n-47.15\n-126.72\n1930-02-26\nsal\n0.1\n\n\n-47.15\n-126.72\n1930-02-26\ntemp\n-18.5\n\n\n-48.87\n-123.4\n1932-01-14\nrad\n1.46\n\n\n-48.87\n-123.4\n1932-01-14\nsal\n0.21\n\n\n-48.87\n-123.4\n1932-01-14\nsal\n22.5\n\n\n-49.85\n-128.57\n1932-03-22\nrad\n11.25\n\n\n\nSite, Visited, Survey 테이블 레코드들이 서로 어떻게 대응하는지 알 수 있는 이유는 테이블들이 기본 키(primary keys)와 외래 키(foreign keys)를 포함하기 때문이다. 기본 키는 테이블 각 레코드를 고유하게 식별하는 값 또는 값의 조합이다. 외래키는 또 다른 테이블에 있는 유일하게 레코드를 식별하는 하나의 값(혹은 여러 값의 조합)이다. 다른 방식으로 말하면, 외래 키는 한 테이블의 기본 키가 다른 테이블에 존재하는 것이다. 예제 데이터베이스에서 Person.ident는 Person 테이블의 기본키인 반면에, Survey.person은 외래키로 Survey 테이블의 항목과 Person 테이블의 항목을 연결하는 외래 키다.\n대부분의 데이터베이스 설계자들은 모든 테이블이 잘 정의된 기본 키를 가져야 한다고 믿는다. 또한, 이 키는 데이터 자체와 별개여야 하므로, 데이터를 변경할 필요가 있을 때 한 곳에서만 변경을 하면 된다. 이를 위한 쉬운 방법 중 하나는 데이터베이스에 레코드를 추가할 때마다 임의 고유 ID를 생성하는 것이다. 실제로 매우 흔한 방법이며, “학생 번호”나 “환자 번호”와 같은 이름을 가진 이러한 ID는 거의 항상 데이터베이스 시스템에서 고유한 레코드 식별자로 사용된다. 아래 쿼리에서 보여주는 것처럼, SQLite는 테이블에 레코드가 추가될 때 [자동으로 레코드 번호를 부여][rowid]하고, 쿼리에서 레코드 번호를 사용한다.\nSELECT rowid, * FROM Person;\n\n\n\nrowid\nid\npersonal\nfamily\n\n\n\n\n1\ndyer\nWilliam\nDyer\n\n\n2\npb\nFrank\nPabodie\n\n\n3\nlake\nAnderson\nLake\n\n\n4\nroe\nValentina\nRoerich\n\n\n5\ndanforth\nFrank\nDanforth",
    "crumbs": [
      "SQL 쿼리",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>조인(Join)</span>"
    ]
  },
  {
    "objectID": "join.html#연습문제",
    "href": "join.html#연습문제",
    "title": "7  조인(Join)",
    "section": "7.2 연습문제",
    "text": "7.2 연습문제\n\n7.2.1 방사능 측정값 목록\nDR-1 사이트의 모든 방사선 측정치를 출력하는 쿼리를 작성하세요.\n\nSELECT\n   Survey.reading\nFROM\n   Site\n   JOIN\n      Visited\n  JOIN\n      Survey\n      ON Site.name = Visited.site\n      AND Visited.id = Survey.taken\nWHERE\n   Site.name = 'DR-1'\n   AND Survey.quant = 'rad';\n\n\n\nreading\n\n\n\n\n9.82\n\n\n7.8\n\n\n11.25\n\n\n\n\n\n7.2.2 프랭크 위치\n“Frank” 가 방문한 모든 사이트를 출력하는 쿼리를 작성하세요.\n\nSELECT\n  DISTINCT Site.name\nFROM\n  Site\n  JOIN Visited\n  JOIN Survey\n  JOIN Person ON Site.name = Visited.site\n  AND Visited.id = Survey.taken\n  AND Survey.person = Person.id\nWHERE\n  Person.personal = 'Frank';\n\n\n\nname\n\n\n\n\nDR-3\n\n\n\n\n\n7.2.3 쿼리 독해\n다음 쿼리가 무슨 결과를 산출하는지 말로 기술하세요.\nSELECT Site.name FROM Site JOIN Visited\nON Site.lat &lt; -49.0 AND Site.name = Visited.site AND Visited.dated &gt;= '1932-01-01';\n\n\n7.2.4 누가 어디에 방문했는가?\n각 사이트의 정확한 위치(위도, 경도)와 방문 날짜별로 정렬된 목록을 작성하고, 사이트를 방문한 사람의 개인 이름과 성, 그리고 측정 유형 및 측정값을 나타내는 쿼리를 작성한다. null 값은 모두 피한다. 힌트: 15개의 레코드와 8개의 필드를 얻어야 한다.\nSELECT Site.name, Site.lat, Site.long, Person.personal, Person.family, Survey.quant, Survey.reading, Visited.dated\nFROM\n   Site\n   JOIN\n      Visited\n   JOIN\n      Survey\n   JOIN\n      Person\n      ON Site.name = Visited.site\n      AND Visited.id = Survey.taken\n      AND Survey.person = Person.id\nWHERE\n   Survey.person IS NOT NULL\n   AND Visited.dated IS NOT NULL\nORDER BY\n   Visited.dated;\n\n\n\n\n\n\n\n\n\n\n\n\n\nname\nlat\nlong\npersonal\nfamily\nquant\nreading\ndated\n\n\n\n\nDR-1\n-49.85\n-128.57\nWilliam\nDyer\nrad\n9.82\n1927-02-08\n\n\nDR-1\n-49.85\n-128.57\nWilliam\nDyer\nsal\n0.13\n1927-02-08\n\n\nDR-1\n-49.85\n-128.57\nWilliam\nDyer\nrad\n7.8\n1927-02-10\n\n\nDR-1\n-49.85\n-128.57\nWilliam\nDyer\nsal\n0.09\n1927-02-10\n\n\nDR-3\n-47.15\n-126.72\nAnderson\nLake\nsal\n0.05\n1930-01-07\n\n\nDR-3\n-47.15\n-126.72\nFrank\nPabodie\nrad\n8.41\n1930-01-07\n\n\nDR-3\n-47.15\n-126.72\nFrank\nPabodie\ntemp\n-21.5\n1930-01-07\n\n\nDR-3\n-47.15\n-126.72\nFrank\nPabodie\nrad\n7.22\n1930-01-12\n\n\nDR-3\n-47.15\n-126.72\nAnderson\nLake\nsal\n0.1\n1930-02-26\n\n\nDR-3\n-47.15\n-126.72\nFrank\nPabodie\nrad\n4.35\n1930-02-26\n\n\nDR-3\n-47.15\n-126.72\nFrank\nPabodie\ntemp\n-18.5\n1930-02-26\n\n\nMSK-4\n-48.87\n-123.4\nAnderson\nLake\nrad\n1.46\n1932-01-14\n\n\nMSK-4\n-48.87\n-123.4\nAnderson\nLake\nsal\n0.21\n1932-01-14\n\n\nMSK-4\n-48.87\n-123.4\nValentina\nRoerich\nsal\n22.5\n1932-01-14\n\n\nDR-1\n-49.85\n-128.57\nValentina\nRoerich\nrad\n11.25\n1932-03-22",
    "crumbs": [
      "SQL 쿼리",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>조인(Join)</span>"
    ]
  },
  {
    "objectID": "null-hygene.html#데이터-위생",
    "href": "null-hygene.html#데이터-위생",
    "title": "6  데이터 결측과 위생",
    "section": "6.1 데이터 위생",
    "text": "6.1 데이터 위생\n데이터 위생(data hygiene)은 특히 데이터 분석, 데이터베이스 관리 및 데이터 처리에서 중요한 개념으로, 오류가 적고 정확한 데이터를 보장하기 위한 일련의 방법론을 의미한다. 지금까지 조인이 어떻게 동작하는지 살펴봤으니, 왜 관계형 모델이 그렇게 유용한지 그리고 어떻게 가장 잘 사용할 수 있는지 살펴보자. 이를 위해, 데이터베이스 설계자들이 데이터를 어떻게 구조화하는지 살펴보자.\n첫번째 규칙은 모든 값은 독립 요소로 분해될 수 없는 원자(atomic)적 속성을 지녀야 한다. 하나의 칼럼에 전체 이름을 넣는 대신에 별도로 구별되는 칼럼에 이름과 성을 저장해서 이름 컴포넌트를 뽑아내는 부분 문자열 연산(substring operation)을 사용할 필요가 없다. 좀더 중요하게는, 이름을 두 부분으로 저장한다. 왜냐하면, 공백으로 쪼개는 것은 신뢰성이 약하다. “Eloise St. Cyr” 혹은 “Jan Mikkel Steubart” 같은 이름을 생각하면 쉽게 알 수 있다.\n두번째 규칙은 모든 레코드는 유일한 기본키를 가져야한다. 내재적인 의미가 전혀없는 일련번호가 될 수도 있고, 레코드 값중의 하나 (Person 테이블의 ident 필드), 혹은 Survey 테이블에서 심지어 모든 측정값을 유일하게 식별하는 (taken, person, quant) 삼중값의 조합도 될 수 있다.\n세번째 규칙은 불필요한 정보가 없어야 한다. 예를 들어, Site테이블을 제거하고 다음과 같이 Visited 테이블을 다시 작성할 수 있다.\n\n\n\nid\nlat\nlong\ndated\n\n\n\n\n619\n-49.85\n-128.57\n1927-02-08\n\n\n622\n-49.85\n-128.57\n1927-02-10\n\n\n734\n-47.15\n-126.72\n1930-01-07\n\n\n735\n-47.15\n-126.72\n1930-01-12\n\n\n751\n-47.15\n-126.72\n1930-02-26\n\n\n752\n-47.15\n-126.72\n-null-\n\n\n837\n-48.87\n-123.40\n1932-01-14\n\n\n844\n-49.85\n-128.57\n1932-03-22\n\n\n\n사실, 스프레드쉬트와 마찬가지로 각 행에 각 측정값에 관한 모든 정보를 기록하는 하나의 테이블을 사용할 수도 있다. 문제는 이와 같은 방식으로 조직된 데이터를 일관성있게 관리하는 것은 매우 어렵다. 만약 특정한 사이트의 특정한 방문 날짜가 잘못된다면, 데이터베이스에 다수의 레코드를 변경해야한다. 더 안좋은 것은 다른 사이트도 그 날짜에 방문되었기 때문에 어느 레코드를 변경할지 추정해야하는 것이다.\n네번째 규칙은 모든 값의 단위는 명시적으로 저장되어야 한다. 예제 데이터베이스는 그렇지 못해서 문제다. 로에리히(Roerich)의 염도 측정치는 다른 사람들보다 몇 배나 더 높지만, 천분율(parts per thousand) 대신 백만분율(parts per million)을 사용했는지, 아니면 1932년 그 사이트에서 실제로 염분 이상 현상이 있었는지 알 수 없습니다.\n한걸음 물러나서 생각하자, 데이터와 저장하는데 사용되는 도구는 공생관계다. 테이블과 조인은 데이터가 특정 방식으로 잘 조직되었다면 매우 효과적이다. 하지만, 만약 특정 형태로 되어 있다면 효과적으로 다룰 수 있는 도구가 있기 때문에 데이터를 그와 같은 방식으로 조직하기도 한다. 인류학자가 말했듯이, 도구는 도구를 만드는 손을 만든다. (the tool shapes the hand that shapes the tool) 즉, 도구(기술, 방법론 등)가 사용자(인간, 조직 등)에게 영향을 미치며, 동시에 사용자가 그 도구를 개선하거나 변형시키는 과정을 의미한다. 결과적으로, 도구와 사용자는 서로 영향을 주고받으며 발전해 나간다는 개념을 내포하고 있다.",
    "crumbs": [
      "SQL 쿼리",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>데이터 결측과 위생</span>"
    ]
  },
  {
    "objectID": "null-hygene.html#db-three-keys",
    "href": "null-hygene.html#db-three-keys",
    "title": "6  데이터 결측과 위생",
    "section": "6.2 세 종류 키",
    "text": "6.2 세 종류 키\n지금까지 데이터를 다중 연결된 테이블에 넣고 키(keys)를 사용하여 행을 연결하는 방식으로 데이터 모델을 생성했는데, 키와 관련된 몇몇 용어를 살펴볼 필요가 있다. 일반적으로 데이터베이스 모델에서 세가지 종류의 키가 사용된다.\n\n논리 키(logical key)는 “실제 세상”이 행을 찾기 위해서 사용하는 키다. 데이터 모델 예제에서, name 필드는 논리키다. 사용자에 대해서 screen_name이고, name 필드를 사용하여 프로그램에서 여러번 사용자 행을 찾을 수 있다. 논리 키에 UNIQUE 제약 사항을 추가하는 것이 의미있다는 것을 종종 이해하게 된다. 논리 키는 어떻게 바깥 세상에서 행을 찾는지 다루기 때문에, 테이블에 동일한 값을 가진 다중 행이 존재한다는 것은 의미가 없다.\n주키(primary key)는 통상적으로 데이터베이스에서 자동 대입되는 숫자다. 프로그램 밖에서는 일반적으로 의미가 없고, 단지 서로 다른 테이블에서 행을 열결할 때만 사용된다. 테이블에 행을 찾을 때, 통상적으로 주키를 사용해서 행을 찾는 것이 가장 빠르게 행을 찾는 방법이다. 주키는 정수형이어서, 매우 적은 저장공간을 차지하고 매우 빨리 비교 혹은 정렬할 수 있다. 이번에 사용된 데이터 모델에서 id 필드가 주키의 한 예가 된다.\n외부 키(foreign key)는 일반적으로 다른 테이블에 연관된 행의 주키를 가리키는 숫자다. 이번에 사용된 데이터 모델의 외부 키의 사례는 from_id다.\n\n주키 id필드명을 호출하고, 항상 외부키에 임의 필드명에 접미사로 _id 붙이는 명명규칙을 사용한다.",
    "crumbs": [
      "SQL 쿼리",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>데이터 결측과 위생</span>"
    ]
  },
  {
    "objectID": "null-hygene.html#연습문제",
    "href": "null-hygene.html#연습문제",
    "title": "6  데이터 결측과 위생",
    "section": "6.3 연습문제",
    "text": "6.3 연습문제\n\n6.3.1 알려진 날짜별로 정렬\n날짜가 알려지지 않은 (즉 null) 항목은 빼고, 날짜 순으로 Visited 테이블에 있는 레코드를 정렬한\nSELECT * FROM Visited WHERE dated IS NOT NULL ORDER BY dated ASC;\n\n\n\nid\nsite\ndated\n\n\n\n\n619\nDR-1\n1927-02-08\n\n\n622\nDR-1\n1927-02-10\n\n\n734\nDR-3\n1930-01-07\n\n\n735\nDR-3\n1930-01-12\n\n\n751\nDR-3\n1930-02-26\n\n\n837\nMSK-4\n1932-01-14\n\n\n844\nDR-1\n1932-03-22\n\n\n\n\n\n6.3.2 집합에서 NULL\n다음 쿼리가 어떤 결과를 생성할 것으로 예상하는가?\nSELECT * FROM Visited WHERE dated IN ('1927-02-08', NULL);\n실제로 어떤 결과를 생성하는가?\n\n위 쿼리는 ‘1927-02-08’ 또는 NULL인 ‘dated’ 행을 반환할 것으로 예상할 수 있다. 하지만 실제로는 ’1927-02-08’인 행만 반환한다. 이는 다음과 같은 더 간단한 쿼리에서 얻을 수 있는 결과와 동일하다:\nSELECT * FROM Visited WHERE dated IN ('1927-02-08');\n이런 결과가 나타나는 이유는 IN 연산자가 값들의 집합과 작동하는데, NULL은 정의상 값이 아니며 따라서 단순히 무시되기 때문이다.\n실제로 NULL을 포함시키고자 한다면, 쿼리를 다음과 같이 IS NULL 조건을 사용하여 다시 작성해야 한다:\nSELECT * FROM Visited WHERE dated = '1927-02-08' OR dated IS NULL;\n이 쿼리는 ’dated’가 ’1927-02-08’이거나 NULL인 행을 모두 반환한다.\n\n\n6.3.3 표식값 장단점\n일부 데이터베이스 설계자들은 null 대신 표식값(sentienl value)을 사용하여 누락된 데이터를 표시하는 것을 선호한다. 예를 들어, 누락된 날짜에 “0000-00-00”을 사용하거나, 염도나 방사능 측정치가 누락된 경우 -1.0을 사용한다(실제 측정치는 음수가 될 수 없으므로). 이러한 접근방법이 단순화시킨 것은 무엇인가? 어떤 부담이나 위험을 도입하는가?\n\n표식값은 누락된 데이터의 존재를 명확히 표현한다. null보다 직관적으로 이해될 수 있고, 특정 값으로 데이터를 표시하면 null 값을 다룰 때 발생할 수 있는 복잡성(예: IS NULL 조건)을 피할 수 있다.\n표식값은 실제 데이터와 혼동될 위험이 있다. 예를 들어, “0000-00-00”이나 -1.0이 실제 측정치로 잘못 해석될 수 있다. 센티넬 값은 데이터 분석 시 추가적인 검증 단계를 필요로 한다. 예를 들어, 평균을 계산할 때 -1.0과 같은 표식값을 제외해야 한다. 센티넬 값의 사용은 데이터베이스 설계 및 유지 관리를 더 복잡하게 만들 수 있다. 모든 사용자 및 개발자가 표식값의 의미를 정확히 이해하고 있어야 한다.\n\n\n6.3.4 원자 값 식별하기\n다음 중 어떤 것이 원자 값인가? 어떤 것이 아닌가? 그 이유는 무엇인가?\n\n뉴질랜드 (New Zealand)\n87 튜링 애비뉴 (87 Turing Avenue)\n1971년 1월 25일 (January 25, 1971)\nXY 좌표 (0.5, 3.3)\n\n\n뉴질랜드는 명확한 원자 값이다.\n주소와 XY 좌표는 별도로 저장되야 하는 여러 정보를 포함하고 있다.\n\n주소, 거리명\nX 좌표, Y 좌표\n\n날짜 항목은 월, 일, 연도 요소를 포함하고 있어 덜 명확하다. 그러나 SQL에는 DATE 데이터 유형이 있으며, 날짜는 이 형식을 사용하여 저장되어야 한다. 월, 일 또는 연도를 별도로 작업해야 하는 경우, 데이터베이스 소프트웨어에서 사용 가능한 SQL 함수를 사용할 수 있다(예: SQLite EXTRACT 또는 STRFTIME).\n\n\n6.3.5 기본 키 식별하기\n다음 테이블에 기본 키는 무엇인가? 즉, 어떤 값 혹은 값들을 조합해야 레코드를 유일무이하게 식별해낼 수 있을까?\n\n\n\nlatitude\nlongitude\ndate\ntemperature\n\n\n\n\n57.3\n-22.5\n2015-01-09\n-14.2\n\n\n\n\n위도, 경도 및 날짜는 모두 온도 기록을 고유하게 식별하는 데 필요하다.",
    "crumbs": [
      "SQL 쿼리",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>데이터 결측과 위생</span>"
    ]
  },
  {
    "objectID": "create.html#삽입",
    "href": "create.html#삽입",
    "title": "10  데이터 생성과 변형",
    "section": "10.1 삽입",
    "text": "10.1 삽입\n테이블이 생성되면, INSERT, UPDATE, DELETE와 같은 다른 명령집합을 사용하여 레코드를 추가, 변경 및 제거할 수 있다.\n다음은 Site 테이블에 행을 삽입하는 예시다.\nINSERT INTO Site (name, lat, long) VALUES ('DR-1', -49.85, -128.57);\nINSERT INTO Site (name, lat, long) VALUES ('DR-3', -47.15, -126.72);\nINSERT INTO Site (name, lat, long) VALUES ('MSK-4', -48.87, -123.40);\n또한, 다른 테이블에서 직접 값을 테이블에 삽입할 수도 있다.\nCREATE TABLE JustLatLong(lat real, long real);\nINSERT INTO JustLatLong SELECT lat, long FROM Site;",
    "crumbs": [
      "SQL 쿼리",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>데이터 생성과 변형</span>"
    ]
  },
  {
    "objectID": "create.html#갱신",
    "href": "create.html#갱신",
    "title": "10  데이터 생성과 변형",
    "section": "10.2 갱신",
    "text": "10.2 갱신\n기존 레코드를 수정하는 것은 UPDATE 문을 사용하여 수행된다. 이 작업을 수행하기 위해 데이터베이스에 어떤 테이블을 업데이트할 것인지, 필드의 값들을 어떻게 변경할 것인지, 어떤 조건에서 값들을 업데이트할 것인지를 지정해줘야 한다.\n예를 들어, 앞서 마지막 INSERT 문에서 위도와 경도 값을 잘못 입력했다면, 업데이트를 통해 이를 수정할 수 있다.\nUPDATE Site SET lat = -47.87, long = -122.40 WHERE name = 'MSK-4';\nWHERE 절을 잊지 않도록 주의하라. 그렇지 않으면 업데이트 문이 Site 테이블 모든 레코드를 수정하게 된다.",
    "crumbs": [
      "SQL 쿼리",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>데이터 생성과 변형</span>"
    ]
  },
  {
    "objectID": "create.html#삭제",
    "href": "create.html#삭제",
    "title": "10  데이터 생성과 변형",
    "section": "10.3 삭제",
    "text": "10.3 삭제\n레코드를 삭제하는 것은 데이터베이스를 내부적으로 일관성 있게 유지해야 하기 때문에 좀 더 복잡하다. 테이블이 하나로 단순하다면, 제거하고자 하는 레코드와 일치하는 WHERE 절과 함께 DELETE 명령을 한다. 예를 들어, 프랭크 댄포스(Frank Danforth)가 어떠한 조사측정 업무도 수행하지 않았다는 것을 확인하고 반영한다면, 다음과 같이 Person 테이블에서 프랭크 댄포스를 제거한다.\nDELETE FROM Person WHERE id = 'danforth';\n그러나 앤더슨 레이크(Anderson Lake)를 대신 제거한다면 어떨까? Survey 테이블은 그가 수행한 측정의 7개 레코드를 여전히 포함할 것이다. 그러나 이러한 일은 절대 일어나서는 안 되는 일이다. Survey.person은 Person 테이블로 외래 키로, 작성된 모든 쿼리는 전자의 모든 값이 매칭되는 후자의 행이 있을 거라고 가정한다.\n이 문제를 참조 무결성(referential integrity)이라고 부른다. 테이블 간 모든 참조가 항상 올바르게 해결될 수 있도록 해야 한다. 이를 해결하는 한 가지 방법은 기본 키로 사용되는 레코드를 삭제하기 전에 외래 키로 lake를 사용하는 모든 레코드를 삭제하는 것이다. 데이터베이스 관리자가 지원한다면, 계단식 삭제(cascading delete)를 자동화할 수 있다. 하지만, 이 기법은 여기서 다루는 SQL 기본영역 밖이다.\n\n\n\n\n\n\n하이브리드 저장 모델\n\n\n\n많은 응용 프로그램들이 모든 것을 데이터베이스에 저장하는 대신 하이브리드 저장 모델을 사용한다. 실제 데이터(예: 천문학적 이미지)는 파일로 저장되며, 데이터베이스는 파일의 이름, 수정 날짜, 하늘의 어느 지역을 커버하는지, 그들의 분광 특성 등을 저장한다. 대부분의 음악 재생기(MP3 플레이어) 소프트웨어가 작성되는 방식이기도 하다. 응용프로그램 내부 데이터베이스는 MP3 파일을 기억하고 있지만, MP3 파일 자체는 디스크에 있다.",
    "crumbs": [
      "SQL 쿼리",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>데이터 생성과 변형</span>"
    ]
  },
  {
    "objectID": "create.html#연습문제",
    "href": "create.html#연습문제",
    "title": "10  데이터 생성과 변형",
    "section": "10.4 연습문제",
    "text": "10.4 연습문제\n\n10.4.1 NULL 치환\nSurvey.person의 null인 모든 사용자를 문자열 'unknown'으로 대체하는 SQL문을 작성하세요.\n\nUPDATE Survey SET person = 'unknown' WHERE person IS NULL;\n\n\n10.4.2 SQL 백업\nSQLite는 SQL 표준에서 벗어난 몇가지 관리 명령어을 가지고 있다. 그 중 하나는 .dump로, 데이터베이스를 다시 생성하는 데 필요한 SQL 명령을 출력한다. 또 다른 하나는 .read로, .dump에 의해 생성된 파일을 읽고 데이터베이스를 복원한다. 당신의 동료는 덤프 파일(텍스트 형식)을 버전 관리에 저장하는 것이 데이터베이스의 변경을 추적하고 관리하는 좋은 방법이라고 생각한다. 이 접근 방식의 장단점은 무엇인가? (힌트: 레코드는 특정한 순서로 저장되지 않는다.)\n\n\n장점\n\n버전 관리 시스템은 덤프 파일 버전 간의 차이를 보여줄 수 있다. 데이터베이스와 같은 이진 파일의 경우에는 이것이 불가능하다\n버전 관리 시스템(VCS)은 각 버전의 전체 복사본이 아닌 버전 간의 변경 사항만 저장한다(디스크 공간 절약)\n버전 관리 로그는 데이터베이스의 각 버전에 대한 변경 이유를 설명한다\n\n단점\n\n레코드에 고정된 순서가 없기 때문에 커밋 사이에 인위적인 차이가 발생할 수 있다",
    "crumbs": [
      "SQL 쿼리",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>데이터 생성과 변형</span>"
    ]
  },
  {
    "objectID": "prog.html#연습문제",
    "href": "prog.html#연습문제",
    "title": "10  데이터베이스 프로그래밍",
    "section": "10.2 연습문제",
    "text": "10.2 연습문제\n\n테이블 채우기 vs. 값 출력하기\nPressure라는 테이블 하나로 구성된 original.db라는 파일에 새로운 데이터베이스를 생성하는 파이썬 프로그램을 작성해보자. 테이블에는 reading이라는 단일 필드가 있으며, 10.0에서 25.0 사이의 100,000개의 무작위 숫자를 삽입한다. 이 프로그램을 실행하는 데 얼마나 걸리는가? 단순히 이 무작위 숫자를 파일에 쓰는 프로그램을 실행하는 데는 얼마나 걸리는가?\n이를 위해 먼저 sqlite3 모듈과 random 모듈을 사용할 수 있다. 데이터베이스와 테이블을 생성하고, 무작위 숫자를 생성하여 테이블에 삽입하는 코드를 작성한다. 그리고 이 프로세스가 얼마나 걸리는지 측정한다. 파일에 숫자를 쓰는 프로그램도 비슷한 방식으로 작성하고 실행 시간을 비교한다.\n\nimport sqlite3\n# import random number generator\nfrom numpy.random import uniform\n\nrandom_numbers = uniform(low=10.0, high=25.0, size=100000)\n\nconnection = sqlite3.connect(\"original.db\")\ncursor = connection.cursor()\ncursor.execute(\"CREATE TABLE Pressure (reading float not null)\")\nquery = \"INSERT INTO Pressure (reading) VALUES (?);\"\n\nfor number in random_numbers:\n    cursor.execute(query, [number])\n\ncursor.close()\n# save changes to file for next exercise\nconnection.commit()\nconnection.close()\n비교를 위해, 다음 프로그램은 무작위 숫자를 random_numbers.txt 파일에 저장한다.\nfrom numpy.random import uniform\n\nrandom_numbers = uniform(low=10.0, high=25.0, size=100000)\nwith open('random_numbers.txt', 'w') as outfile:\n    for number in random_numbers:\n        # need to add linebreak \\n\n        outfile.write(\"{}\\n\".format(number))\n\n\nSQL 필터링 vs. 파이썬 필터링\noriginal.db와 동일한 구조를 가진 새 데이터베이스 backup.db를 생성하는 파이썬 프로그램을 작성하고, original.db에서 20.0보다 큰 모든 값을 backup.db로 복사한다. 어느 것이 더 빠른가? 쿼리에서 값을 필터링하는 것, 아니면 모든 것을 메모리에 읽어 파이썬에서 필터링하는 것.\n\n첫 번째 예시는 모든 데이터를 메모리로 읽어들이고 파이썬의 if문을 사용하여 숫자를 필터링한다.\nimport sqlite3\n\nconnection_original = sqlite3.connect(\"original.db\")\ncursor_original = connection_original.cursor()\ncursor_original.execute(\"SELECT * FROM Pressure;\")\nresults = cursor_original.fetchall()\ncursor_original.close()\nconnection_original.close()\n\nconnection_backup = sqlite3.connect(\"backup.db\")\ncursor_backup = connection_backup.cursor()\ncursor_backup.execute(\"CREATE TABLE Pressure (reading float not null)\")\nquery = \"INSERT INTO Pressure (reading) VALUES (?);\"\n\nfor entry in results:\n    # number is saved in first column of the table\n    if entry[0] &gt; 20.0:\n        cursor_backup.execute(query, entry)\n\ncursor_backup.close()\nconnection_backup.commit()\nconnection_backup.close()\n반대로 다음 예제는 조건부 SELECT 문을 사용하여 SQL에서 숫자를 필터링한다. 변경된 부분은 오직 5번째 줄에서 original.db에서 값을 가져오는 부분과 15번째 줄에서 시작하는 backup.db에 숫자를 삽입하는 for 루프다. 이 버전에서는 파이썬의 if문이 필요하지 않다는 점에 주목하자.\nimport sqlite3\n\n# original.db에서 20.0보다 큰 데이터 읽기\nconn_orig = sqlite3.connect('original.db')\ncursor_orig = conn_orig.cursor()\ncursor_orig.execute('SELECT reading FROM Pressure WHERE reading &gt; 20.\nimport sqlite3\n\nconnection_original = sqlite3.connect(\"original.db\")\ncursor_original = connection_original.cursor()\ncursor_original.execute(\"SELECT * FROM Pressure WHERE reading &gt; 20.0;\")\nresults = cursor_original.fetchall()\ncursor_original.close()\nconnection_original.close()\n\nconnection_backup = sqlite3.connect(\"backup.db\")\ncursor_backup = connection_backup.cursor()\ncursor_backup.execute(\"CREATE TABLE Pressure (reading float not null)\")\nquery = \"INSERT INTO Pressure (reading) VALUES (?);\"\n\nfor entry in results:\n    cursor_backup.execute(query, entry)\n\ncursor_backup.close()\nconnection_backup.commit()\nconnection_backup.close()\n\n\n10.2.1 삽입문 생성\n동료중의 한명이 Robert Olmstead가 측정한 온도 측정치를 포함하는 다음과 같은 형식의 CSV 파일을 보내왔다.\nTaken,Temp\n619,-21.5\n622,-15.5\nsurvey 데이터베이스에 레코드로 추가하려고 CSV 파일을 읽고 SQL insert문을 출력하는 작은 파이썬 프로그램을 작성하세요. Person 테이블에 Olmstead 항목을 추가할 필요가 있을 것이다. 반복적으로 프로그램을 테스트하려면, SQL INSERT or REPLACE 문을 자세히 살펴볼 필요도 있다.",
    "crumbs": [
      "SQL 쿼리",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>데이터베이스 프로그래밍</span>"
    ]
  },
  {
    "objectID": "prog-r.html#r-도우미-함수",
    "href": "prog-r.html#r-도우미-함수",
    "title": "12  데이터베이스 프로그래밍",
    "section": "12.1 R 도우미 함수",
    "text": "12.1 R 도우미 함수\nR의 데이터베이스 인터페이스 패키지들(예: RSQLite)은 데이터베이스를 탐색하고 전체 테이블을 한 번에 읽거나 쓰는 데 유용한 공통된 도우미 함수(helper function)들을 공유한다.\n데이터베이스의 모든 테이블을 보기 위해서는 dbListTables()를 사용한다.\n```{r}\nconnection &lt;- dbConnect(SQLite(), \"survey.db\")\ndbListTables(connection)\n```\n\"Person\"  \"Site\"    \"Survey\"  \"Visited\"\n테이블 모든 칼럼 이름을 보려면 dbListFields()를 사용한다.\n```{r}\ndbListFields(connection, \"Survey\")\n```\n\"taken\"   \"person\"  \"quant\"   \"reading\"\n전체 테이블을 데이터프레임으로 읽으려면 dbReadTable()을 사용한다.\n```{r}\ndbReadTable(connection, \"Person\")\n```\n        id  personal   family\n1     dyer   William     Dyer\n2       pb     Frank  Pabodie\n3     lake  Anderson     Lake\n4      roe Valentina  Roerich\n5 danforth     Frank Danforth\n데이터베이스에 전체 테이블을 쓰기 위해 dbWriteTable()을 사용한다. R이 행 이름을 별도 열로 쓰는 것을 방지하고자 할 경우, row.names = FALSE 인수를 설정한다. 예시에서 R에 내장된 iris 데이터셋을 survey.db 데이터베이스에 테이블로 쓰는 방법은 다음과 같다.\n```{r}\ndbWriteTable(connection, \"iris\", iris, row.names = FALSE)\nhead(dbReadTable(connection, \"iris\"))\n```\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n그리고 항상 그렇듯이 완료되면 데이터베이스 연결을 닫는 것을 잊지 말자.\n```{r}\ndbDisconnect(connection)\n```",
    "crumbs": [
      "SQL 쿼리",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>데이터베이스 프로그래밍</span>"
    ]
  },
  {
    "objectID": "prog-r.html#연습문제",
    "href": "prog-r.html#연습문제",
    "title": "12  데이터베이스 프로그래밍",
    "section": "연습문제",
    "text": "연습문제\n\n테이블 채우기 vs. 값 출력하기\nPressure라는 테이블 하나로 구성된 original.db라는 파일에 새로운 데이터베이스를 생성하는 R 프로그램을 작성해보자. 테이블에는 reading이라는 단일 필드가 있으며, 10.0에서 25.0 사이의 100,000개의 무작위 숫자를 삽입한다. 이 프로그램을 실행하는 데 얼마나 걸리는가? 단순히 이 무작위 숫자를 파일에 쓰는 프로그램을 실행하는 데는 얼마나 걸리는가?\n\n\nSQL 필터링 vs. R 필터링\noriginal.db와 동일한 구조를 가진 새 데이터베이스 backup.db를 생성하는 R 프로그램을 작성하고, original.db에서 20.0보다 큰 모든 값을 backup.db로 복사한다. 어느 것이 더 빠른가? 쿼리에서 값을 필터링하는 것, 아니면 모든 것을 메모리에 읽어 R에서 필터링하는 것.",
    "crumbs": [
      "SQL 쿼리",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>데이터베이스 프로그래밍</span>"
    ]
  },
  {
    "objectID": "spreadsheet.html#엑셀-사용",
    "href": "spreadsheet.html#엑셀-사용",
    "title": "11  엑셀의 한계",
    "section": "11.1 엑셀 사용",
    "text": "11.1 엑셀 사용\n회계부정으로 파산한 엔론(Enron)은 많은 유산(Hermans 와/과 Murphy-Hill 2015)을 남겼다. 유산중에 대기업에서 스프레드쉬트 엑셀을 어떻게 사용했는지에 대한 다양한 사례를 파악할 수 있다. 엑셀 코퍼스 분석 결과에 따르면, 엔론 스프레드시트 중 24%에서 엑셀 오류가 발견되었다.\n\n\n\n\n표 11.1: 회계부정 파산한 엔론사 스프레드시트 고빈도 함수\n\n\n\n\n\n\n\n\n\n엔론(Enron) 엑셀 빈도수 높은 함수\n\n\n순위\n함수\n스프레드쉬트 갯수\n누적 백분율(%)\n\n\n\n\n1\nSUM\n578\n6.4%\n\n\n2\n+\n1,259\n14.0%\n\n\n3\n-\n2,262\n25.1%\n\n\n4\n/\n2,625\n29.1%\n\n\n5\n*\n3,959\n43.9%\n\n\n6\nIF\n4,260\n47.3%\n\n\n7\nNOW\n5,322\n59.1%\n\n\n8\nAVERAGE\n5,664\n62.8%\n\n\n9\nVLOOKUP\n5,733\n63.6%\n\n\n10\nROUND\n5,990\n66.5%\n\n\n11\nTODAY\n6,182\n68.6%\n\n\n12\nSUBTOTAL\n6,480\n71.9%\n\n\n13\nMONTH\n6,520\n72.3%\n\n\n14\nCELL\n6,774\n75.2%\n\n\n15\nYEAR\n6,812\n75.6%\n\n\n\n출처: Enron's spreadsheets and related emails: A dataset and analysis\n\n\n\n\n\n\n\n\n\n\n\n표 11.1 에 사용된 함수들 중 핵심적인 15개의 함수가 전체 스프레드시트의 76%를 차지했고, 매일 100개의 스프레드시트가 이메일에 첨부되어 유통되었으며, 전체 전자주편 중 10%에서 스프레드시트가 첨부되거나 주제로 전달되었다.",
    "crumbs": [
      "활용사례",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>엑셀의 한계</span>"
    ]
  },
  {
    "objectID": "spreadsheet.html#엑셀의-한계",
    "href": "spreadsheet.html#엑셀의-한계",
    "title": "11  엑셀의 한계",
    "section": "11.2 엑셀의 한계",
    "text": "11.2 엑셀의 한계\n엑셀은 작은 데이터 분석과 계산에 최적화되어 있지만, 데이터베이스로 사용하는 것에는 제약이 있다. 많은 기업과 조직이 중요한 데이터를 엑셀 스프레드시트에 보관하고 작업하는데, 이는 사소한 실수로 중요한 의사결정이 왜곡될 위험이 있다.\n엑셀을 사용하는 주된 이유는 엑셀 사용 습관으로 데이터를 엑셀로 저장하고 분석하는 습관이 있으며, 데이터 내보내기가 쉽기 때문이다. 또한, 데이터가 작아서 데이터베이스가 필요 없다고 생각하지만, 사업이 커지고 업무량이 늘면서 엑셀의 작업량과 복잡성이 증가한다.\n엑셀 스프레드시트의 단점으로 다음이 많이 언급된다.\n\n한 번에 한 사람만 작업 가능: 다른 사람이 작업 중이면 읽기 전용으로만 접근 가능.\n데이터 감사의 부재: 한 사람이 주로 관리하므로, 그 사람이 떠나면 지식과 정보가 손실될 수 있다.\n정형화된 작업흐름 부족: 엑셀로 정의된 업무 프로세스는 수작업으로 취합과 정리가 필요하다.\n모형 지원 부족: 엑셀은 크기가 커지면 오류에 취약해진다.\n보고서 생성의 어려움: 데이터베이스에서는 쿼리를 통해 보고서를 더 쉽게 생성할 수 있다.\n보안과 규제의 어려움: 스프레드시트는 보안과 규제를 가하기 어렵다.\n\n여러 기업이 엑셀 사용을 중요한 업무에서 단순히 피하는 것을 넘어 전면 중단시킨 사례가 존재한다. 표 11.2에 2015년 전후까지 기록된 대표적인 스프레드시트 참사를 기록한 사례들인데, 최근 들어 이런 사례들은 크게 줄었지만 코로나 전후를 틈타 다시 엑셀 참사 사례가 다양하게 언론에 보도되고 있다.\n\n\n\n\n표 11.2: 대표적인 엑셀 참사 기록\n\n\n\n\n\n\n\n\n\n2015년 이전 역대 엑셀 참사 사례\n\n\n회사\n참사 비용\n발생일\n영향\n참사 개요\n\n\n\n\n옥스포드 대학\n미확인\n'11.12월\n학생 인터뷰 일정 지연\n엑셀이 수식이 꼬여 인터뷰 일정이 뒤죽박죽\n\n\nMI5\n미확인\n'11년\n잘못된 전화번호 작업\n엑셀 서식 수식이 꼬여 엉뚱한 전화번호 작업\n\n\n'12년 런던 올림픽\n£ 0.5백만\n'12.01월\n티켓 환불 소동\n수영장 10,000 티켓이 초과 판매 (엑셀 입력 오류)\n\n\nMouchel\n£ 4.3백만\n'10.11월\nCEO 사임, 주가폭락\n연금펀드평가 £ 4.3백만 엑셀 오류\n\n\nC&C Group\n£ 9 백만\n'09.7월\n주가 15% 하락 등\n매출 3% 상승이 아니고 5% 하락, 엑셀 오류\n\n\nUK 교통부\n최소 £ 50 백만\n'12.10월\n영국민 추가 세금 부담\n영국 철도 입찰 오류\n\n\nKing 펀드\n£ 130 백만\n'11.05월\n브래드 이미지 하락\n웨일즈 지방 NHS 지출 엑셀 오류\n\n\nAXA Rosenberg\n£ 150 백만\n'11.02월\n은폐, 벌금, 브래드 이미지 하락\n엑셀 오류를 감춰서 $242 백만 벌금\n\n\nJP Morgan Chase\n£ 250 백만\n'13.01월\n명성, 고객 신뢰도 저하\n바젤 II VaR 위험 평가 엑셀 오류\n\n\nFidelity Magellan 펀드\n£ 1.6 십억\n'95.01월\n투자자에게 약속한 배당금 지급 못함\n음수 부호 누락으로 자본이득 과대계상\n\n\n미연방준비위원회\n£ 2.5 십억\n'10.10월\n명확하지 않음\n리볼빙 카드 신용액 산출 과정에 엑셀 오류\n\n\n하버드 대학\n평가 불능\n'13.04월\n유럽 정부 긴축예산 편성 근거\nGDP 대비 정부 부채 영향도 분석 엑셀 오류\n\n\n\n출처: the dirty dozen 12 modelling horror stories & spreadsheet disasters\n\n\n\n\n\n\n\n\n\n\n\n반면, 데이터베이스는 사용자 활동 기록, 정형화된 작업흐름 지원, 오류 감소, 효율적인 보고서 생성 및 강력한 보안과 규제 기능을 제공한다. 따라서 엑셀 스프레드시트 대신 데이터베이스를 사용하는 것이 여러 면에서 이점을 제공한다.\n\n\n\n\n\n\n도널드 럼스펠트와 제니 브라이언\n\n\n\n“전쟁은 현재 가용한 군대와 함께 가야지, 나중에 원하거나 바라는 군대와 함께 가는 것은 아니다” – 도널드 럼스펠트\n“데이터 분석은 알고 있는 도구와 함께 시작하지, 필요한 도구와 함께 시작하는 것이 아니다” – 제니 브라이언",
    "crumbs": [
      "활용사례",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>엑셀의 한계</span>"
    ]
  },
  {
    "objectID": "spreadsheet.html#스프레드쉬트-이해하기",
    "href": "spreadsheet.html#스프레드쉬트-이해하기",
    "title": "11  엑셀의 한계",
    "section": "11.3 스프레드쉬트 이해하기",
    "text": "11.3 스프레드쉬트 이해하기\n데이터 분석을 위해, 사람과 컴퓨터 모두 읽을 수 있는 프로그램들이 많이 존재한다. 컴퓨터 프로그램을 열어보면, 컴퓨터가 실행하는 코드와 함께, 컴퓨터에서는 무시되지만 사람에게 매우 중요한 주석이 포함되어 있다. 따라서 컴퓨터와 사람 모두가 읽을 수 있는 코드는 데이터 과학에 필수적이다.\n코드는 사람과 컴퓨터 모두에게 가독성을 제공해야 한다. 그러나 데이터 역시 사람과 컴퓨터 모두에게 가독성을 제공해야 한다는 것이 문제다. 특히 스프레드시트는 사람과 컴퓨터 양쪽 모두에게 가독성을 제공하지 않는 경우가 많다. 스프레드시트는 처음에는 쉽게 접근할 수 있지만, 1주일 지난 후 해독하기 어려운 복잡성을 종종 경험하게 된다.\n“ALGORITHMS BY COMPLEXITY”(그림 11.1) 제목 아래에 다양한 알고리즘과 시스템이 복잡도에 따라 나열되어 있으며, 왼쪽에서 오른쪽으로 갈수록 복잡도가 증가한다. 가장 왼쪽에는 “LEFTPAD”와 “QUICKSORT”가 있고, 그 오른쪽에는 “GIT MERGE”가 있다. 그 다음에는 “SELF-DRIVING CAR”과 “GOOGLE SEARCH BACKEND”가 위치하고 있으며 오른쪽 끝에 “SPRAWLING EXCEL SPREADSHEET BUILT UP OVER 20 YEARS BY A CHURCH GROUP IN NEBRASKA TO COORDINATE THEIR SCHEDULING”이라는 문구가 있고, 20년 동안 네브래스카의 한 교회 그룹이 그들의 스케줄링을 조정하기 위해 만들어온 방대한 엑셀 스프레드시트 복잡성을 강조하고 있다.\n\n\n\n\n\n\n그림 11.1: 엑셀 알고리즘 복잡성\n\n\n\n스프레드쉬트는 데이터, 서식, 수식으로 구성된다. 숫자 데이터를 엑셀로 가져오게 되면 엑셀 내장 함수를 통해 수식 계산을 수행하고, 엑셀 사용자 본인 혹은 외부 사람을 위해 서식을 입히는 과정을 거쳐 비로소 완성된 스프레드쉬트가 된다.\n\n\n\n\n\n\n그림 11.2: 엑셀 구성요소\n\n\n\n\n\n\n\n표 11.3: 데이터, 서식, 수식 엑셀 구성요소별 R/파이썬 비교\n\n\n\n\n\n\n\n\n\n스프레드쉬트와 R/파이썬 비교\n\n\n스프레드쉬트\nR\nPython\n\n\n\n\n데이터 저장 및 관리\n데이터프레임, tibble\n판다스, 넘파이\n\n\n데이터 처리 및 변환\ndplyr, tidyr\npandas, NumPy\n\n\n고급 수식 및 계산\ndplyr\nNumPy, SciPy\n\n\n데이터 시각화\nggplot2, plotly\nmatplotlib, seaborn, plotly\n\n\n인터랙티브 대시보드\nShiny 앱\nDash, Streamlit apps\n\n\n복잡한 데이터 분석\nRMarkdown/Pandoc\n쥬피터 노트북, IPython\n\n\n\n\n\n\n\n\n\n\n스프레드쉬트과 R/파이썬이 비교된 표 11.3 를 보면, 기능이 일대일로 대응되는 것을 확인할 수 있다.",
    "crumbs": [
      "활용사례",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>엑셀의 한계</span>"
    ]
  },
  {
    "objectID": "spreadsheet.html#데이터베이스-도입",
    "href": "spreadsheet.html#데이터베이스-도입",
    "title": "11  엑셀의 한계",
    "section": "11.4 데이터베이스 도입",
    "text": "11.4 데이터베이스 도입\n데이터 복잡성이 일반적으로 처리할 수 있는 것을 넘어서면 “추출(Extract), 변환(Transform), 로드(Load)”라고 하는 과정을 수행해야 한다. 예를 들어 서지학 데이터에서 다음과 같은 질문에 답을 하고자 한다고 가정해 보자.\n\n각 사람이 기여한 논문의 수는 얼마인가?\n누가 누구와 협업하는가?\n\n불행히도, “다중 값 필드(multi-valued field)”가 있는 필드 때문에 바로 스프레드시트/CSV 형식 서지학 데이터를 데이터베이스에 넣을 수 없다.\n저자 한명에만 관심이 있다면, 첫번째 질문에 답하기 위해 스프레드시트에서 저자명을 검색한 다음, 그 행을 선택하고 수동으로 그녀의 공동 저자를 집계하여 두 번째 질문에 답할 수 있다. 그러나 모든 저자에 대해서 동일한 작업을 한땀한땀 수행하는 데는 며칠이 걸릴 것이며, 거의 확실하게 실수(휴먼 에러)가 있을 것이며, 그러면 누군가가 또 다른 더 큰 스프레드시트를 건네주고 처음부터 다시 시작해야 할 것이다. 하지만, 모든 저자에 대해 하나씩 이런 작업을 수행하는 것은 몇일이 소요된다. 거의 확실히 실수도 할 것이다.\n두가지 질문에 답하기 위해 많은 작업처럼 보일 수 있지만, 수십줄 이상되는 데이터에 대해서는 많은 시간을 절약할 수 있다.\n\n데이터가 데이터베이스에 존재한다면 다른 질문들도 묻고 답하기 쉬워진다.\n향후 또다른 형태 스프레드시트에 개발한 도구를 재사용할 수 있다.\n지금까지 수행한 일에 대한 기록을 가질 수 있다(스프레드시트에서 클릭하는 것으로는 얻을 수 없는 것).\n정확할 가능성이 훨씬 더 높고 빠르다.\n\n이 접근 방식을 통해 데이터를 보다 체계적이고 효율적으로 관리할 수 있으며, 데이터 분석을 위한 기반을 마련할 수 있다. 데이터베이스에 데이터를 저장함으로써, 데이터의 일관성을 유지하고, 복잡한 쿼리를 쉽게 실행할 수 있으며, 나중에 데이터를 검토하거나 업데이트할 때 시간과 노력을 절약할 수 있다. 전체적인 작업흐름은 다음과 같다.\n\n모든 논문에 모든 기여자에 대한 (키값, 저자명) 짝을 출력하는 작은 파이썬 프로그램을 작성한다. 예를 들어, 작성한 프로그램이 스프레드쉬트 첫 세줄을 다음과 같이 변환한다:\n\n8SW85SQM McClelland, James L\n85QV9X5F McClelland, J. L.\n85QV9X5F McNaughton, B. L.\n85QV9X5F O'Reilly, R. C.\nZ4X6DT6N Ratcliff, R.\n\n프로그램을 변경해서 데이터베이스에 키값과 저자를 삽입하는 SQL insert 문장을 생성한다.\nSQL 쿼리를 사용해서 최초 질문에 답한다.\n\n\n\n\n\n\n\n바흐라이(Bahlai) 법칙\n\n\n\n“다른 사람의 데이터는 항상 일관성이 없고 잘못된 형식으로 되어 있다. (”Other people’s data is always inconsistent and in the wrong format.”)",
    "crumbs": [
      "활용사례",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>엑셀의 한계</span>"
    ]
  },
  {
    "objectID": "spreadsheet.html#데이터셋",
    "href": "spreadsheet.html#데이터셋",
    "title": "11  엑셀의 한계",
    "section": "11.5 데이터셋",
    "text": "11.5 데이터셋\n다음 간단한 예제를 통해서 데이터(스프레드쉬트에 내장된 참고문헌정보)를 어떻게 받아서 유용한 것으로 변경할지 살펴보자. 출발점은 다음과 같은 2,937행을 갖는 bibliography.csv 라는 스프레드쉬트(엑셀) 파일이다.\n\n\n\n\n표 11.4: 참고문헌정보 .csv 파일\n\n\n\n\n\n\n\n\n\n정형 참고문헌정보 일부\n\n\nkey\ntype\nyear\nauthors\ntitle\njournal\n\n\n\n\n8SW85SQM\njournalArticle\n2013\nMcClelland, James L\nIncorporating Rapid Neocortical Learning of New Schema-Consistent Information Into Complementary Learning Systems Theory.\nJ Exp Psychol Gen\n\n\n85QV9X5F\njournalArticle\n1995\nMcClelland, J. L.; McNaughton, B. L.; O'Reilly, R. C.\nWhy There are Complementary Learning Systems in the Hippocampus and Neocortex: Insights from the Successes and Failures of Connectionist Models of Learning and Memory\nPsychological Review\n\n\nZ4X6DT6N\njournalArticle\n1990\nRatcliff, R.\nConnectionist models of recognition memory: constraints imposed by learning and forgetting functions.\nPsychological review\n\n\n\n\n\n\n\n\n\n\n본격적인 개발에 들어가기 전에 프로그램을 개발하는 것이 시간을 얼마나 절약할 수 있고 정확도를 높이는지 살펴보자.\n\n11.5.1 확률이 얼마나 될까?\n스프레드쉬트는 2,937행을 담고 있다. 전체 분석작업의 99%를 틀리지 않게 하는데, 손으로하는 전사작업은 얼마나 정확성이 있을까? 즉, 행당 오류율이 얼마나 되어야 전체 작업을 올바르게 완수하는데 0.99 확률이 될까?\n\n\n# 전체 행의 수\ntotal_rows &lt;- 2937\n\n# 전체 작업의 99%가 정확해야 함\ndesired_accuracy &lt;- 0.99\n\n# 행당 오류율을 찾기\n# 정확도 = (1 - 오류율) ^ 전체 행의 수\n# 따라서 오류율 = 1 - (정확도의 1/전체 행의 수제곱)\nrow_error_rate &lt;- 1 - (desired_accuracy ^ (1/total_rows))\nscales::percent(row_error_rate, accuracy = 0.000001)\n#&gt; [1] \"0.000342%\"\n\n\"0.000342%\"\n\n\n11.5.2 손익분기점\n수작업으로 5분만 소요되는 작업을 (전산화해서) 10분 걸려 프로그램 작성한다면, 해당 작업을 두번 이상 수행한다면, 프로그램으로 작성할 가치가 있다. 유사하게, 특정한 저자와 공저자가 누구인지만 알아내려고 하고, 다른 질문은 전혀 없을 것이거나, 반복작업을 할 필요가 없다면, 수작업으로 스프레드쉬트를 검색하는 것이 데이터를 데이터베이스로 옮기는 프로그램을 작성하는 것보다 아마도 더 빠를 것이다.\n현재 수작업으로 하고 있는 작업을 선택하라. 매번 얼마의 시간이 소요되고, 얼마나 자주 수행하는지 추정하고, 대신에 작업을 프로그램으로 만드는데 얼마나 소요되는지 추정하라. 프로그래밍이 실질적으로 얼마나 시간을 절약해줄까? 얼마나 확신이 되나요?\n\n이 문제를 해결하기 위해, 현재 수작업으로 진행 중인 작업을 선정하고, 그 작업에 대한 다음 정보들을 추정해야 한다.\n\n작업에 소요되는 시간: 각 작업 수행에 걸리는 평균 시간을 추정한다.\n작업의 빈도: 이 작업이 얼마나 자주 수행되는지 추정한다. 예를 들어, 일주일에 몇 번 또는 한 달에 몇번 등이 된다.\n프로그램 작성에 소요되는 시간: 동일한 작업을 자동화하는 프로그램을 작성하는 데 필요한 시간을 추정한다.\n\n상기 정보를 바탕으로 프로그래밍이 실질적으로 시간을 절약해주는지를 평가할 수 있다. 시간 절약의 계산은 다음과 같은 간단한 공식으로 이루어집니다:\n\n총 절약시간 = (수작업 시간 * 작업 빈도 * 기간) - 프로그램 작성 시간\n\n여기서, “기간”은 프로그램이 사용될 예상 기간이 된다.\n예를 들어, 매주 2시간 걸리는 작업이 있고, 이를 자동화하는 프로그램을 작성하는 데 10시간이 걸린다고 가정해 보자. 프로그램이 1년 동안 사용될 것이라고 가정하면, 총 절약 시간은 다음과 같습니다:\n\n총 절약시간 = (2시간/주 * 52주) - 10시간 = 94시간\n\n프로그램 작성에 들인 시간을 고려하더라도 연간 94시간을 절약할 수 있음을 의미한다.\n유념할 점은 이러한 추정은 작업의 복잡성, 작업 빈도 및 프로그래밍 능력에 따라 달라질 수 있으므로, 여러가지 요소들을 고려하여 신중하게 추정해야 한다.\n\n\n\n\nHermans, Felienne, 와/과 Emerson Murphy-Hill. 2015. “Enron’s Spreadsheets and Related Emails: A Dataset and Analysis”. In 37th International Conference on Software Engineering, ICSE ’15.",
    "crumbs": [
      "활용사례",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>엑셀의 한계</span>"
    ]
  },
  {
    "objectID": "etl.html#데이터베이스-도입",
    "href": "etl.html#데이터베이스-도입",
    "title": "12  ETL 데이터베이스",
    "section": "12.1 데이터베이스 도입",
    "text": "12.1 데이터베이스 도입\n데이터 복잡성이 일반적으로 처리할 수 있는 것을 넘어서면 “추출(Extract), 변환(Transform), 로드(Load)”라고 하는 과정을 수행해야 한다. 예를 들어 서지학 데이터에서 다음과 같은 질문에 답을 하고자 한다고 가정해 보자.\n\n각 사람이 기여한 논문의 수는 얼마인가?\n누가 누구와 협업하는가?\n\n불행히도, “다중 값 필드(multi-valued field)”가 있는 필드 때문에 바로 스프레드시트/CSV 형식 서지학 데이터를 데이터베이스에 넣을 수 없다.\n저자 한명에만 관심이 있다면, 첫번째 질문에 답하기 위해 스프레드시트에서 저자명을 검색한 다음, 그 행을 선택하고 수동으로 그녀의 공동 저자를 집계하여 두 번째 질문에 답할 수 있다. 그러나 모든 저자에 대해서 동일한 작업을 한땀한땀 수행하는 데는 며칠이 걸릴 것이며, 거의 확실하게 실수(휴먼 에러)가 있을 것이며, 그러면 누군가가 또 다른 더 큰 스프레드시트를 건네주고 처음부터 다시 시작해야 할 것이다. 하지만, 모든 저자에 대해 하나씩 이런 작업을 수행하는 것은 몇일이 소요된다. 거의 확실히 실수도 할 것이다.\n두가지 질문에 답하기 위해 많은 작업처럼 보일 수 있지만, 수십줄 이상되는 데이터에 대해서는 많은 시간을 절약할 수 있다.\n\n데이터가 데이터베이스에 존재한다면 다른 질문들도 묻고 답하기 쉬워진다.\n향후 또다른 형태 스프레드시트에 개발한 도구를 재사용할 수 있다.\n지금까지 수행한 일에 대한 기록을 가질 수 있다(스프레드시트에서 클릭하는 것으로는 얻을 수 없는 것).\n정확할 가능성이 훨씬 더 높고 빠르다.\n\n이 접근 방식을 통해 데이터를 보다 체계적이고 효율적으로 관리할 수 있으며, 데이터 분석을 위한 기반을 마련할 수 있다. 데이터베이스에 데이터를 저장함으로써, 데이터의 일관성을 유지하고, 복잡한 쿼리를 쉽게 실행할 수 있으며, 나중에 데이터를 검토하거나 업데이트할 때 시간과 노력을 절약할 수 있다. 전체적인 작업흐름은 다음과 같다.\n\n모든 논문에 모든 기여자에 대한 (키값, 저자명) 짝을 출력하는 작은 파이썬 프로그램을 작성한다. 예를 들어, 작성한 프로그램이 스프레드쉬트 첫 세줄을 다음과 같이 변환한다:\n\n8SW85SQM McClelland, James L\n85QV9X5F McClelland, J. L.\n85QV9X5F McNaughton, B. L.\n85QV9X5F O'Reilly, R. C.\nZ4X6DT6N Ratcliff, R.\n\n프로그램을 변경해서 데이터베이스에 키값과 저자를 삽입하는 SQL insert 문장을 생성한다.\nSQL 쿼리를 사용해서 최초 질문에 답한다.\n\n\n\n\n\n\n\n바흐라이(Bahlai) 법칙\n\n\n\n“다른 사람의 데이터는 항상 일관성이 없고 잘못된 형식으로 되어 있다. (”Other people’s data is always inconsistent and in the wrong format.”)",
    "crumbs": [
      "활용사례",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>ETL 데이터베이스</span>"
    ]
  },
  {
    "objectID": "etl.html#데이터셋",
    "href": "etl.html#데이터셋",
    "title": "12  ETL 데이터베이스",
    "section": "12.2 데이터셋",
    "text": "12.2 데이터셋\n다음 간단한 예제를 통해서 데이터(스프레드쉬트에 내장된 참고문헌정보)를 어떻게 받아서 유용한 것으로 변경할지 살펴보자. 출발점은 다음과 같은 2,937행을 갖는 bibliography.csv 라는 스프레드쉬트(엑셀) 파일이다.\n\n\n\n\n\n\n\n\n\n\n\nkey\ntype\nyear\nauthors\ntitle\njournal\n\n\n\n\n8SW85SQM\njournalArticle\n2013\nMcClelland, James L\nIncorporating Rapid Neocortical Learning of New Schema-Consistent Information Into Complementary Learning Systems Theory.\nJ Exp Psychol Gen\n\n\n85QV9X5F\njournalArticle\n1995\nMcClelland, J. L.; McNaughton, B. L.; O’Reilly, R. C.\nWhy There are Complementary Learning Systems in the Hippocampus and Neocortex: Insights from the Successes and Failures of Connectionist Models of Learning and Memory\nPsychological Review\n\n\nZ4X6DT6N\njournalArticle\n1990\nRatcliff, R.\nConnectionist models of recognition memory: constraints imposed by learning and forgetting functions.\nPsychological review\n\n\n\n본격적인 개발에 들어가기 전에 프로그램을 개발하는 것이 시간을 얼마나 절약할 수 있고 정확도를 높이는지 살펴보자.\n\n12.2.1 확률이 얼마나 될까?\n스프레드쉬트는 2,937행을 담고 있다. 전체 분석작업의 99%를 틀리지 않게 하는데, 손으로하는 전사작업은 얼마나 정확성이 있을까? 즉, 행당 오류율이 얼마나 되어야 전체 작업을 올바르게 완수하는데 0.99 확률이 될까?\n\n\n# 전체 행의 수\ntotal_rows &lt;- 2937\n\n# 전체 작업의 99%가 정확해야 함\ndesired_accuracy &lt;- 0.99\n\n# 행당 오류율을 찾기\n# 정확도 = (1 - 오류율) ^ 전체 행의 수\n# 따라서 오류율 = 1 - (정확도의 1/전체 행의 수제곱)\nrow_error_rate &lt;- 1 - (desired_accuracy ^ (1/total_rows))\nscales::percent(row_error_rate, accuracy = 0.000001)\n#&gt; [1] \"0.000342%\"\n\n\"0.000342%\"\n\n\n12.2.2 손익분기점\n수작업으로 5분만 소요되는 작업을 (전산화해서) 10분 걸려 프로그램 작성한다면, 해당 작업을 두번 이상 수행한다면, 프로그램으로 작성할 가치가 있다. 유사하게, 특정한 저자와 공저자가 누구인지만 알아내려고 하고, 다른 질문은 전혀 없을 것이거나, 반복작업을 할 필요가 없다면, 수작업으로 스프레드쉬트를 검색하는 것이 데이터를 데이터베이스로 옮기는 프로그램을 작성하는 것보다 아마도 더 빠를 것이다.\n현재 수작업으로 하고 있는 작업을 선택하라. 매번 얼마의 시간이 소요되고, 얼마나 자주 수행하는지 추정하고, 대신에 작업을 프로그램으로 만드는데 얼마나 소요되는지 추정하라. 프로그래밍이 실질적으로 얼마나 시간을 절약해줄까? 얼마나 확신이 되나요?\n\n이 문제를 해결하기 위해, 현재 수작업으로 진행 중인 작업을 선정하고, 그 작업에 대한 다음 정보들을 추정해야 한다.\n\n작업에 소요되는 시간: 각 작업 수행에 걸리는 평균 시간을 추정한다.\n작업의 빈도: 이 작업이 얼마나 자주 수행되는지 추정한다. 예를 들어, 일주일에 몇 번 또는 한 달에 몇번 등이 된다.\n프로그램 작성에 소요되는 시간: 동일한 작업을 자동화하는 프로그램을 작성하는 데 필요한 시간을 추정한다.\n\n상기 정보를 바탕으로 프로그래밍이 실질적으로 시간을 절약해주는지를 평가할 수 있다. 시간 절약의 계산은 다음과 같은 간단한 공식으로 이루어집니다:\n\n총 절약시간 = (수작업 시간 * 작업 빈도 * 기간) - 프로그램 작성 시간\n\n여기서, “기간”은 프로그램이 사용될 예상 기간이 된다.\n예를 들어, 매주 2시간 걸리는 작업이 있고, 이를 자동화하는 프로그램을 작성하는 데 10시간이 걸린다고 가정해 보자. 프로그램이 1년 동안 사용될 것이라고 가정하면, 총 절약 시간은 다음과 같습니다:\n\n총 절약시간 = (2시간/주 * 52주) - 10시간 = 94시간\n\n프로그램 작성에 들인 시간을 고려하더라도 연간 94시간을 절약할 수 있음을 의미한다.\n유념할 점은 이러한 추정은 작업의 복잡성, 작업 빈도 및 프로그래밍 능력에 따라 달라질 수 있으므로, 여러가지 요소들을 고려하여 신중하게 추정해야 한다.\n\n\n\n\n\n\n데이터 모델링\n\n\n\n관계형 데이터베이스의 진정한 힘은 다중 테이블과 테이블 사이의 관계를 생성할 때 생긴다. 응용프로그램 데이터를 쪼개서 다중 테이블과 두 테이블 간에 관계를 설정하는 것을 데이터 모델링(data modeling)이라고 한다. 테이블 정보와 테이블 관계를 표현하는 설계 문서를 데이터 모델(data model)이라고 한다.\n데이터 모델링(data modeling)은 상대적으로 고급 기술이여서 이번 장에서는 관계형 데이터 모델링의 가장 기본적인 개념만을 소개한다. 데이터 모델링에 대한 좀더 자세한 사항은 다음 링크에서 시작해 볼 수 있다.\n문자열 데이터 중복은 데이터베이스 정규화(database normalization) 모범 사례(best practice)를 위반하게 만든다. 기본적으로 데이터베이스 정규화는 데이터베이스에 결코 한번 이상 동일한 문자열을 저장하지 않는다. 만약 한번 이상 데이터가 필요하다면, 그 특정 데이터에 대한 숫자 키(key)를 생성하고, 그 키를 사용하여 실제 데이터를 참조한다.\n실무에서, 문자열이 컴퓨터 주기억장치나 디스크에 저장되는 정수형 자료보다 훨씬 많은 공간을 차지하고 더 많은 처리시간이 비교나 정렬에 소요된다. 항목이 단지 수백개라면, 저장소나 처리 시간이 그다지 문제되지 않는다. 하지만, 데이터베이스에 수백만명의 사람 정보와 1억건 이상의 링크가 있다면, 가능한 빨리 데이터를 스캔하는 것이 정말 중요하다.",
    "crumbs": [
      "활용사례",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>ETL 데이터베이스</span>"
    ]
  },
  {
    "objectID": "etl.html#자료-추출",
    "href": "etl.html#자료-추출",
    "title": "12  ETL 데이터베이스",
    "section": "12.3 자료 추출",
    "text": "12.3 자료 추출\n첫번째 단계는 스프레드쉬트 행을 (키값, 저자명) 짝으로 추출(extract)하는 것이다. 파이썬이 올바르게 스프레드쉬트를 읽어들이는지 확인하는 것부터 시작된다. 윈도우에서 파이썬으로 CSV 파일을 읽어드리게 되면 인코딩 오류가 발생하는 경우가 있다. 이를 방지하기 위해 인코딩을 UTF-8으로 설정한다.\n```{python}\n# count-lines.py\n# 스프레드쉬트에 얼마나 많은 줄이 있는지 계수한다.\n\n# -*- coding: utf-8 -*-\n\nimport sys\n\nfilename = sys.argv[1]\nreader = open(filename, 'r', encoding='UTF-8')\ncount = 0\nfor line in reader:\n    count += 1\nreader.close()\n\nprint(count)\n```\n상기 코드가 이제는 칙숙해보여야 된다: 파일명이 첫번째 명령라인 인자(sys.argv[1])로 주어졌다. 따라서, 파일을 열고, for 루프를 사용해서 한줄씩 읽어들인다. 매번 루프가 실행될 때, 1을 count 변수에 더한다; 루프가 종류될 때, 파일을 닫고 계수 결과를 출력한다.\n상기 프로그램을 다음과 같이 실행한다:\n$ python code/count-lines.py data/bibliography.csv\n물론, 결과는 다음과 같다:\n2937\n그래서, 파이썬이 모든 행을 읽어들인 것을 알게된다.\n다음 단계는 각 줄을 필드로 쪼개서 각 항목에 대한 키값과 저자명을 얻게된다. 필드는 콤마로 구분된다. 그래서 str.split 사용해서 시도해볼 수 있다. 하지만, 동작하지는 않는데 이유는 저자명에도 콤마가 포함되어서 그렇다(“성, 이름”같은 형식으로 되어 있어서 그렇다).\n대신에 취할 수 있는 조치는 선호하는 검색엔진에 도움을 청한다. 물론, “python csv”에 대한 검색결과는 csv 라이브러리가 나오고, 표준 파이썬 배포판의 일부이기도 하다. 라이브러리 문서에 일부 예제가 포함되어 있다. 몇번 실험을 한 뒤에, 다음과 같은 결과가 나오게 된다:\n```{python}\n# read-fields.py\n# CSV 파일에서 필드값을 제대로 읽어 오는지 확인한다.\n\n# -*- coding: utf-8 -*-\n\nimport sys\nimport csv\n\nraw = open(sys.argv[1], 'r', encoding='utf-8')\nreader = csv.reader(raw);\n\nfor line in reader:\n    print(line)\n    \nraw.close()\n```\n작성한 프로그램은 참고문헌 파일을 열어서 시작한다(다시 한번, 첫번째 명령-라인 인자로 파일명을 넘긴다) 그리고 나서, csv.reader 메쏘드를 호출해서 파일주위에 래퍼를 생성한다. open으로 생성된 기본 파일 객체가 한번에 한줄씩 읽어올 때, csv.reader에 의해서 생성된 래퍼가 해당 라인을 올바른 지점에서 필드로 쪼갠다. csv.reader는 해당 필드에 내장된 콤마, 특수문자, 신경쓰지 않아도 되는 다른 엄청난 것에 대해 어떻게 처리하는지 알고 있다.\n올바르게 동작하는지 점검하려면, csv.reader에 의한 처리가 끝난 후에 각 줄을 출력하면 된다. 출력결과 중 첫 몇줄이 다음에 나와 있다:\n$ python code/read-fields.py data/bibliography.csv | head -5\n\n['8SW85SQM', 'journalArticle', '2013', 'McClelland, James L', 'Incorporating Rapid Neocortical Learning of New Schema-Consistent Information Into Complementary Learning Systems Theory.', 'J Exp Psychol Gen', '', '1939-2222', '', 'http://www.biomedsearch.com/nih/Incorporating-Rapid-Neocortical-Learning-New/23978185.html', '', '', '', '', '', '', '', '', '']\n['85QV9X5F', 'journalArticle', '1995', \"McClelland, J. L.; McNaughton, B. L.; O'Reilly, R. C.\", 'Why There are Complementary Learning Systems in the Hippocampus and Neocortex: Insights from the Successes and Failures of Connectionist Models of Learning and Memory', 'Psychological Review', '', '', '', '', '', '', '', '', '', '', '', '', '']\n['Z4X6DT6N', 'journalArticle', '1990', 'Ratcliff, R.', 'Connectionist models of recognition memory: constraints imposed by learning and forgetting functions.', 'Psychological review', '', '0033-295X', '', 'http://view.ncbi.nlm.nih.gov/pubmed/2186426', '', '', '', '', '', '', '', '', '']\n['F5DGU3Q4', 'bookSection', '1989', 'McCloskey, M.; Cohen, N. J.', 'Catastrophic Interference in Connectionist Networks: The Sequential Learning Problem', 'The Psychology of Learning and Motivation, Vol. 24', '', '', '', '', '', '', '', '', '', '', '', '', '']\n['PNGQMCP5', 'conferencePaper', '2006', 'Bucilu\\xc7\\x8e, Cristian; Caruana, Rich; Niculescu-Mizil, Alexandru', 'Model compression', 'Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining', '', '', '', '', '', '', '', '', '', '', '', '', '']\n(프로그램 출력결과를 head 명령어로 실행해서 출력결과를 스크롤해서 다시 위로 올라가기 보다 첫 몇줄만 화면에 출력함에 주목한다.) 상기 결과는 정확하게 필요한 결과다: 키값이 각 리스트 첫번째 구성요소로 있고, 저자는 모두 네번째에 몰려있따. 프로그램을 변경해서, 단지 두 필드만 출력하게 변경하자:\n```{python}\n# display-fields.py\n# 키값과 저자 모두를 화면에 출력한다.\n\n# -*- coding: utf-8 -*-\n\nimport sys\nimport csv\n\nraw = open(sys.argv[1], 'r', encoding='utf-8')\nreader = csv.reader(raw);\n\nfor line in reader:\n    print (line[0], line[3])\n\nraw.close()\n    \n```\n출력결과는 다음과 같다:\n8SW85SQM McClelland, James L\n85QV9X5F McClelland, J. L.; McNaughton, B. L.; O'Reilly, R. C.\nZ4X6DT6N Ratcliff, R.\nF5DGU3Q4 McCloskey, M.; Cohen, N. J.\nPNGQMCP5 Buciluǎ, Cristian; Caruana, Rich; Niculescu-Mizil, Alexandru\n마지막 단계는 저자 다수를 갖는 행을 복수개 행으로 단일저자가 한줄에 나타나도록 변경한다. 이번이 str.split 메쏘드를 사용할 때다: 저자명이 세미콜론으로 구분되어 있어서, 저자 목록을 각 저자별로 나눌 수 있다. 또다른 루프를 사용해서 하나씩 결과를 화면에 출력한다:\n$ python code/display-authors-1.py data/bibliography.csv | head -10\n\n8SW85SQM McClelland, James L\n85QV9X5F McClelland, J. L.\n85QV9X5F McNaughton, B. L.\n85QV9X5F O'Reilly, R. C.\nZ4X6DT6N Ratcliff, R.\nF5DGU3Q4 McCloskey, M.\nF5DGU3Q4 Cohen, N. J.\nPNGQMCP5 Buciluǎ, Cristian\nPNGQMCP5 Caruana, Rich\nPNGQMCP5 Niculescu-Mizil, Alexandru\n이제 원하는 바에 가까워졌다. 하지만, 꼭 그렇지는 않다; 저자명은 실제로 세미콜론과 공백으로 구분되는데 세미콜론만으로 구분했기 때문에, 각 줄마다 두번째와 이어진 명칭에 원치않는 공백이 앞에 온다. 세미콜론과 공백으로 쪼개면 어떻게 될까?\n```{python}\n# display-authors-2.py\n# (키값, 저자명) 짝을 화면에 출력한다.\n\n# -*- coding: utf-8 -*-\n\nimport sys\nimport csv\n\nraw = open(sys.argv[1], 'r', encoding='utf-8')\nreader = csv.reader(raw);\n\ntry:\n    for line in reader:\n      key, authors = line[0], line[3]\n      for auth in authors.split('; '): # 세미콜론 대신에, 세미콜론과 공백 사용\n          print (key, auth)\nexcept BrokenPipeError:\n  sys.stderr.close()\n          \nraw.close()\n```\n8SW85SQM McClelland, James L\n85QV9X5F McClelland, J. L.\n85QV9X5F McNaughton, B. L.\n85QV9X5F O'Reilly, R. C.\nZ4X6DT6N Ratcliff, R.\nF5DGU3Q4 McCloskey, M.\nF5DGU3Q4 Cohen, N. J.\nPNGQMCP5 Buciluǎ, Cristian\nPNGQMCP5 Caruana, Rich\nPNGQMCP5 Niculescu-Mizil, Alexandru\n\n12.3.1 버전 관리\n이로써 데이터 추출의 첫 번째 단계가 완료되어, 재사용 가능한 유용한 코드를 얻었기 때문에, 후속 작업을 위해서 저장한다. 깃(Git) 버전 제어는 데이터 처리 및 분석의 복잡성과 변화에 대응하는 데 있어 핵심적인 역할을 한다. ETL 과정은 데이터의 추출, 변환 및 로드 과정에서 수많은 쿼리와 스크립트를 포함하며, 이러한 작업들은 지속적으로 수정 및 개선이 필요하다. Git을 사용하면 이러한 변경사항을 효과적으로 추적하고 관리할 수 있다. 오류 발생시 이전 버전으로 쉽게 되돌릴 수 있게 해주며, 팀원 간의 협업에서도 변경 사항을 쉽게 공유하고 통합할 수 있게 한다. 또한, Git은 작업의 히스토리를 기록하여 프로젝트의 진행 상황을 명확하게 파악할 수 있도록 해주어 프로젝트 관리에도 큰 도움이 된다.\ngit init . 명령어를 사용하여 현재 디렉토리에서 새로운 Git 저장소를 초기화하는데 .git 폴더를 생성하여 Git 관련 데이터를 저장할 수 있는 토대를 만든다. 다음 단계로 git add -A 명령어는 작업 디렉토리의 모든 변경사항(새로운 파일, 수정된 파일 등)을 Git 스테이징 영역에 추가하여 커밋할 파일을 준비한다. git status 명령어는 현재 Git 저장소의 상태를 보여주는데 스테이징 영역에 추가된 변경사항, 커밋되지 않은 변경사항 등이 포함된 것이 확인된다. git commit -m \"메시지\" 명령어는 스테이징 영역에 추가된 변경사항을 저장소의 이력에 기록한다. -m 옵션 뒤에는 커밋에 대한 설명을 추가한다. 예시로 “Extracting (key, author) pairs from bibliography”라는 메시지와 함께 커밋이 이루어졌으며, 6개의 파일이 변경되었고 2996개의 추가되었다.\n$ git init .\n\nInitialized empty Git repository in /Users/aturing/lessons/capstone-novice-spreadsheet-biblio/.git\n\n$ git add -A\n$ git status\n\nOn branch master\n\nInitial commit\n\nChanges to be committed:\n  (use \"git rm --cached &lt;file&gt;...\" to unstage)\n\n  new file:   code/count-lines.py\n  new file:   code/display-authors-1.py\n  new file:   code/display-authors-2.py\n  new file:   code/display-fields.py\n  new file:   code/read-fields.py\n  new file:   data/bibliography.csv\n\n$ git commit -m \"Extracting (key, author) pairs from bibliography\"\n\n[master (root-commit) 9db78ed] Extracting (key, author) pairsfrom bibliography\n 6 files changed, 2996 insertions(+)\n create mode 100644 code/count-lines.py\n create mode 100644 code/display-authors-1.py\n create mode 100644 code/display-authors-2.py\n create mode 100644 code/display-fields.py\n create mode 100644 code/read-fields.py\n create mode 100644 data/bibliography.csv",
    "crumbs": [
      "활용사례",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>ETL 데이터베이스</span>"
    ]
  },
  {
    "objectID": "etl.html#자료-변환",
    "href": "etl.html#자료-변환",
    "title": "12  ETL 데이터베이스",
    "section": "12.4 자료 변환",
    "text": "12.4 자료 변환\n정규화(normalization)는 데이터를 여러 테이블로 분할하는 과정으로 전체 데이터셋을 여러 테이블로 분할하는 것으로 이해하면 된다. 다만, 기억해야 할 몇 가지 규칙이 있다.\n\n다중 값 속성은 사용하지 않는다!\n모든 행은 그 행을 고유하게 식별하는 “키”를 가져야 한다.\n모든 속성은 오직 키와만 관련되어야 한다.\n\n시간이 있다면 서지학 데이터에 대한 정규화 방법을 면밀히 검토할 수 있지만 빠르고 간단한 해결책에 집중해보자.\n앞선 데이터 추출 작업으로 짝(키값, 저자명) 데이터를 갖게 되었다. 다음 단계는 관계형 데이터베이스에 데이터를 삽입하는 것이다. 데이터가 데이터베이스에 입력되면, 쿼리를 전송해서 질의 응답을 할 수 있다. 시작점으로 이전 학습과정에서 나온 최종 스크립트를 바탕으로 시작해보자.\n```{python}\n# display-authors-2.py\n# (키값, 저자명) 짝을 화면에 출력한다.\n\n# -*- coding: utf-8 -*-\n\nimport sys\nimport csv\n\nraw = open(sys.argv[1], 'r', encoding='utf-8')\nreader = csv.reader(raw);\n\ntry:\n    for line in reader:\n      key, authors = line[0], line[3]\n      for auth in authors.split('; '): # 세미콜론 대신에, 세미콜론과 공백 사용\n          print (key, auth)\nexcept BrokenPipeError:\n  sys.stderr.close()\n          \nraw.close()\n```\n상기 프로그램 실행결과는 다음과 같다:\n8SW85SQM McClelland, James L\n85QV9X5F McClelland, J. L.\n85QV9X5F McNaughton, B. L.\n85QV9X5F O'Reilly, R. C.\nZ4X6DT6N Ratcliff, R.\nF5DGU3Q4 McCloskey, M.\nF5DGU3Q4 Cohen, N. J.\nPNGQMCP5 Buciluǎ, Cristian\nPNGQMCP5 Caruana, Rich\nPNGQMCP5 Niculescu-Mizil, Alexandru\n데이터베이스에 넣을 수 있는 CSV 파일을 생성해보자.\n```{python}\n# convert-1.py\n# 데이터베이스에 로드할 수 있는 CSV로 출력결과 전송\n\n# -*- coding: utf-8 -*-\n\nimport sys\nimport csv\n\noutput_rows=[]\n\nwith open(sys.argv[1], 'r') as raw:\n    reader = csv.reader(raw);\n    for line in reader:\n        key, authors = line[0], line[3]\n        for auth in authors.split('; '): # 세미콜론 대신에, 세미콜론과 공백 사용\n            output_rows.append([key, auth])\n\n# 출력 파일에 대해 두 번째 인수가 필요\nwith open(sys.argv[2], 'w') as csvout:\n    writer = csv.writer(csvout) # 두 번째 인수로 csv 파일 생성\n    writer.writerow([\"Key\", \"Author\"]) # 칼럼명 헤더 생성\n    writer.writerows(output_rows) # output_rows를 파일에 쓰기\n\nprint(len(output_rows)) # rows output_rows 행수를 알아야 데이터베이스에 확실하게 적재되었는지 확인 가능.\n```\n상기 프로그램을 실행하면, 다음과 같은 결과를 얻게 된다. 6,587개 키-저자 쌍이 존재함을 확인하고 데이터베이스에 적재할 준비가 되었다.\n$ python code/convert-db-1.py data/bibliography.csv data/key_author.csv\n6587\n$ head key_author.csv\nKey,Author\n8SW85SQM,\"McClelland, James L\"\n85QV9X5F,\"McClelland, J. L.\"\n85QV9X5F,\"McNaughton, B. L.\"\n85QV9X5F,\"O'Reilly, R. C.\"\nZ4X6DT6N,\"Ratcliff, R.\"\nF5DGU3Q4,\"McCloskey, M.\"\nF5DGU3Q4,\"Cohen, N. J.\"\nPNGQMCP5,\"Buciluǎ, Cristian\"\nPNGQMCP5,\"Caruana, Rich\"\n\n\n\n\n\n\n절차 생략(Cutting Corners)\n\n\n\n파이썬과 다른 언어들은 데이터베이스와 상호작용하기 위한 라이브러리를 가지고 있음에도 불구하고, 왜 이렇게 대량으로 데이터를 로드하는 방식을 사용하는 것일까? 그런데, 왜 INSERT 문장을 SQLite 안으로 흘려보내지 않는가? 대답은 지금까지 소개한 모든 도구를 사용하는 단순한 해답이 있기 때문이다. 만약 데이터로 좀더 복잡한 어떤 것을 해야한다면, 거의 확실히 import sqlite3 사용해서 작업을 수행하게 된다.",
    "crumbs": [
      "활용사례",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>ETL 데이터베이스</span>"
    ]
  },
  {
    "objectID": "etl.html#적재-load",
    "href": "etl.html#적재-load",
    "title": "12  ETL 데이터베이스",
    "section": "12.5 적재 (Load)",
    "text": "12.5 적재 (Load)\n이제 데이터를 적재하기 위해 스크립트를 만들어 load_bibliography.sql 파일을 호출한다. key_author.csv 파일이 현재 작업디렉토리 아래 data 폴더 아래 저장되어 있기 대문에 다음과 같이 SQL 코드를 작성한다.\n```{sql}\n# load_bibliography.sql\n.mode csv\n.import key_author.csv key_author\n\n.header on\n.mode column\n\nSELECT *\n  FROM key_author\n LIMIT 10;\n\nSELECT count(*)\n  FROM key_author;\n```\nload_bibliography.sql 파일을 실행하게 되면 다음 결과를 얻게 된다.\n$ sqlite3\nSQLite version 3.34.1 2021-01-20 14:10:07\nEnter \".help\" for usage hints.\nConnected to a transient in-memory database.\nUse \".open FILENAME\" to reopen on a persistent database.\n\nsqlite&gt; .read code/load_bibliography.sql\nKey       Author\n--------  --------------------------\n8SW85SQM  McClelland, James L\n85QV9X5F  McClelland, J. L.\n85QV9X5F  McNaughton, B. L.\n85QV9X5F  O'Reilly, R. C.\nZ4X6DT6N  Ratcliff, R.\nF5DGU3Q4  McCloskey, M.\nF5DGU3Q4  Cohen, N. J.\nPNGQMCP5  Buciluǎ, Cristian\nPNGQMCP5  Caruana, Rich\nPNGQMCP5  Niculescu-Mizil, Alexandru\ncount(*)\n--------\n6587\nsqlite&gt; .quit",
    "crumbs": [
      "활용사례",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>ETL 데이터베이스</span>"
    ]
  },
  {
    "objectID": "etl.html#쿼리-작성",
    "href": "etl.html#쿼리-작성",
    "title": "12  ETL 데이터베이스",
    "section": "12.6 쿼리 작성",
    "text": "12.6 쿼리 작성\n\n12.6.1 다작 저자\n가장 활발히 논문을 저술한 저자를 찾아보자. SQL 쿼리문을 작성해서 가장 빈도수가 높은 저자를 찾아 상위 10명을 추려보자.\n```{sql}\nsqlite&gt; SELECT author, count(*)\n   ...&gt; FROM key_author\n   ...&gt; GROUP BY author\n   ...&gt; ORDER BY count(*) desc\n   ...&gt; LIMIT 10;\n```\n그동안 노력이 결실을 맺은 첫번째 결과로, 단 하나의 명령으로 가장 많이 저술한 저자들을 알아낼 수 있다는 것이다. 두번째로 알게 된 것은 작업이 아직 끝나지 않았다는 점이다. “Bengio, Yoshua”와 “Bengio, Y.”는 거의 확실히 동일 인물이며, “LeCun, Yann”과 “LeCun, Y.”도 마찬가지다. 정말로 누가 가장 많은 논문을 썼는지 알고 싶다면, 동일 인물에 대한 식별작업을 별도로 수행해야 한다.\nAuthor                 count(*)\n---------------------  --------\nBengio, Yoshua         122\nBengio, Y.             111\nHinton, Geoffrey E.    78\nLeCun, Yann            56\nHinton, G. E.          45\nSalakhutdinov, Ruslan  34\nLeCun, Y.              31\nVincent, Pascal        29\nJordan, M. I.          27\nFrasconi, P.           25\n\n\n\n\n\n\n비정규화(Denormalization)는 많은 악의 뿌리가 된다.\n\n\n\n데이터가 표준을 따르고, 어떤 군더더기도 없다면 정규화된 것이다. 예제 데이터는 비정규화(denormalized) 적절한 사례다. 이유는 특정 저자명이 몇가지 다른 방식으로 표현되었다. 경험적(heuristics, 휴리스틱)으로 데이터를 정규화할 수 있다. 예를 들어, “만약 성과 다른 이름의 첫부분이 매칭되면, 동일인으로 간주한다.”와 같은 방식이다. 하지만, 오류가 항상 끼어들 여지가 있다. 예제 데이터에서, “Albar, M.”이 Mohammd Albar 혹은 Michael Albar인지 어느 것이 맞는지 알 수가 없다. 따라서, 경헙적으로 정규화된 데이터에 기초한 대답은 항상 현실에 대한 근사다. 이런 경우에 사용한 경험적 방법과 직접적으로 수행한 변환에 대해 기록하는 것이 매우 중요하다. 그렇게 해서 다른 사람(미래의 본인 자신을 포함해서)이 작업한 결과물을 상호검사할 수 있게 한다.\n\n\n\n\n12.6.2 공저자\n파이썬 데이터 전처리 단계의 일부로 이름을 정규화하고 데이터를 정제하는 작업은 별도로 하지 않고, 다른 질문에 답할 수 있는지 살펴봅시다. 먼저, 공저자로 가장 많이 참여한 저자를 찾아보자.\n```{sql}\nSELECT a.author, b.author \n  FROM key_author a \n  JOIN key_author b USING(key) \n WHERE a.author &gt; b.author\n LIMIT 10;\n```\nAuthor             Author           \n-----------------  -----------------\nMcNaughton, B. L.  McClelland, J. L.\nO'Reilly, R. C.    McClelland, J. L.\nO'Reilly, R. C.    McNaughton, B. L.\nMcCloskey, M.      Cohen, N. J.     \nCaruana, Rich      Buciluǎ, Cristian\nNiculescu-Mizil,   Buciluǎ, Cristian\nNiculescu-Mizil,   Caruana, Rich    \nRigamonti, Robert  Fua, Pascal      \nRigamonti, Robert  Lepetit, Vincent \nSironi, Amos       Fua, Pascal \n(a.author &gt; b.author를 사용하여 각기 다른 저자 쌍이 한 번만 나타나도록 한다.) 만약 다른 저자들이 얼마나 자주 함께 작업했는지 알고 싶다면 어떻게 해야 할까?\n```{sql}\nSELECT a.author, b.author, count(*)\n  FROM key_author a \n  JOIN key_author b USING(key) \n WHERE a.author &gt; b.author\n GROUP BY a.author, b.author\n ORDER BY count(*) desc\nlimit 10;\n```\nAuthor           Author          count(*)  \n---------------  --------------  ----------\nVincent, Pascal  Bengio, Yoshua  27        \nRoux, Nicolas L  Bengio, Yoshua  20        \nDelalleau, Oliv  Bengio, Yoshua  19        \nBengio, Y.       Bengio, S.      18        \nLarochelle, Hug  Bengio, Yoshua  15        \nRoux, Nicolas L  Delalleau, Oli  15        \nVincent, P.      Bengio, Y.      15        \nChapados, N.     Bengio, Y.      14        \nGori, M.         Frasconi, P.    14        \nSalakhutdinov,   Hinton, Geoffr  14  \n다시 한번, 우리는 데이터 정규화 문제에 직면하고 있다: “Vincent, Pascal”과 “Bengio, Yoshua” 쌍은 거의 확실히 “Vincent, P.”와 “Bengio, S.” 쌍과 동일할 것이다. 하지만 이 문제는 수작업으로 데이터를 분석할 때에도 존재했으며, 데이터베이스에 데이터를 넣음으로써 새로운 질문들을 쉽게 물어볼 수 있게 되어, 그렇지 않았다면 다룰 수 없었을 연구를 진행할 수 있게 되었다. 마지막 단계는 작업한 스크립트를 Git에 커밋하고 커피 한잔 하면서 자축할 시간만 남았다.",
    "crumbs": [
      "활용사례",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>ETL 데이터베이스</span>"
    ]
  },
  {
    "objectID": "etl.html#다른-접근방법",
    "href": "etl.html#다른-접근방법",
    "title": "12  ETL 데이터베이스",
    "section": "12.7 다른 접근방법",
    "text": "12.7 다른 접근방법\n\n12.7.1 R 프로그래밍\n난이도가 있는 복잡한 데이터의 경우 ETL 과정을 통해 데이터베이스에 넣고 SQL 쿼리를 사용하여 원하는 정보를 추출하는 것도 가능하지만 R 프로그래밍을 사용하여 동일한 작업을 수행하는 것도 가능하다. tidyverse 라이브러리를 사용하여 bibliography.csv 데이터를 읽고 처리하는 방식은 다음과 같다. 먼저, read_csv 함수로 파일을 불러오고, janitor::clean_names()로 칼럼명을 정리한 후, select와 set_names로 원하는 칼럼을 선택하고 칼럼명을 지정한다. 그런 다음 mutate와 str_split로 저자 이름을 분할하고, unnest로 이를 펼친 후, count로 각 key와 author 조합의 빈도를 계산하고 필요없는 열을 제거한다. 이후 inner_join을 사용하여 같은 데이터 프레임을 자기 자신과 조인하고, filter로 특정 조건을 만족하는 행을 필터링한다. group_by와 summarise로 그룹별로 집계하고, ungroup, arrange, top_n을 통해 결과를 정렬하고 상위 10개의 결과를 추출한다. 작성된 코드를 통해 저자들 간의 공동 작업 빈도를 분석하여 가장 자주 협업한 저자 쌍을 찾을 수 있다.\n\nlibrary(tidyverse)\n\nbiblio &lt;-  read_csv(\"data/bibliography.csv\", col_names = FALSE )\n\nbiblio_tbl &lt;- biblio |&gt; \n  janitor::clean_names() |&gt; \n  select(x1:x5) |&gt; \n  set_names(c(\"key\", \"jounral\", \"year\", \"author\", \"affiliation\"))\n\nkey_author &lt;- biblio_tbl |&gt; \n  mutate(author = str_split(author, \"; \")) |&gt; \n  unnest(author) |&gt; \n  count(key, author) |&gt; \n  select(-n)\n\nkey_author %&gt;%\n  inner_join(key_author, by = \"key\", suffix = c(\".a\", \".b\")) %&gt;%\n  filter(author.a &gt; author.b) %&gt;%\n  group_by(author.a, author.b) %&gt;%\n  summarise(count = n()) %&gt;%\n  ungroup() %&gt;%\n  arrange(desc(count)) %&gt;%\n  top_n(10, count)\n\n# A tibble: 10 × 3\n   author.a              author.b            count\n   &lt;chr&gt;                 &lt;chr&gt;               &lt;int&gt;\n 1 Vincent, Pascal       Bengio, Yoshua         27\n 2 Roux, Nicolas Le      Bengio, Yoshua         20\n 3 Delalleau, Olivier    Bengio, Yoshua         19\n 4 Bengio, Y.            Bengio, S.             18\n 5 Larochelle, Hugo      Bengio, Yoshua         15\n 6 Roux, Nicolas Le      Delalleau, Olivier     15\n 7 Vincent, P.           Bengio, Y.             15\n 8 Chapados, N.          Bengio, Y.             14\n 9 Gori, M.              Frasconi, P.           14\n10 Salakhutdinov, Ruslan Hinton, Geoffrey E.    14\n\n\n12.7.2 쉘 프로그래밍\n쉘 기반 파이썬 프로그래밍으로 SQL 데이터베이스를 제작하는 방식은 명령줄 인터페이스(CLI)를 통해 파이썬 스크립트를 실행하여 데이터베이스와 테이블을 생성하고 데이터를 조작하는 것이다. 파이썬 sqlite3 라이브러리를 사용하여 데이터베이스 파일을 생성하고 연결을 설정한다. 연결 객체를 사용하여 커서 객체를 생성하고, SQL 명령어를 커서를 통해 실행하여 데이터베이스 테이블을 생성하고 데이터를 삽입한다. 예를 들어, CREATE TABLE SQL 명령어로 테이블을 생성하고, INSERT INTO 명령어로 데이터를 삽입한다. 모든 작업이 끝나면 데이터베이스 연결을 커밋하고 닫아서 변경사항을 저장한다. 이러한 방식을 통해 쉘 환경에서 파이썬 스크립트를 실행하여 데이터베이스를 제어하고 데이터를 처리함으로써, 데이터베이스 작업을 자동화하고 프로그래밍적으로 접근할 수 있는 장점을 제공한다.\nCSV 파일에서 행을 데이터베이스 레코드로 SQL INSERT 문장 통해 삽입한다. 먼저, SQL INSERT 코드를 다음과 같이 작성한다.\nINSERT INTO data VALUES ('8SW85SQM', 'McClelland, James L');\nINSERT INTO data VALUES ('85QV9X5F', 'McClelland, J. L.');\nINSERT INTO data VALUES ('85QV9X5F', 'McNaughton, B. L.');\n그래서, 대신에 상기 데이터를 출력하도록 프로그램을 변경하자:\n```{python}\n# convert-1.py\n# 데이터베이스에 (키값, 저자명) 짝을 집어넣는 SQL 문장을 생성한다.\n# -*- coding: utf-8 -*-\n\nimport sys\nimport csv\n\nINSERT = \"INSERT INTO data VALUES('{0}', '{1}');\"\n\nraw = open(sys.argv[1], 'r', encoding='utf-8')\n\nreader = csv.reader(raw);\n\nfor line in reader:\n  key, authors = line[0], line[3]\n  for auth in authors.split('; '): # 세미콜론 대신에, 세미콜론과 공백 사용\n    print(INSERT.format(key, auth))\n        \nraw.close()\n```\n첫번째 변경사항이 INSERT 정의로, 삽입 문장에 대한 형식 문자열(format string)이 된다. 두번째 변경사항은 키값과 저자명을 직접적으로 출력하는 대신에, 데이터 값을 str.format 을 사용하는 INSERT 안으로 삽입하는 것이다.\n잘 동작한다, 하지만 “동작한다”라는 말은 단지 “분명한 오류없이 작업완료가 되도록 동작한다”라는 의미다. 더 가까이 검사하면, 문제가 두가지가 보인다:\n\n실제로 어떤 누구도 데이터를 삽입하는데 데이터베이스를 생성해주지는 않는다.\n저자명에 단일 인용부호를 포함할 수 있다.\n\n첫번째 문제는 풀기 쉽다. 프로그램 시작부분에 다음 코드를 추가한다.\nCREATE = 'CREATE TABLE data(key text not null, author text not null);'\n어떤 insert 문장을 출력하기 전에 출력한다. 두번째 문제는 더 까다롭다: 만약 “O’Mear, Fiona” 같은 저자명을 INSERT해서 끼워넣으려면, 결과가 다음과 같이 된다:\n\"INSERT INTO data VALUES('RJS8QDC4', 'O'Mear, Fiona');\"\n상기 방식은 적법한 파이썬 방법이 아니다. 문제 해결방식은 단일 인용부호 대신에 문자열 주위를 이중 인용부호를 사용하는 것이다. 왜냐하면 사람 이름에 이중 인용부호는 포함될 수 없기 때문이다. 변경사항을 마치고 나면, 전체 프로그램은 다음과 같다:\n```{python}\n# convert-2.py\n# 키값과 저자명에 대한 데이터베이스 생성.\n# -*- coding: utf-8 -*-\n\nimport sys\nimport csv\n\nCREATE = 'CREATE TABLE data(key text not null, author text not null);'\nINSERT = 'INSERT INTO data VALUES(\"{0}\", \"{1}\");'\n\nprint(CREATE)\n\nraw = open(sys.argv[1], 'r')\nreader = csv.reader(raw);\nfor line in reader:\n    key, authors = line[0], line[3]\n    for auth in authors.split('; '): # 세미콜론 대신에, 세미콜론과 공백 사용\n        print INSERT.format(key, auth)\nraw.close()\n```\n프로그램을 실행해보자:\n$ python code/convert-2.py data/bibliography.csv | head -5\nCREATE TABLE data(key text not null, author text not null);\nINSERT INTO data VALUES(\"8SW85SQM\", \"McClelland, James L\");\nINSERT INTO data VALUES(\"85QV9X5F\", \"McClelland, J. L.\");\nINSERT INTO data VALUES(\"85QV9X5F\", \"McNaughton, B. L.\");\nINSERT INTO data VALUES(\"85QV9X5F\", \"O'Reilly, R. C.\");\n결과가 상당히 좋아 보인다. 그래서, 실제 데이터베이스를 생성하는데 이것을 사용하기로 한다:\n$ python code/convert-2.py data/bibliography.csv | sqlite3 bibliography.db\n상기 파이프라인 작업은 저자 컴퓨터에서 실행하는데 약 4초 걸렸고, 205 킬로바이트 bibliography.db 파일을 생성했다. 데이터베이스가 담고 있는 것을 살펴보자:\n$ sqlite3 bibliography.db\nSQLite version 3.8.5 2014-08-15 22:37:57\nEnter \".help\" for usage hints.\n\nsqlite&gt; .schema\nCREATE TABLE data(key text not null, author text not null);\n\nsqlite&gt; SELECT * FROM data LIMIT 10;\n8SW85SQM|McClelland, James L\n85QV9X5F|McClelland, J. L.\n85QV9X5F|McNaughton, B. L.\n85QV9X5F|O'Reilly, R. C.\nZ4X6DT6N|Ratcliff, R.\nF5DGU3Q4|McCloskey, M.\nF5DGU3Q4|Cohen, N. J.\nPNGQMCP5|Buciluǎ, Cristian\nPNGQMCP5|Caruana, Rich\nPNGQMCP5|Niculescu-Mizil, Alexandru\n결과가 좋아보인다. 그래서, 질의를 던져보자:\nSELECT author, COUNT(*)\nFROM data\nGROUP BY author\nORDER BY COUNT(*) DESC\nLIMIT 10;\n\nBengio, Yoshua|122\nBengio, Y.|111\nHinton, Geoffrey E.|78\nLeCun, Yann|56\nHinton, G. E.|45\nSalakhutdinov, Ruslan|34\nLeCun, Y.|31\nVincent, Pascal|29\nJordan, M. I.|27\nFrasconi, P.|25\n첫번째로 보이는 것은 프로그램 작업이 성과를 내고 있다는 것이다: 누가 가장 다작하는 저자인지 명령문 하나로 이제는 알아낼 수 있다. 두번째로 보이는 것이 아직 작업을 완수한 것은 아니라는 것이다: “Bengio, Yoshua”와 “Bengio, Y.”는 거의 확실히 동일한 사람이다. 마찬가지로 “LeCun, Yann”와 “LeCun, Y.”도 동일인이다. 정말로 누가 가장 많은 논문을 썼는지 알아내려고 한다면, 동일인에 대한 다른 이름을 맞출 필요가 있다.\n저자명을 정규화하는 대신에, 대답할 수 있는 다른 질문을 살펴보자. 누가 누구와 공동으로 논문을 썼을까?\nSELECT a.author, b.author\nFROM data a\nJOIN data b ON a.key = b.key AND a.author &gt; b.author\nLIMIT 10;\n\n\nMcNaughton, B. L.|McClelland, J. L.\nO'Reilly, R. C.|McClelland, J. L.\nO'Reilly, R. C.|McNaughton, B. L.\nMcCloskey, M.|Cohen, N. J.\nCaruana, Rich|Buciluǎ, Cristian\nNiculescu-Mizil, Alexandru|Buciluǎ, Cristian\nNiculescu-Mizil, Alexandru|Caruana, Rich\nRigamonti, Roberto|Fua, Pascal\nRigamonti, Roberto|Lepetit, Vincent\nSironi, Amos|Fua, Pascal\n(a.author &gt; b.author 을 사용하게 되면, 완전히 다른 저자명 짝이 한번만 나오게 한다.) 다른 저자 짝이 얼마나 자주 함께 논문을 작성했는지 알고자 한다면 어떨까?\nselect a.author, b.author, count(*)\nfrom data a join data b\non a.key=b.key and a.author &gt; b.author\ngroup by a.author, b.author\norder by count(*) desc\nlimit 10;\n\nVincent, Pascal|Bengio, Yoshua|27\nRoux, Nicolas Le|Bengio, Yoshua|20\nDelalleau, Olivier|Bengio, Yoshua|19\nBengio, Y.|Bengio, S.|18\nLarochelle, Hugo|Bengio, Yoshua|15\nRoux, Nicolas Le|Delalleau, Olivier|15\nVincent, P.|Bengio, Y.|15\nChapados, N.|Bengio, Y.|14\nGori, M.|Frasconi, P.|14\nSalakhutdinov, Ruslan|Hinton, Geoffrey E.|14",
    "crumbs": [
      "활용사례",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>ETL 데이터베이스</span>"
    ]
  },
  {
    "objectID": "etl.html#db-debugging",
    "href": "etl.html#db-debugging",
    "title": "12  ETL 데이터베이스",
    "section": "12.8 디버깅",
    "text": "12.8 디버깅\nSQLite 데이터베이스에 연결하는 파이썬 프로그램을 개발할 때 하나의 일반적인 패턴은 파이썬 프로그램을 실행하고 SQLite 데이터베이스 브라우저를 통해서 결과를 확인하는 것이다. 브라우저를 통해서 빠르게 프로그램이 정상적으로 작동하는지를 확인할 수 있다.\nSQLite에서 두 프로그램이 동시에 동일한 데이터를 변경하지 못하기 때문에 주의가 필요하다. 예를 들어, 브라우저에서 데이터베이스를 열고 데이터베이스에 변경을 하고 “저장(save)”버튼을 누르지 않는다면, 브라우져는 데이터베이스 파일에 “락(lock)”을 걸구, 다른 프로그램이 파일에 접근하는 것을 막는다. 특히, 파일이 잠겨져 있으면 작성하고 있는 파이썬 프로그램이 파일에 접근할 수 없다.\n해결책은 데이터베이스가 잠겨져 있어서 파이썬 코드가 작동하지 않는 문제를 피하도록 파이썬에서 데이터베이스에 접근하려 시도하기 전에 데이터베이스 브라우져를 닫거나 혹은 File 메뉴를 사용해서 브라우져 데이터베이스를 닫는 것이다.",
    "crumbs": [
      "활용사례",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>ETL 데이터베이스</span>"
    ]
  },
  {
    "objectID": "etl.html#연습문제",
    "href": "etl.html#연습문제",
    "title": "12  ETL 데이터베이스",
    "section": "연습문제",
    "text": "연습문제\n\n작업을 올바른 방식으로 수행하기\nprint 문장 대신에, sqlite3 라이브러리를 사용해서, 데이터베이스를 생성하도록 파이썬 프로그램을 다시 작성하시오.\n\n\n고유한 쌍\na.author &gt; b.author 을 사용하게 되면 왜 완전히 다른 저자명 짝이 한번만 나타나게 되는지 설명하시오.\n\n\n데이터 정제\n입력값으로 저자명 두명을 받아서 만약 아마도 동일인이면 True를 반환하고 만약 동일인이 아니라면 False를 반환하는 함수를 작성하시오. 작성한 함수를 옆사람과 비교하고, 두명이 불일치하는 사례를 찾을 수 있는가?\n\n\n데이터 정제 (계속)\n지금까지 작성한 함수를 재활용하여 저자명을 정규화하시오. 작업 결과를 옆사람과 비교하시오. 정확하게 저자명과 동일한 목록을 만들어 냈는지 확인하시고, 만약 그렇지 못하다면, 상응하는 목록을 만들어 냈는지 확인하세요.",
    "crumbs": [
      "활용사례",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>ETL 데이터베이스</span>"
    ]
  },
  {
    "objectID": "case_dvd.html#about-postgreSQL",
    "href": "case_dvd.html#about-postgreSQL",
    "title": "15  DVD 대여 데이터베이스",
    "section": "15.1 PostgreSQL 1 2",
    "text": "15.1 PostgreSQL 1 2\nPostgreSQL은 확장 가능성 및 표준 준수를 강조하는 객체-관계형 데이터베이스 관리 시스템(ORDBMS)의 하나로 BSD 라이선스로 배포되며 오픈소스 개발자 및 관련 회사들이 개발에 참여하고 있다. 소규모의 단일 머신 애플리케이션에서부터 수많은 동시 접속 사용자가 있는 대형의 인터넷 애플리케이션(또는 데이터 웨어하우스용)에 이르기까지 여러 부하를 관리할 수 있으며 macOS 서버의 경우 PostgreSQL은 기본 데이터베이스로 상용 오라클 데이터베이스를 대체하는 오픈소스 데이터베이스로 알려져 있다.\n\n15.1.1 PostgreSQL 설치 3\nPostgreSQL: The World’s Most Advanced Open Source Relational Database 웹사이트에서 PostgreSQL 다운로드 한다. 윈도우에 설치하는 경우 다음을 참고한다. 설치과정에서 나중에 도움이 될만한 정보는 다음과 같다.\n\n설치 디렉토리: C:\\Program Files\\PostgreSQL\\16\n포트: 5432\n사용자명: postgres\n\n\n\n\npostgreSQL 설치\n\n\nPostgreSQL 16 → SQL Shell (psql)을 클릭한 후에 postgreSQL 헬로월드를 찍어본다. 설치과정에서 등록한 비번만 넣어주고 나머지는 로컬호스트와 기본 디폴트 설정된 데이터베이스를 사용할 것이라 postgres 사용자 비밀번호만 넣어준다. 그리고 나서 postgre=# 쉘에 SELECT version() 명령어를 넣어준다.\n\n\n\npostgreSQL 헬로월드\n\n\n\n\n15.1.2 예제 데이터베이스 - pagila 4\nPostgreSQL Sample Database를 Github에서 구해서 설치하거나, PostgreSQL Sample Database, Load PostgreSQL Sample Database을 참조하여 DVD 대여 데이터베이스를 설치한다.\n\nSQL Shell (psql) 쉘을 실행하여 dvd 데이터베이스를 생성한다.\n\n\nDatabase [postgres]:\nPort [5432]:\nUsername [postgres]:\npostgres 사용자의 암호:\npsql (11.5)\n도움말을 보려면 \"help\"를 입력하십시오.\n\npostgres=# CREATE DATABASE dvdrental;\nCREATE DATABASE\n\n\nWindows + R 단축키를 실행시켜 cmd를 입력하여 윈도우 쉘을 구동시킨다. 그리고 postgreSQL을 설치한 윈도우 디렉토리로 이동한다. C:\\Program Files\\PostgreSQL\\11\\bin 디렉토리가 된다. 그리고 나서 다운받은 dvdrental.zip 파일 압축을 풀어 dvdrental.tar을 지정한다.\n\n\npg_restore 명령어는 데이터베이스를 생성시키는 역할을 한다.\n-U postgres 인자는 사용자를 지정한다.\n-d dvd 인자는 데이터베이스를 지정한다.\nC:\\dvdrental\\dvdrental.tar 인자는 파일로 저장된 데이터베이스 정보를 담고 있다.\n\nC:\\Program Files\\PostgreSQL\\16\\bin&gt; pg_restore -U postgres -d dvdrental C:\\dvdrental\\dvdrental.tar\n암호:\nC:\\Program Files\\PostgreSQL\\11\\bin&gt;\n\n\n\npostgreSQL DVD 데이터베이스\n\n\n\n\n15.1.3 DVD 대여 질의문 작성 5\ndvd 데이터베이스가 설치되었기 때문에 쿼리를 던지기 위해서는 postgreSQL 데이터베이스에 접속을 해야한다. 이를 위해서 pgAdmin 4를 실행시키게 되면 웹브라우져에 웹인터페이스가 생기게 된다. 데이터베이스를 dvd로 지정하고 하고 나서, Tools → Query Tool을 클릭하게 되면 해당 데이터베이스 테이블에 쿼리를 던질 수가 있게 된다.\n\n\n\npostgreSQL select 쿼리문 예시\n\n\n\n데이터베이스 사용자 추가\npostgres 사용자는 이미 존재하기 때문에 별도로 tidyverse 사용자를 추가하고 권한을 부여한다. \\du 명령어로 사용자가 정상 등록되었는지 확인한다.\n\npostgres=# create user tidyverse with encrypted password '1234';\nCREATE ROLE\npostgres=# grant all privileges on database dvd to tidyverse;\nGRANT\npostgres=# \\du\n                                 롤 목록\n  롤 이름  |                      속성                      | 소속 그룹:\n-----------+------------------------------------------------+------------\n postgres  | 슈퍼유저, 롤 만들기, DB 만들기, 복제, RLS 통과 | {}\n tidyverse |                                                | {}\n\n\n\nR에서 postgreSQL 연결\npostgreSQL DBMS 내부에 dvd 데이터베이스가 생성되었다. 이를 R에서 작업하기 위해서 RPostgreSQL, DBI 팩키지를 도입한다. dbConnect() 함수에 데이터베이스와 연결에 필요한 모든 정보를 저장시킨다. 그리고 나서 dbGetQuery() 함수로 쿼리를 던져 원하는 결과를 받아온다.\n\nlibrary(tidyverse)\nlibrary(RPostgreSQL)\n\npgdrv &lt;- dbDriver(\"PostgreSQL\")\n\nrental_con &lt;- DBI::dbConnect(pgdrv, \n                             dbname=\"dvdrental\", \n                             host=\"localhost\",\n                             port=\"5432\", \n                             user=\"postgres\", \n                             password=Sys.getenv(\"POSTGRES_PASSWORD\"))\n\nactor &lt;- dbGetQuery(rental_con, \"SELECT * FROM actor LIMIT 5\")\n\n# DBI::dbDisconnect(rental_con)\n\ndbGetQuery()로 가져온 데이터프레임을 dplyr 동사로 후속작업을 진행한다.\n\nlibrary(tidyverse)\n\nactor %&gt;% \n  filter(actor_id ==1)\n\n  actor_id first_name last_name         last_update\n1        1   Penelope   Guiness 2013-05-26 14:47:57\n\n\n작업에 필요한 테이블 찾기\n데이터베이스에서 쿼리 작업을 수행할 때 가장 먼저 해야 되는 일중의 하나가 적합한 테이블을 찾는 것이다. 이를 위해서 각 DBMS마다 나름대로 정리를 해둔 메타테이블이 존재한다. postgreSQL의 경우는 pg_catalog.pg_tables가 된다. 가장 많이 사용되는 SQL 데이터베이스별로 동일한 사안에 대해서 찾아보자.\n\npostgreSQL: SELECT * FROM pg_catalog.pg_tables;\nsqlite3: .tables\nMS SQL 서버 - Transact-SQL: SELECT * FROM INFORMATION_SCHEMA.TABLES;\nMySQL: SHOW TABLES;\n\n\nqry &lt;- \"SELECT *\n        FROM pg_catalog.pg_tables\"\n\ndbGetQuery(rental_con, qry) %&gt;% \n  filter(schemaname == 'public') \n\n   schemaname     tablename tableowner tablespace hasindexes hasrules hastriggers rowsecurity\n1      public         actor   postgres       &lt;NA&gt;       TRUE    FALSE        TRUE       FALSE\n2      public         store   postgres       &lt;NA&gt;       TRUE    FALSE        TRUE       FALSE\n3      public       address   postgres       &lt;NA&gt;       TRUE    FALSE        TRUE       FALSE\n4      public      category   postgres       &lt;NA&gt;       TRUE    FALSE        TRUE       FALSE\n5      public          city   postgres       &lt;NA&gt;       TRUE    FALSE        TRUE       FALSE\n6      public       country   postgres       &lt;NA&gt;       TRUE    FALSE        TRUE       FALSE\n7      public      customer   postgres       &lt;NA&gt;       TRUE    FALSE        TRUE       FALSE\n8      public    film_actor   postgres       &lt;NA&gt;       TRUE    FALSE        TRUE       FALSE\n9      public film_category   postgres       &lt;NA&gt;       TRUE    FALSE        TRUE       FALSE\n10     public     inventory   postgres       &lt;NA&gt;       TRUE    FALSE        TRUE       FALSE\n11     public      language   postgres       &lt;NA&gt;       TRUE    FALSE        TRUE       FALSE\n12     public        rental   postgres       &lt;NA&gt;       TRUE    FALSE        TRUE       FALSE\n13     public         staff   postgres       &lt;NA&gt;       TRUE    FALSE        TRUE       FALSE\n14     public       payment   postgres       &lt;NA&gt;       TRUE    FALSE        TRUE       FALSE\n15     public          film   postgres       &lt;NA&gt;       TRUE    FALSE        TRUE       FALSE\n\n\n테이블 별 칼럼명\n다음으로 테이블을 찾았다고 하면, 해당되는 칼럼명을 찾을 수 있어야 한다. 이를 통해서 유의미한 의미를 찾아낼 수 있는데 칼럼명을 통해 영감을 받아 다가설 수 있게 된다.\n\ncol_qry &lt;- \"SELECT table_name,\n                   STRING_AGG(column_name, ', ') AS columns\n            FROM information_schema.columns\n            WHERE table_schema = 'public'\n            GROUP BY table_name;\"\n\ndbGetQuery(rental_con, col_qry) %&gt;% \n  filter(table_name %in% c( \"actor\", \"rental\", \"store\"))\n  \n# DBI::dbDisconnect(con)\n\n  table_name                                                                               columns\n1      actor                                          actor_id, last_update, first_name, last_name\n2     rental rental_id, rental_date, inventory_id, customer_id, return_date, staff_id, last_update\n3      store                                   store_id, manager_staff_id, address_id, last_update\n\n\nDVD ER 다이어그램\n후속 쿼리 분석 작업을 위해서 도움이 되는 ER 다이어그램은 다음과 같다.\n\n\n\nDVD ER 다이어그램",
    "crumbs": [
      "활용사례",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>DVD 대여 데이터베이스</span>"
    ]
  },
  {
    "objectID": "case_dvd.html#dvd-insight-rental",
    "href": "case_dvd.html#dvd-insight-rental",
    "title": "15  DVD 대여 데이터베이스",
    "section": "15.2 DVD DB 인사이트",
    "text": "15.2 DVD DB 인사이트\nDVT 대여 데이터베이스를 설치했다면 다음 단계로 다양한 SQL 쿼리문을 던져 뭔가 가치 있는 정보를 추출해야만 한다. 데이터 과학: “postgreSQL - DVD 대여 데이터베이스”에서 데이터베이스 설치와 접속에 대한 사항은 확인한다.\n\n15.2.1 DB 접속 헬로월드 6\n먼저 DBI::dbConnect()를 통해 접속하고 SQL 쿼리 헬로월드를 던져보자.\n\nlibrary(tidyverse)\nlibrary(RPostgreSQL)\n\npgdrv &lt;- dbDriver(\"PostgreSQL\")\n\nrental_con &lt;- DBI::dbConnect(pgdrv, \n                 dbname=\"dvdrental\", \n                 host=\"localhost\",\n                 port=\"5432\", \n                 user=\"postgres\", \n                 password=Sys.getenv(\"POSTGRES_PASSWORD\"))\n\nactor &lt;- dbGetQuery(rental_con, \"SELECT * FROM actor LIMIT 5\")\n\nactor\n\n  actor_id first_name    last_name         last_update\n1        1   Penelope      Guiness 2013-05-26 14:47:57\n2        2       Nick     Wahlberg 2013-05-26 14:47:57\n3        3         Ed        Chase 2013-05-26 14:47:57\n4        4   Jennifer        Davis 2013-05-26 14:47:57\n5        5     Johnny Lollobrigida 2013-05-26 14:47:57\n\n\n15.2.2 이탈/잔존고객 구매금액\ncustomer 테이블에는 active 칼럼을 통해 잔존고객과 이탈고객을 파악할 수 있다. 이를 통해서 잔존고객과 이탈고객이 몇명이고 구매금액을 파악할 수 있다. 먼저 datamodelr 팩키지를 통해 해당 테이블을 뽑아내서 이를 시각화해보자.\n\nlibrary(tidyverse)\nlibrary(datamodelr)\n\npayment &lt;- tbl(rental_con, \"payment\") %&gt;% collect()\ncustomer &lt;- tbl(rental_con, \"customer\") %&gt;% collect()\n\npayment_customer_model &lt;- dm_from_data_frames(payment, customer)\n\npayment_customer_model &lt;- dm_add_references(\n  payment_customer_model,\n  customer$customer_id ==  payment$customer_id\n)\n\npayment_customer_graph &lt;- dm_create_graph(payment_customer_model, rankdir = \"LR\", col_attr = c(\"column\", \"type\"))\ndm_render_graph(payment_customer_graph)\n\n\n\n\n테이블 구조 시각화 - 구매금액\n\n\ncon을 통해 DVD 대여 데이터베이스에 접속이 이루어진 상태다. 이탈고객과 잔존고객별로 구매금액에 대한 평균, 최소, 최대, 총합계를 구하려면 두 테이블을 INNER JOIN으로 customer_id를 키값으로 합치고 나서 기술통계를 산출한다.\n\nsql_query &lt;- \n\"SELECT active, \n       COUNT(*) AS num_active, \n       MIN(amount) AS min_amt, \n       AVG(amount) AS avg_amt,\n       MAX(amount) AS max_amt, \n       SUM(amount) AS total_amt\nFROM payment AS p\nINNER JOIN customer AS c\n  ON p.customer_id = c.customer_id\nGROUP BY c.active;\"\n\ndbGetQuery(con, sql_query)\n\n  active num_active min_amt  avg_amt max_amt total_amt\n1      0        369    0.99 4.092981   11.99   1510.31\n2      1      14227    0.00 4.203397   11.99  59801.73\n\n\n15.2.3 쟝르별 평균 대여평점\n앞서와 마찬가지로 쟝르별 평균 대여평점을 계산할 수 있는 테이블을 쭉 뽑아본다. 이를 통해서 3개 테이블, 즉 category, film_category, film을 뽑아놓고 각 해당 키값을 사용하여 결합시킨다.\n\ncategory &lt;- tbl(rental_con, \"category\") %&gt;% collect()\nfilm_category &lt;- tbl(rental_con, \"film_category\") %&gt;% collect()\nfilm &lt;- tbl(rental_con, \"film\") %&gt;% collect()\n\nrental_rating_model &lt;- dm_from_data_frames(category, film_category, film)\n\nrental_rating_model &lt;- dm_add_references(\n  rental_rating_model,\n  category$category_id == film_category$category_id,\n  film_category$film_id == film$film_id\n)\n\nrental_rating_graph &lt;- dm_create_graph(rental_rating_model, rankdir = \"LR\", col_attr = c(\"column\", \"type\"))\ndm_render_graph(rental_rating_graph)\n\n\n\n\n테이블 구조 시각화 - 쟝르별 대여평점\n\n\n먼저 film_category와 category를 결합시켜 영화(film)가 속한 쟝르(category)를 파악한다.\n\nrate_qry &lt;- \n\"SELECT * \nFROM category AS c\nINNER JOIN film_category AS fc\n  ON c.category_id = fc.category_id\nLIMIT 5;\"\n\ndbGetQuery(rental_con, rate_qry)\n\n  category_id        name         last_update film_id category_id..5      last_update..6\n1           6 Documentary 2006-02-15 09:46:27       1              6 2006-02-15 10:07:09\n2          11      Horror 2006-02-15 09:46:27       2             11 2006-02-15 10:07:09\n3           6 Documentary 2006-02-15 09:46:27       3              6 2006-02-15 10:07:09\n4          11      Horror 2006-02-15 09:46:27       4             11 2006-02-15 10:07:09\n5           8      Family 2006-02-15 09:46:27       5              8 2006-02-15 10:07:09\n다음으로 film 테이블을 조인하여 rental_rate를 결합하고 쟝르(category) 별로 평균평점을 구하고 이를 ORDER BY ... DESC를 사용해서 내림차순으로 정렬한다.\n\nrate_qry &lt;- \n\"SELECT c.name,\n        AVG(rental_rate) AS avg_rental_rate\nFROM category AS c\nINNER JOIN film_category AS fc\n  ON c.category_id = fc.category_id \nINNER JOIN film AS f\n  ON fc.film_id = f.film_id\nGROUP BY c.category_id\nORDER BY avg_rental_rate DESC;\"\n\ndbGetQuery(rental_con, rate_qry)\n\n          name avg_rental_rate\n1        Games        3.252295\n2       Travel        3.235614\n3       Sci-Fi        3.219508\n4       Comedy        3.162414\n5       Sports        3.125135\n6          New        3.116984\n7      Foreign        3.099589\n8       Horror        3.025714\n9        Drama        3.022258\n10       Music        2.950784\n11    Children        2.890000\n12   Animation        2.808182\n13      Family        2.758116\n14    Classics        2.744386\n15 Documentary        2.666471\n16      Action        2.646250\n\n\n15.2.4 Top 10 DVD 영화\n가장 많이 대여된 Top 10 DVD 영화를 찾아내기 위해서 이에 해당되는 연관 테이블을 검색하여 찾아낸다. film, inventory, rental 테이블을 특정하고 서로 연결시킬 수 있는 키값을 찾아 연결시킨다.\n\nfilm &lt;- tbl(rental_con, \"film\") %&gt;% collect()\ninventory &lt;- tbl(rental_con, \"inventory\") %&gt;% collect()\nrental &lt;- tbl(rental_con, \"rental\") %&gt;% collect()\n\ntop_10_model &lt;- dm_from_data_frames(film, inventory, rental)\n\ntop_10_model &lt;- dm_add_references(\n  top_10_model,\n  film$film_id == inventory$film_id,\n  inventory$inventory_id == rental$inventory_id\n)\n\ntop_10_graph &lt;- dm_create_graph(top_10_model, rankdir = \"LR\", col_attr = c(\"column\", \"type\"))\ndm_render_graph(top_10_graph)\n\n\n\n\n테이블 구조 시각화 - Top 10 DVD 영화\n\n\nfilm → inventory → rental 테이블을 순차적으로 film_id, inventory_id를 키값으로 삼아 결합시킨다. 그리고 나서 가장 많이 대여된 영화를 찾기 위해서 COUNT() 함수로 개수하고 나서 이를 내림차순 정리한다.\n\ntop_query &lt;- \n\"SELECT f.title AS movie_title, \n        COUNT(f.title) AS num_rentals\nFROM film AS f\nINNER JOIN inventory AS i\n  ON f.film_id = i.film_id\nINNER JOIN rental AS r\n  ON i.inventory_id = r.inventory_id\nGROUP BY f.title\nORDER BY num_rentals DESC;\"\n\ndbGetQuery(rental_con, top_query) %&gt;% \n  slice_max(n=10, order_by = num_rentals)\n\n           movie_title num_rentals\n1   Bucket Brotherhood          34\n2     Rocketeer Mother          33\n3       Juggler Hardly          32\n4  Ridgemont Submarine          32\n5        Scalawag Duck          32\n6       Grit Clockwork          32\n7       Forward Temple          32\n8       Timberland Sky          31\n9            Zorro Ark          31\n10        Robbers Joon          31\n11        Hobbit Alien          31\n12        Network Peak          31\n13       Apache Divine          31\n14     Rush Goodfellas          31\n15           Wife Turn          31\n16   Goodfellas Salute          31",
    "crumbs": [
      "활용사례",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>DVD 대여 데이터베이스</span>"
    ]
  },
  {
    "objectID": "case_dvd.html#db-summary",
    "href": "case_dvd.html#db-summary",
    "title": "15  DVD 대여 데이터베이스",
    "section": "15.3 요약",
    "text": "15.3 요약\n이번 장은 파이썬에서 데이터베이스 사용 기본적인 개요에 대해 폭넓게 다루었다. 데이터를 저장하기 위해서 파이썬 딕셔너리나 일반적인 파일보다 데이터베이스를 사용하여 코드를 작성하는 것이 훨씬 복잡하다. 그래서, 만약 작성하는 응용프로그램이 실질적으로 데이터베이스 역량을 필요하지 않는다면 굳이 데이터베이스를 사용할 이유는 없다. 데이터베이스가 특히 유용한 상황은 (1) 큰 데이터셋에서 작은 임의적인 갱신이 많이 필요한 응용프로그램을 작성할 때 (2) 데이터가 너무 커서 딕셔너리에 담을 수 없고 반복적으로 정보를 검색할 때, (3) 한번 실행에서 다음 실행 때까지 데이터를 보관하고, 멈추고, 재시작하는데 매우 긴 실행 프로세스를 갖는 경우다.\n많은 응용프로그램 요구사항을 충족시키기 위해서 단일 테이블로 간단한 데이터베이스를 구축할 수 있다. 하지만, 대부분의 문제는 몇개의 테이블과 서로 다른 테이블간에 행이 연결된 관계를 요구한다. 테이블 사이 연결을 만들 때, 좀더 사려깊은 설계와 데이터베이스의 역량을 가장 잘 사용할 수 있는 데이터베이스 정규화 규칙을 따르는 것이 중요하다. 데이터베이스를 사용하는 주요 동기는 처리할 데이터의 양이 많기 때문에, 데이터를 효과적으로 모델링해서 프로그램이 가능하면 빠르게 실행되게 만드는 것이 중요하다.",
    "crumbs": [
      "활용사례",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>DVD 대여 데이터베이스</span>"
    ]
  },
  {
    "objectID": "advanced_db.html",
    "href": "advanced_db.html",
    "title": "16  SQL 데이터셋",
    "section": "",
    "text": "17 survey.db",
    "crumbs": [
      "챗GPT SQL",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>SQL 데이터셋</span>"
    ]
  },
  {
    "objectID": "advanced_db.html#sqlite3와-duckdb",
    "href": "advanced_db.html#sqlite3와-duckdb",
    "title": "16  SQL 데이터셋",
    "section": "16.1 Sqlite3와 DuckDB",
    "text": "16.1 Sqlite3와 DuckDB\nSQLite3와 DuckDB는 각각 OLTP(Online Transaction Processing)와 OLAP(Online Analytical Processing) 관점으로 살펴보면, SQLite3는 OLTP에 더 적합한 데이터베이스로, 애플리케이션 거래 처리(transaction)를 위한 운영 데이터 관리에 사용되고, 빠른 쓰기와 읽기, 낮은 지연 시간, 높은 트랜잭션 보장이 필요한 환경에 맞게 설계되었다. 특히, SQLite는 가벼운 구조와 서버리스 아키텍처로 인해 임베디드 시스템, 모바일 애플리케이션, 소규모 웹 애플리케이션에서 데이터를 저장하고 관리하는 데 자주 사용된다.\n반면, DuckDB는 OLAP에 더 적합하며 데이터 분석 작업에 최적화된 데이터베이스로, 큰 규모 데이터셋에 대한 복잡한 쿼리와 데이터 분석을 빠르게 수행할 수 있다. DuckDB의 벡터화 쿼리 처리와 멀티 코어 시스템에서 병렬 처리 능력은 대용량 데이터 분석, 보고서 생성, 데이터 마이닝과 같은 작업에 탁월한 성능을 제공한다. DuckDB는 주로 데이터 과학자들과 분석가들이 사용하는 도구로써, 빅데이터의 효율적인 처리와 복잡한 분석 쿼리를 빠르게 실행시킬 수 있다.",
    "crumbs": [
      "챗GPT SQL",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>SQL 데이터셋</span>"
    ]
  },
  {
    "objectID": "advanced_db.html#시카고-범죄-데이터",
    "href": "advanced_db.html#시카고-범죄-데이터",
    "title": "16  SQL 데이터셋",
    "section": "16.2 시카고 범죄 데이터",
    "text": "16.2 시카고 범죄 데이터\n\n2021년\n2022년\n2023년\n\n\nlibrary(tidyverse)\nlibrary(duckdb)\n\ncrime_csv &lt;- fs::dir_ls(\"data/crime/\")\n\ncrime_raw &lt;- crime_csv |&gt; \n  enframe(value = \"filepath\") |&gt; \n  select(-name) |&gt; \n  mutate(data = map(filepath, read_csv)) |&gt; \n  mutate(year = str_extract(filepath, \"\\\\d{4}\")) |&gt; \n  select(year, data) \n\ncrime_tbl &lt;- crime_raw |&gt; \n  unnest(data) |&gt; \n  janitor::clean_names()\n\n  \ncrime_tbl |&gt; \n  glimpse()\n#&gt; Rows: 707,153\n#&gt; Columns: 23\n#&gt; $ year                 &lt;chr&gt; \"2021\", \"2021\", \"2021\", \"2021\", \"2021\", \"2021\", \"…\n#&gt; $ id                   &lt;dbl&gt; 12342615, 26262, 13209581, 13209369, 12374520, 13…\n#&gt; $ case_number          &lt;chr&gt; \"JE202211\", \"JE366265\", \"JG422927\", \"JG422777\", \"…\n#&gt; $ date                 &lt;chr&gt; \"04/17/2021 03:20:00 PM\", \"09/08/2021 04:45:00 PM…\n#&gt; $ block                &lt;chr&gt; \"081XX S PRAIRIE AVE\", \"047XX W HARRISON ST\", \"01…\n#&gt; $ iucr                 &lt;chr&gt; \"0325\", \"0110\", \"1563\", \"1153\", \"0486\", \"1153\", \"…\n#&gt; $ primary_type         &lt;chr&gt; \"ROBBERY\", \"HOMICIDE\", \"SEX OFFENSE\", \"DECEPTIVE …\n#&gt; $ description          &lt;chr&gt; \"VEHICULAR HIJACKING\", \"FIRST DEGREE MURDER\", \"CR…\n#&gt; $ location_description &lt;chr&gt; \"RESIDENCE\", \"CAR WASH\", \"APARTMENT\", \"RESIDENCE\"…\n#&gt; $ arrest               &lt;lgl&gt; TRUE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FA…\n#&gt; $ domestic             &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, F…\n#&gt; $ beat                 &lt;chr&gt; \"0631\", \"1131\", \"0411\", \"0915\", \"1123\", \"1412\", \"…\n#&gt; $ district             &lt;chr&gt; \"006\", \"011\", \"004\", \"009\", \"011\", \"014\", \"017\", …\n#&gt; $ ward                 &lt;dbl&gt; 6, 24, 8, 11, 28, 35, 45, 25, 25, 20, 9, 44, 28, …\n#&gt; $ community_area       &lt;dbl&gt; 44, 25, 45, 60, 27, 21, 16, 31, 31, 40, 73, 6, 28…\n#&gt; $ fbi_code             &lt;chr&gt; \"03\", \"01A\", \"17\", \"11\", \"08B\", \"11\", \"11\", \"17\",…\n#&gt; $ x_coordinate         &lt;dbl&gt; 1179448, 1144907, NA, NA, 1154131, NA, NA, NA, NA…\n#&gt; $ y_coordinate         &lt;dbl&gt; 1851073, 1896933, NA, NA, 1900784, NA, NA, NA, NA…\n#&gt; $ year_2               &lt;dbl&gt; 2021, 2021, 2021, 2021, 2021, 2021, 2021, 2021, 2…\n#&gt; $ updated_on           &lt;chr&gt; \"09/14/2023 03:41:59 PM\", \"09/14/2023 03:41:59 PM…\n#&gt; $ latitude             &lt;dbl&gt; 41.74663, 41.87319, NA, NA, 41.88358, NA, NA, NA,…\n#&gt; $ longitude            &lt;dbl&gt; -87.61803, -87.74345, NA, NA, -87.70948, NA, NA, …\n#&gt; $ location             &lt;chr&gt; \"(41.746626309, -87.618031954)\", \"(41.873191445, …\n\n\ncrime_tbl |&gt; \n  count(primary_type, sort = TRUE) |&gt; \n  head(10) |&gt; \n  ggplot(aes(x = fct_reorder(primary_type, n), y = n)) +\n    geom_col() +\n    coord_flip() +\n    labs(x = \"범죄 유형\", y = \"범죄 건수\", title = \"시카고 범죄 유형별 건수\")\n\n\n\n\n\n\n\n\nDuckDB는 Analytical SQLite라는 별명을 갖고 있다. SQLite가 OLTP 데이터베이스라면, DuckDB OLAP 데이터베이스라고 볼 수 있다.\n\ncon_dd &lt;- duckdb::dbConnect(duckdb::duckdb())\n\nduckdb::dbWriteTable(con_dd, \"crime_tbl\", crime_tbl, overwrite = TRUE)\n\nduckdb::dbListTables(con_dd)\n#&gt; [1] \"crime_tbl\"\n\ndbGetQuery(con_dd, \"SELECT COUNT(*) AS '범죄수' FROM crime_tbl\")\n#&gt;   범죄수\n#&gt; 1 707153\n\n\ntime &lt;- function(call) {\n  print(system.time(call())[[1]])\n}\n\ntime(\\() duckdb::dbWriteTable(con_dd, \"crime_tbl\", crime_tbl, overwrite = TRUE))\n#&gt; [1] 0.71\n\ntime(\\() dbGetQuery(con_dd, \"SELECT COUNT(*) AS '범죄수' FROM crime_tbl\"))\n#&gt; [1] 0\n\n\nlibrary(RSQLite)\n\ncon_sqlite &lt;- dbConnect(RSQLite::SQLite())\n\ntime(\\() RSQLite::dbWriteTable(con_sqlite, \"crime_tbl\", crime_tbl, overwrite = TRUE))\n#&gt; [1] 1.75\n\n\ncrime_schema &lt;- DBI::dbGetQuery(con_dd, 'SELECT * FROM duckdb_tables;')\n\ncrime_schema |&gt; \n  select(sql)\n#&gt;                                                                                                                                                                                                                                                                                                                                                                                                                                                       sql\n#&gt; 1 CREATE TABLE crime_tbl(\"year\" VARCHAR, id DOUBLE, case_number VARCHAR, date VARCHAR, block VARCHAR, iucr VARCHAR, primary_type VARCHAR, description VARCHAR, location_description VARCHAR, arrest BOOLEAN, domestic BOOLEAN, beat VARCHAR, district VARCHAR, ward DOUBLE, community_area DOUBLE, fbi_code VARCHAR, x_coordinate DOUBLE, y_coordinate DOUBLE, year_2 DOUBLE, updated_on VARCHAR, latitude DOUBLE, longitude DOUBLE, \"location\" VARCHAR);\n\n\ncomplex_query &lt;- \"\n  SELECT \n      primary_type,\n      ROUND(AVG(latitude), 5) AS avg_latitude,\n      ROUND(AVG(longitude), 5) AS avg_longitude,\n      SUM(CASE WHEN arrest THEN 1 ELSE 0 END) AS total_arrests,\n      COUNT(*) AS total_crimes,\n      year\n  FROM \n      crime_tbl\n  WHERE \n      year &gt;= '2021' AND year &lt;= '2023'\n  GROUP BY \n      primary_type, year\n  HAVING \n      COUNT(*) &gt; 100\n  ORDER BY \n      total_crimes DESC, primary_type\n  LIMIT 5;\n\"\n\ndbGetQuery(con_dd, complex_query)\n#&gt;   primary_type avg_latitude avg_longitude total_arrests total_crimes year\n#&gt; 1        THEFT     41.86707     -87.66762          2738        56770 2023\n#&gt; 2        THEFT     41.86597     -87.66691          2019        54853 2022\n#&gt; 3      BATTERY     41.83902     -87.66873          6726        43958 2023\n#&gt; 4      BATTERY     41.83833     -87.66838          5932        40924 2022\n#&gt; 5        THEFT     41.86354     -87.66865          1552        40806 2021\n\n작성된 SQL 쿼리는 시카고 범죄 데이터베이스에서 2021년부터 2023년까지 가장 흔한 범죄 유형 상위 5가지를 찾고, 이들의 평균 발생 위치, 체포 건수, 연도별 발생 횟수를 파악하는 것이다.\n\nWHERE 절: 범죄 발생 연도를 2021년부터 2023년 사이로 범위를 제한한다.\nGROUP BY 절: 결과를 범죄 유형(primary_type)과 발생 연도(year)별로 그룹화한다.\nROUND(AVG(latitude), 5) 및 ROUND(AVG(longitude), 5): 각 범죄 유형의 평균 위도와 경도를 계산하여 범죄가 일반적으로 발생하는 위치를 파악한다. 평균은 소수점 다섯째 자리까지 반올림된다.\nSUM(CASE WHEN arrest THEN 1 ELSE 0 END) AS total_arrests: 각 범죄 유형별로 발생한 총 체포 건수를 합산하고, arrest가 참일 때마다 1을 더하는 방식으로 계산한다.\nCOUNT(*) AS total_crimes: 각 범죄 유형별로 발생한 총 범죄 건수를 계산한다.\nHAVING COUNT(*) &gt; 100: 100건 이상 발생한 범죄 유형만을 필터링하여 신뢰성 있는 데이터만을 대상으로 한다.\nORDER BY total_crimes DESC, primary_type: 총 범죄 건수가 많은 순으로 결과를 정렬하고, 동일한 건수일 경우 범죄 유형(primary_type)에 따라 정렬한다.\nLIMIT 5: 결과를 상위 5가지 범죄 유형으로 한정한다.\n\n\ntime(\\() dbGetQuery(con_dd, complex_query) )\n#&gt; [1] 0.17\n\ntime(\\() dbGetQuery(con_sqlite, complex_query) )\n#&gt; [1] 0.48",
    "crumbs": [
      "챗GPT SQL",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>SQL 데이터셋</span>"
    ]
  },
  {
    "objectID": "advanced_db.html#프로그래밍",
    "href": "advanced_db.html#프로그래밍",
    "title": "16  SQL 데이터셋",
    "section": "17.1 프로그래밍",
    "text": "17.1 프로그래밍\nGUI 챗팅 인터페이스 대신 프로그래밍을 통해 파악해보자. 자연어로 SQL 프로그래밍 들어가기 전에 기본적인 설정을 다음과 같이 설정한다. 파이썬에서 OpenAI의 GPT 모델을 사용해 사용자 질문에 자동으로 답하는 스크립트를 작성한다.\n\n모듈 가져오기:\n\nos는 운영 체제와 상호작용하고, 환경 변수에 접근하는 데 사용된다.\nopenai는 OpenAI의 Python 클라이언트 라이브러리로, GPT 모델을 사용하는 데 필요하다.\ndotenv는 .env 파일에서 환경 변수를 로드하는 데 사용된다.\n\n환경 변수 로드:\n\nload_dotenv()는 프로젝트 루트의 .env 파일로부터 환경 변수를 로드한다. .env 파일에 저장된 OPENAI_API_KEY 환경변수를 가져온다.\n\nOpenAI 클라이언트 초기화:\n\nOpenAI를 사용해 API 클라이언트를 생성한다.\napi_key=os.getenv('OPENAI_API_KEY')는 환경 변수에서 OPENAI_API_KEY를 가져와 클라이언트를 인증한다.\n\n채팅 완성 생성:\n\nclient.chat.completions.create는 OpenAI의 채팅 완성 API를 사용해 채팅 대화를 생성한다.\nmessages는 사용자의 입력 메시지를 담고 있다. 이 경우 “SQL이 뭔지 간략하게 설명해줘”라는 질문이 포함되어 있다.\nmodel=\"gpt-3.5-turbo\"는 사용할 GPT 모델을 지정한다.\n\n결과 출력:\n\nprint(chat_completion.choices[0].message.content)는 생성된 채팅 대화에서 첫 번째 선택 항목의 메시지 내용을 출력한다. 이는 GPT 모델이 생성한 답변을 보여준다.\n\n\n\nimport os\nfrom openai import OpenAI\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nclient = OpenAI(\n    api_key=os.getenv('OPENAI_API_KEY'),\n)\n\nchat_completion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"SQL이 뭔지 간략하게 설명해줘\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\nprint(chat_completion.choices[0].message.content)\n\nSQL(Structured Query Language)은 데이터베이스 관리 시스템(DBMS)에서 데이터를 관리하고 조작하는 데 사용하는 표준화된 프로그래밍 언어입니다. SQL은 사용자가 데이터베이스에 대한 쿼리를 작성하여 데이터를 검색, 삽입, 수정, 삭제하는 등 데이터베이스에 대한 다양한 작업을 수행할 수 있게 해줍니다. SQL은 간결한 구문과 명확한 명령어를 사용하며, 이를 통해 데이터베이스에 저장된 대량의 데이터를 효율적으로 조작할 수 있습니다. SQL은 관계형 데이터베이스 시스템(RDBMS)에서 주로 사용되며, 사용자의 요구에 따라 데이터를 가져오거나 수정하는 등 다양한 작업을 수행할 수 있으므로 데이터베이스 관리에 필수적인 언어로 사용됩니다.\n\nsql_message = \"\"\"\nYou are an expert in SQL. The following table definitions have been provided to you. Please convert my query into an appropriate SQL statement.\n\nCREATE TABLE Person(\n        ident    text,\n        personal text,\n        family   text\n);\nCREATE TABLE Site(\n        name text,\n        lat  real,\n        long real\n);\nCREATE TABLE Visited(\n        ident integer,\n        site  text,\n        dated text\n);\nCREATE TABLE Survey(\n        taken   integer,\n        person  text,\n        quant   text,\n        reading real\n);\n\nLet's write an SQL query to display the names of scientists on the screen.\n\"\"\"\n\nchat_completion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": sql_message,\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\nprint(chat_completion.choices[0].message.content)\n\nTo write an SQL query to display the names of scientists on the screen, we need to identify what table contains the names of scientists. Based on the given table definitions, it appears that the \"Person\" table contains the relevant information. Therefore, we can use the following SQL statement to achieve the desired result:\n\nSELECT personal, family\nFROM Person;\n\ndef create_message(query):\n\n    class message:\n        def __init__(message, system, user):\n            message.system = system\n            message.user = user\n    \n    system_message = \"\"\"\n    You are an expert in SQL. The following table definitions have been provided to you. Please convert my query into an appropriate SQL statement. \\n\n    \n    CREATE TABLE Person(\n            ident    text,\n            personal text,\n            family   text\n    );\n    CREATE TABLE Site(\n            name text,\n            lat  real,\n            long real\n    );\n    CREATE TABLE Visited(\n            ident integer,\n            site  text,\n            dated text\n    );\n    CREATE TABLE Survey(\n            taken   integer,\n            person  text,\n            quant   text,\n            reading real\n    ); \\n\n    \"\"\"\n\n    user_message = \"No explanation. Write only SQL query that returns - {}\"\n\n    system = system_message\n    user = user_message.format(query)\n\n    m = message(system = system, user = user)\n    \n    return m\n\n\nquery = \"display the names of scientists on the screen\"\n\nmsg = create_message(query = query)\n\n# print(msg.system)\nprint(msg.user)\n\nWrite an SQL query that returns - display the names of scientists on the screen\n\nquery = \"display the names of scientists on the screen\"\nprompt = create_message(query = query)\n\nresponse = client.chat.completions.create(\n    messages = [\n      {\n        \"role\": \"system\",\n        \"content\": prompt.system\n      },\n      {\n        \"role\": \"user\",\n        \"content\": prompt.user\n      }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\nsql = response.choices[0].message.content\n\nprint(sql)\n\n'SELECT personal FROM Person;'\n\ndef translate_sql(query):\n  \n  prompt = create_message(query = query)\n\n  response = client.chat.completions.create(\n      messages = [\n        {\n          \"role\": \"system\",\n          \"content\": prompt.system\n        },\n        {\n          \"role\": \"user\",\n          \"content\": prompt.user\n        }\n      ],\n      model=\"gpt-3.5-turbo\",\n  )\n\n  sql = response.choices[0].message.content\n  \n  return sql\n\ntranslate_sql(query = 'aggregate the hours for each scientist separately')\n\n'SELECT person, SUM(reading) AS total_hours\\nFROM Survey\\nGROUP BY person;'",
    "crumbs": [
      "챗GPT SQL",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>SQL 데이터셋</span>"
    ]
  },
  {
    "objectID": "advanced_db.html#dplyr-연결",
    "href": "advanced_db.html#dplyr-연결",
    "title": "16  SQL 데이터셋",
    "section": "17.2 dplyr 연결",
    "text": "17.2 dplyr 연결",
    "crumbs": [
      "챗GPT SQL",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>SQL 데이터셋</span>"
    ]
  },
  {
    "objectID": "advanced_db.html#r-sql",
    "href": "advanced_db.html#r-sql",
    "title": "16  SQL 데이터셋",
    "section": "17.3 R에서 바라본 SQL",
    "text": "17.3 R에서 바라본 SQL\nNoSQL과 빅데이터가 주목을 받고 있지만, 데이터 분석을 하는 입장에서는 여전히 SQL의 중요성이 크다. SQL에 대한 이해는 데이터 조작의 기본을 다지는 데 필수적이며, 이를 바탕으로 DBI 패키지를 사용해 dplyr과 연결하여 활용하는 방법을 살펴볼 수 있다.\n\n\n\ndplyr, dbi, sqlite3 변환\n\n\nR 개발자들은 데이터 분석을 할 때 dplyr을 사용하고, 데이터베이스 작업을 할 때는 SQL을 사용하는 것이 일반적인 작업 흐름이었다. 이렇게 두 가지 다른 도구를 사용하는 것은 동일한 작업을 중복해서 수행한다는 불편함을 가져왔다. 그러나 dplyr을 다양한 데이터베이스 시스템에 DBI와 odbc와 연결하면 문제를 해결할 수 있다. 즉, dplyr 동사를 직접 사용하여 SQL 작업을 수행할 수 있어 dplyr 동사를 데이터베이스에 직접 던져서 작업을 수행할 수 있는 중간 통역이 제공되어 보다 효율적으로 작업을 진행할 수 있게 된다.\n\n17.3.1 SQL 자료처리\n자료를 원하는 방향으로 처리하기 위해서는 다음과 같은 다양한 기본적인 자료 처리 방법을 조합하여 사용한다.\n\n데이터 선택하기\n정렬과 중복 제거하기\n필터링(filtering)\n새로운 값 계산하기\n결측 데이터 (Missing Data)\n집합 (Aggregation)\n데이터 조합하기 (Combining Data)\n\n\n# 칼럼과 행 선택\nSELECT 칼럼명1, 칼럼명2....칼럼명N\nFROM   테이블명\nWHERE  조건;\n\n# 그룹에 따른 정렬 및 총계(aggregation)\nSELECT SUM(칼럼명)\nFROM   테이블명\nWHERE  조건\nORDER BY 칼럼명 {오름차순|내림차순};\nGROUP BY 칼럼명;\n\n\n\n17.3.2 SQLite와 실습 데이터베이스\n먼저 이론을 이해하는 것과 더블어 예제 데이터베이스를 설치하고 실습을 진행하기 위해서 먼저 명령-라인을 사용하여 어떻게 디렉토리 여기저기 이동하는지와 명령-라인에서 명령문을 어떻게 실행하는지 숙지할 필요가 있다.\n이런 주제와 친숙하지 않다면, 유닉스 쉘(Unix Shell) 학습을 참조한다. 우선, SQLite 데이터베이스가 어떻게 동작하는지 설명을 할 필요가 있다.\n인터랙티브하게 학습을 수행하기 위해서는 설치 방법에 언급된 SQLite 를 참조하여 설치하고, 학습자가 선택한 위치에 “software_carpentry_sql” 디렉토리를 생성한다. 예를 들어,\n\n명령-라인 터미널 윈도우를 연다.\n다음과 같이 명령어를 타이핑해서 디렉토리를 생성한다.\n\n\n$ mkdir ~/swc/sql\n\n\n생성한 디렉토리로 현재 작업 디렉토리를 변경한다.\n\n\n$ cd ~/swc/sql",
    "crumbs": [
      "챗GPT SQL",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>SQL 데이터셋</span>"
    ]
  },
  {
    "objectID": "advanced_db.html#install-sqlite",
    "href": "advanced_db.html#install-sqlite",
    "title": "16  SQL 데이터셋",
    "section": "17.4 SQLite 설치 1",
    "text": "17.4 SQLite 설치 1\n\n\n\n\n\n\n느긋한 계산법\n\n\n\n데이터베이스를 다룰 때, dplyr은 가능한 느긋(laziness)하게 동작한다.\n\n명시적으로 요청하지 않는 한, 데이터를 R 환경으로 바로 가져오지 않는다.\n가능한 마지막 순간까지 작업을 지연시킨다 - 작업하고 싶은 모든 것을 모아 한 단계로 데이터베이스로 보낸다.",
    "crumbs": [
      "챗GPT SQL",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>SQL 데이터셋</span>"
    ]
  },
  {
    "objectID": "advanced_db.html#connect-sqlite",
    "href": "advanced_db.html#connect-sqlite",
    "title": "16  SQL 데이터셋",
    "section": "17.5 SQLite DB 연결/설치 테스트",
    "text": "17.5 SQLite DB 연결/설치 테스트\n생성된 데이터베이스에 연결하기 위해서, 데이터베이스를 생성한 디렉토리 안에서 SQLite를 시작한다. 그래서 ~/swc/sql 디렉토리에서 다음과 같이 타이핑한다.\n\nroot@hangul:~/swc/sql$ sqlite3 survey.db\n\nsqlite3 survey.db 명령문이 데이터베이스를 열고 데이터베이스 명령-라인 프롬프트로 안내한다. SQLite에서 데이터베이스는 플랫 파일(flat file)로 명시적으로 열 필요가 있다. 그리고 나서 SQLite 시작되고 sqlite로 명령-라인 프롬프트가 다음과 같이 변경되어 표시된다.\n\nSQLite version 3.20.0 2017-08-01 13:24:15\nEnter \".help\" for usage hints.\nConnected to a transient in-memory database.\nUse \".open FILENAME\" to reopen on a persistent database.\nsqlite&gt;  \n\n다음 출력결과가 보여주듯이 .databases 명령문으로 소속된 데이터베이스 이름과 파일 목록을 확인한다.\n\nsqlite&gt; .databases\nseq  name             file                                                      \n---  ---------------  ----------------------------------------------------------\n0    main             ~/novice/sql/survey.db\n\n다음과 같이 타이핑해서 필요한 “Person”, “Survey”, “Site” “Visited” 테이블이 존재하는 것을 확인한다. .table의 출력결과는 다음과 같다.\n\nsqlite&gt; .tables\nPerson   Site     Survey   Visited",
    "crumbs": [
      "챗GPT SQL",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>SQL 데이터셋</span>"
    ]
  },
  {
    "objectID": "advanced_db.html#db-connection",
    "href": "advanced_db.html#db-connection",
    "title": "16  SQL 데이터셋",
    "section": "18.1 데이터베이스 연결",
    "text": "18.1 데이터베이스 연결\n가장 먼저 앞에서 생성한 sqlite3 데이터베이스에 R과 연결시킨다. 그리고 연결된 데이터베이스에 들어있는 테이블을 살펴본다.\n\n# 0. 환경설정 -----------------------\n\nlibrary(dbplyr)\nlibrary(tidyverse)\n\n# 1. 데이터 연결 -----------------------\n\nsurvey_con &lt;- DBI::dbConnect(RSQLite::SQLite(), \"data/survey.db\")\ndbListTables(survey_con)\n#&gt; [1] \"Person\"  \"Site\"    \"Survey\"  \"Visited\"",
    "crumbs": [
      "챗GPT SQL",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>SQL 데이터셋</span>"
    ]
  },
  {
    "objectID": "advanced_db.html#dplyr-sql",
    "href": "advanced_db.html#dplyr-sql",
    "title": "16  SQL 데이터셋",
    "section": "18.2 SQL을 직접 던지기",
    "text": "18.2 SQL을 직접 던지기\ndbGetQuery 명령어를 통해 연결된 con을 통해 데이터베이스에 질의문(query)를 직접 던질 수 있다.\n\n# 2. SQL 활용 -----------------------\ndbGetQuery(survey_con, 'SELECT * FROM Person LIMIT 5')\n#&gt;         id  personal   family\n#&gt; 1     dyer   William     Dyer\n#&gt; 2       pb     Frank  Pabodie\n#&gt; 3     lake  Anderson     Lake\n#&gt; 4      roe Valentina  Roerich\n#&gt; 5 danforth     Frank Danforth",
    "crumbs": [
      "챗GPT SQL",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>SQL 데이터셋</span>"
    ]
  },
  {
    "objectID": "advanced_db.html#dplyr-dplyr",
    "href": "advanced_db.html#dplyr-dplyr",
    "title": "16  SQL 데이터셋",
    "section": "18.3 dplyr 동사 활용",
    "text": "18.3 dplyr 동사 활용\ntbl 함수로 con 으로 연결된 데이터베이스의 특정 테이블 “Survey”를 뽑아낸다.\n\n# 3. dplyr 방식 -----------------------\n\nsurvey_df &lt;- tbl(survey_con, \"Survey\")\nhead(survey_df)\n#&gt; # Source:   SQL [6 x 4]\n#&gt; # Database: sqlite 3.41.2 [D:\\tcs\\gpt-sql\\data\\survey.db]\n#&gt;   taken person quant reading\n#&gt;   &lt;int&gt; &lt;chr&gt;  &lt;chr&gt;   &lt;dbl&gt;\n#&gt; 1   619 dyer   rad      9.82\n#&gt; 2   619 dyer   sal      0.13\n#&gt; 3   622 dyer   rad      7.8 \n#&gt; 4   622 dyer   sal      0.09\n#&gt; 5   734 pb     rad      8.41\n#&gt; 6   734 lake   sal      0.05",
    "crumbs": [
      "챗GPT SQL",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>SQL 데이터셋</span>"
    ]
  },
  {
    "objectID": "advanced_db.html#dataframe",
    "href": "advanced_db.html#dataframe",
    "title": "16  SQL 데이터셋",
    "section": "18.4 데이터프레임 변환",
    "text": "18.4 데이터프레임 변환\ntbl 함수로 con 으로 연결된 데이터베이스의 특정 테이블 “Survey”를 뽑아낸 상태는 아직 R에서 작업이 가능한 데이터프레임이 아니라 collect 함수를 활용해서 데이터프레임으로 변환시켜 후속 작업을 R에서 실행한다.\n\n# 4. 데이터프레임 변환 -----------------------\n\nsurvey_df %&gt;% collect() %&gt;% \n    ggplot(aes(x= quant, y=reading)) +\n    geom_boxplot()",
    "crumbs": [
      "챗GPT SQL",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>SQL 데이터셋</span>"
    ]
  },
  {
    "objectID": "advanced_db.html#conversion",
    "href": "advanced_db.html#conversion",
    "title": "16  SQL 데이터셋",
    "section": "18.5 SQL 변환 과정 살펴보기",
    "text": "18.5 SQL 변환 과정 살펴보기\nshow_query 함수를 사용해서 dplyr 동사가 SQL 질의문으로 변환된 상황을 살펴볼 수도 있다.\n\n# 5. 내부 SQL 변환과정 살펴보기 --------------\n\n\nsurvey_df |&gt; \n  filter(quant == \"sal\") |&gt; \n  show_query()\n#&gt; &lt;SQL&gt;\n#&gt; SELECT *\n#&gt; FROM `Survey`\n#&gt; WHERE (`quant` = 'sal')",
    "crumbs": [
      "챗GPT SQL",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>SQL 데이터셋</span>"
    ]
  },
  {
    "objectID": "advanced_db.html#disconnect",
    "href": "advanced_db.html#disconnect",
    "title": "16  SQL 데이터셋",
    "section": "18.6 데이터베이스 연결 끊기",
    "text": "18.6 데이터베이스 연결 끊기\n데이터베이스는 혼자 사용하는 것이 아니라 사용하지 않는 경우 필히 연결시켜 놓은 con을 반듯이 끊어 놓는다.\n\n# 7. 연결 끊기 -----------------------\n\ndbDisconnect(survey_con)\n\n\n\n\n\nPandey, Ainesh. 2023. “Text to SQL Benchmarks and the Current State of the Art”. Medium. https://medium.com/dataherald/text-to-sql-benchmarks-and-the-current-state-of-the-art-63dd3b3943fe.",
    "crumbs": [
      "챗GPT SQL",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>SQL 데이터셋</span>"
    ]
  },
  {
    "objectID": "llm.html#lm-스튜디오-설치",
    "href": "llm.html#lm-스튜디오-설치",
    "title": "17  오픈 LLM 리더보드",
    "section": "17.1 LM 스튜디오 설치",
    "text": "17.1 LM 스튜디오 설치\nLM Studio는 사용자가 다양한 오픈 소스 거대 언어 모델(LLM)을 실험할 수 있는 데스크톱 앱으로 로컬 컴퓨터에서 비용 걱정 없이 LM Studio를 사용해서 OpenAI 호환 로컬 LLM 서버를 업무에 활용할 수 있다. LM Studio에서 오픈소스 LLM 모델을 로컬 서버에 배포하고 OpenAI에서 개발한 openai 패키지를 사용해서 OpenAI API와 동일한 방식으로 로컬 LLM 서버에 접근할 수 있기 때문에 기존 OpenAI API를 가정하고 제작한 소스코드를 재사용할 수 있다는 점에서 매우 유용하다.\n먼저 LM Studio를 설치하기 위해서는 LM Studio 웹사이트에서 운영체제에 맞게 앱을 다운로드 받아서 설치한다. 좌측 돋보기 모양 아이콘을 클릭하여 검색창에 sql을 입력하면 sql을 학습한 LLM 모델을 찾을 수 있고 안내에 따라 다운로드하면 SQL 코드를 생성하는 AI 모델을 로컬 컴퓨터에 설치하게 된다.\n\n\n\nSQL 코드 생성 LLM 검색\n\n\n좌측 메뉴에서 채팅 아이콘을 클릭하면, 다운로드 받은 LLM 모델을 상단 “Select a model to load”에서 골라 로드하면 OpenAI 챗GPT 서비스처럼 대화형으로 SQL 코드를 작성해 나갈 수 있다.\n\n\n\nsqlcoder 34B LLM 모형 채팅\n\n\n좌측 채팅 아이콘 아래 양방향 화살표(↔︎)를 클릭하면 “Local Inference Server” 아래 “Start Server” 버튼이 있고 이를 클릭하면 오픈소스 LLM 모델을 서버로 배포하여 OpenAI 호환 API를 사용하는 것처럼 로컬 LLM 서버에 접근할 수 있게 된다. “Client Code Example”에서 curl, 파이썬(chat, ai assistant, vision) 예제 코드가 주석과 함께 제시되어 있어 이를 참고하여 후속 개발작업에 유용하게 활용할 수 있다.",
    "crumbs": [
      "챗GPT SQL",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>오픈 LLM 리더보드</span>"
    ]
  },
  {
    "objectID": "llm.html#survey.db-쿼리",
    "href": "llm.html#survey.db-쿼리",
    "title": "17  오픈 LLM 리더보드",
    "section": "17.2 survey.db 쿼리",
    "text": "17.2 survey.db 쿼리\n앞서 개발자가 직접 SQL 쿼리를 직접 작성하는 대신 SQL 특화된 LLM 모델을 사용하여 SQL 쿼리를 생성하는 것도 가능하다. 정확도 높은 SQL 쿼리를 작성하기 위해 명확하게 역할도 부여하고 데이터베이스 스키마 정보도 함께 제공하여 LLM 모델이 SQL 쿼리를 생성할 수 있도록 환경을 설정하고 자연어로 원하는 바를 지정하면 SQL 쿼리를 생성하고 관련 설명도 함께 출력하다.\n주목할 점은 4GB 전후 크기를 갖는 SQL LLM은 생각보다 SQL 쿼리 생성이 눈높이에 맞게 되는 것은 아닌 것으로 보이며, 35.86 GB 크기를 갖는 sqlcoder LLM 모델은 SQL 쿼리 생성에 있어서 더욱 정확한 결과를 보여주고 있다. 물론, OpenAI GPT-4와 비교하면 속도를 비롯하여 여러가지 면에서 부족한 것으로 보이지만 가격이 무료이며, 다른 제약조건이 없다는 점을 생각하면 충분히 사용할 만한 수준이라고 생각된다. 또한, 오픈소스 SQL LLM은 지속적으로 발전을 할 것이기 때문에 작년과 비교하여 올해 비약적인 발전이 이뤄질 것으로 예상된다.\n```{python}\n\n# Example: reuse your existing OpenAI setup\nfrom openai import OpenAI\n\n# Point to the local server\nclient = OpenAI(base_url = \"http://localhost:1234/v1\", \n                api_key  = \"not-needed\")\n\nsystem_message = \"\"\"\nYou are an expert in SQL. The following table definitions have been provided to you. Please convert my query into an appropriate SQL statement. \\n\n    \n    CREATE TABLE Person(\n            ident    text,\n            personal text,\n            family   text\n    );\n    CREATE TABLE Site(\n            name text,\n            lat  real,\n            long real\n    );\n    CREATE TABLE Visited(\n            ident integer,\n            site  text,\n            dated text\n    );\n    CREATE TABLE Survey(\n            taken   integer,\n            person  text,\n            quant   text,\n            reading real\n    ); \\n\n\"\"\"    \n\ncompletion = client.chat.completions.create(\n  model=\"local-model\", # this field is currently unused\n  messages=[\n    {\"role\": \"system\", \n    \"content\": system_message},\n    {\"role\": \"user\", \n    \"content\": \"No explanation. Write only SQL query that returns - display the names of scientists\"}\n  ],\n  temperature=0.7,\n)\n\nprint(completion.choices[0].message.content)\n```\nSELECT DISTINCT p.ident FROM person p JOIN visited v ON p.ident = v.ident JOIN survey s ON p.ident = s.person WHERE s.quant ILIKE '%radiation%';",
    "crumbs": [
      "챗GPT SQL",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>오픈 LLM 리더보드</span>"
    ]
  },
  {
    "objectID": "llm.html#sql-데이터셋",
    "href": "llm.html#sql-데이터셋",
    "title": "17  오픈 LLM 리더보드",
    "section": "17.3 SQL 데이터셋",
    "text": "17.3 SQL 데이터셋\n(Pandey 2023)\nGPT-4와 Bard 같은 AI 모델의 자연어 처리 능력이 급격히 향상되면서, 자연어로 질문된 내용에 대한 SQL 쿼리를 생성하는 ’텍스트-투-SQL’과 같은 다양한 NLP 사용 사례에 대한 발전이 가속화되고 있다. 여러 접근 방식과 솔루션이 시장에 나오면서 어느 것이 가장 효율적인지, 어느 것이 정확한 답을 더 신뢰성 있게 생성하는지, 어느 것이 다양한 데이터셋에 가장 잘 적응하는지 평가하는 문제가 생겼다. 이 질문들에 답하기 위해 오픈소스 산업과 학계는 여러 벤치마크를 제시했지만, 오늘날 가장 많이 사용되는 세 가지는 WikiSQL, Spider, BIRD(BIg Bench for LaRge-scale Database Grounded Text-to-SQL Evaluation)이다. WikiSQL은 Salesforce에 의해 2017년 말에 소개된 최초의 대규모 텍스트-투-SQL 데이터 집합이지만, 단순함이라는 큰 단점이 있다. 제공되는 모든 SQL 쿼리는 SELECT, FROM, WHERE 절만을 포함하는 매우 간단하며, 데이터셋 내의 테이블들은 다른 테이블과의 연결이 없다. WikiSQL로 훈련된 모델은 새로운 데이터베이스에서도 작동할 수 있지만, 간단한 자연어 질문에 대해서만 답할 수 있다. 이러한 이유로 최근 텍스트-투-SQL 분야의 연구는 더 복잡한 벤치마크에 초점을 맞추고 있다. 실제로 WikiSQL 리더보드에는 2021년 이전의 제출물만 있으며, 테스트 정확도 90% 이상을 달성한 여러 제출물들이 있지만(가장 성능이 좋은 제출물은 93%에 도달함), 이제 실무자들은 WikiSQL로는 턱없이 부족한 훨씬 더 복잡한 쿼리 생성에 초점을 맞추고 있다.\nSpider 데이터셋은 WikiSQL 데이터셋의 단점 중 일부를 보완하려고 한다. 예일 대학의 11명 학생들이 1,000시간 이상의 노력을 통해 개발된 Spider 데이터셋은 복잡성과 교차 도메인성이라는 두 가지 중요한 요소를 도입한다. 복잡성 측면에서 SQL 쿼리는 WikiSQL이 한정된 간단한 SELECT와 WHERE 절을 넘어서, 더 복잡한 GROUP BY, ORDER BY, HAVING 절과 중첩된 쿼리를 포함한다. 또한, 모든 데이터베이스는 외래 키를 통해 여러 테이블이 연결되어 있어 테이블 간에 조인하는 복잡한 쿼리를 가능하게 한다. 교차 도메인성 측면에서 Spider는 200개의 복잡한 데이터베이스를 많은 도메인에 걸쳐 포함하여, 테스트 세트에서 본 적 없는 데이터베이스를 포함시켜 모델의 일반화 가능성을 테스트할 수 있게 한다.\n\n\n\n\nPandey, Ainesh. 2023. “Text to SQL Benchmarks and the Current State of the Art”. Medium. https://medium.com/dataherald/text-to-sql-benchmarks-and-the-current-state-of-the-art-63dd3b3943fe.",
    "crumbs": [
      "챗GPT SQL",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>오픈 LLM 리더보드</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "참고문헌",
    "section": "",
    "text": "Hermans, Felienne, and Emerson Murphy-Hill. 2015. “Enron’s\nSpreadsheets and Related Emails: A Dataset and Analysis.” In\n37th International Conference on Software Engineering,\nICSE ’15.\n\n\nPandey, Ainesh. 2023. “Text to SQL Benchmarks and the Current\nState of the Art.” Medium. https://medium.com/dataherald/text-to-sql-benchmarks-and-the-current-state-of-the-art-63dd3b3943fe.\n\n\n이광춘, and 신종화. 2023. 챗gpt 유닉스쉘. \"한국 R 사용자회\".",
    "crumbs": [
      "참고문헌"
    ]
  },
  {
    "objectID": "prog.html#파이썬",
    "href": "prog.html#파이썬",
    "title": "10  데이터베이스 프로그래밍",
    "section": "10.1 파이썬",
    "text": "10.1 파이썬\n마무리 하면서, 파이썬 같은 범용 프로그래밍 언어에서 데이터베이스를 어떻게 접근하는지 살펴보자. 다른 언어도 거의 같은 모델을 사용한다. 라이브러리와 함수 이름이 다를지 모르지만, 개념은 동일한다.\nsurvey.db라는 이름의 파일에 저장된 SQLite 데이터베이스에서 위도와 경도를 선택하는 짧은 파이썬 프로그램이 다음에 있다.\n```{python}\nimport sqlite3\n\nconnection = sqlite3.connect(\"survey.db\")\ncursor = connection.cursor()\ncursor.execute(\"SELECT Site.lat, Site.long FROM Site;\")\nresults = cursor.fetchall()\nfor r in results:\n    print(r)\ncursor.close()\nconnection.close()\n```\n(-49.85, -128.57)\n(-47.15, -126.72)\n(-48.87, -123.4)\nsqlite3 라이브러리를 가져오면서 프로그램이 시작된다. MySQL, DB2 또는 다른 데이터베이스에 접속한다면 다른 라이브러리를 사용할 것이지만, 기능은 동일하다. 따라서 데이터베이스 A에서 데이터베이스 B로 변경한다면 프로그램의 다른 부분은 많이 변경할 필요가 없다.\n두 번째 줄은 데이터베이스 연결을 설정한다. SQLite를 사용하므로 필요한 것은 데이터베이스 파일 이름뿐이다. 일반적인 다른 데이터베이스 시스템은 사용자 이름과 비밀번호를 전달해야 한다.\n세 번째 줄은 연결을 통해 커서를 생성한다. 편집기 커서처럼, 데이터베이스의 어느 위치에 있는지를 추적하는 것이 커서의 역할이다.\n네 번째 줄에서 커서를 사용하여 사용자 대신 데이터베이스에 쿼리를 실행하도록 요청한다. 쿼리는 SQL로 작성되며 문자열 형태로 cursor.execute에 전달된다. SQL이 제대로 작성되었는지 확인하는 것은 사용자 귀책이다. 제대로 작성되지 않았거나 실행 중 문제가 발생하면 데이터베이스가 오류를 출력한다.\n다섯 번째 줄에서 cursor.fetchall 호출에 응답하여 데이터베이스가 쿼리 결과를 반환한다. 결과는 각 레코드마다 하나의 항목을 가진 결과집합(result set) 리스트다. 리스트를 반복하여 각 항목을 출력하면(여섯 번째, 일곱 번째 줄), 각각이 각 필드에 하나의 요소를 갖는 튜플(tuple)임을 알 수 있다.\n마지막으로, 여덟 번째와 아홉 번째 줄에서 커서와 데이터베이스 연결을 종료한다. 데이터베이스는 한 번에 열 수 있는 연결과 커서의 수가 제한되어 있기 때문이다. 연결을 설정하는 데 시간이 걸리기 때문에, 연결을 열고 하나의 작업을 수행한 후 연결을 닫았다가 몇 마이크로초 후에 다른 작업을 위해 다시 연결하는 것은 피해야 한다. 대신, 프로그램이 수행되는 동안 하나의 연결을 생성하고 유지하는 것이 일반적이다.\n실제 응용 프로그램에서 쿼리는 사용자가 제공하는 값에 따라 달라진다. 예를 들어, 다음 함수는 사용자 ID를 매개변수로 받아 이름을 반환한다.\n```{python}\nimport sqlite3\n\ndef get_name(database_file, person_id):\n    query = \"SELECT personal || ' ' || family FROM Person WHERE id='\" + person_id + \"';\"\n\n    connection = sqlite3.connect(database_file)\n    cursor = connection.cursor()\n    cursor.execute(query)\n    results = cursor.fetchall()\n    cursor.close()\n    connection.close()\n\n    return results[0][0]\n\nprint(\"Full name for dyer:\", get_name('survey.db', 'dyer'))\n```\nFull name for dyer: William Dyer\n함수의 첫번째 행에 문자열 결함을 사용해서 사용자가 넘겨준 사용자 ID를 포함하는 쿼리를 완성한다. 단순하게 보일지 모르지만, 만약 누군가 다음 문자열을 입력값으로 준다면 무슨 일이 일어날까?\ndyer'; DROP TABLE Survey; SELECT '\n프로젝트 이름 뒤에는 쓰레기(garbage)처럼 보이지만, 매우 주의깊게 고른 쓰레기다. 만약 이 문자열을 쿼리에 삽입하면, 결과는 다음과 같다.\nSELECT personal || ' ' || family FROM Person WHERE id='dyer'; DROP TABLE Survey; SELECT '';\n만약 쿼리를 실행하게 된다면, 데이터베이스에 있는 테이블 중의 하나를 삭제한다.\n이것을 SQL 주입 공격(SQL injection attack)이라고 하며, 수년에 걸쳐 수천 개의 프로그램을 공격하는 데 사용되었다. 특히, 사용자로부터 데이터를 받는 많은 웹사이트들이 값을 쿼리에 직접 삽입하는데, 이를 신중하게 검사하지 않는 경우가 많다.\n악의를 가진 사용자가 다양한 많은 방식으로 쿼리에 명령어를 몰래 밀어넣으려고 한다. 이러한 위협을 다루는 가장 안전한 방식은 인용부호 같은 문자를 대체 상응값으로 대체하는 것이다. 그렇게 해서 안전하게 문자열 내부에 사용자가 입력한 무엇이든지 넣을 수 있다. 문자열로 문장을 작성하는 대신에 준비된 문장(prepared statement)를 사용해서 작업할 수 있다. 만약에 준비된 문장을 사용한다면, 예제 프로그램은 다음과 같다.\nimport sqlite3\n\ndef get_name(database_file, person_id):\n    query = \"SELECT personal || ' ' || family FROM Person WHERE id=?;\"\n\n    connection = sqlite3.connect(database_file)\n    cursor = connection.cursor()\n    cursor.execute(query, [person_id])\n    results = cursor.fetchall()\n    cursor.close()\n    connection.close()\n\n    return results[0][0]\n\nprint(\"Full name for dyer:\", get_name('survey.db', 'dyer'))\nFull name for dyer: William Dyer\n주요 변경사항은 쿼리 문자열과 execute 호출에 있다. 직접 쿼리를 포맷하는 대신, 값을 삽입하고 싶은 쿼리 템플릿에 물음표를 넣는다. execute를 호출할 때, 쿼리에 있는 물음표 수만큼의 값을 포함하는 리스트를 제공한다. 라이브러리는 순서대로 값과 물음표를 매칭하고, 값의 특수 문자를 이스케이프된 대응물로 변환하여 사용하기 안전하게 만든다.\nsqlite3의 커서를 사용하여 데이터베이스에 변경을 가할 수도 있다. 예를 들어, 새로운 이름을 삽입하는 것과 같이. 예를 들어, add_name이라는 새로운 함수를 다음과 같이 정의할 수 있다:\n```{python}\nimport sqlite3\n\ndef add_name(database_file, new_person):\n    query = \"INSERT INTO Person (id, personal, family) VALUES (?, ?, ?);\"\n\n    connection = sqlite3.connect(database_file)\n    cursor = connection.cursor()\n    cursor.execute(query, list(new_person))\n    cursor.close()\n    connection.close()\n\n\ndef get_name(database_file, person_id):\n    query = \"SELECT personal || ' ' || family FROM Person WHERE id=?;\"\n\n    connection = sqlite3.connect(database_file)\n    cursor = connection.cursor()\n    cursor.execute(query, [person_id])\n    results = cursor.fetchall()\n    cursor.close()\n    connection.close()\n\n    return results[0][0]\n\n# Insert a new name\nadd_name('survey.db', ('barrett', 'Mary', 'Barrett'))\n# Check it exists\nprint(\"Full name for barrett:\", get_name('survey.db', 'barrett'))\n```\nIndexError: list index out of range\nSQLite3 버전 2.5 이상에서는 앞서 설명한 get_name 함수가 IndexError: list index out of range 오류를 발생시킨다. add_name을 사용하여 테이블에 Mary의 항목을 추가했음에도 불구하고 발생된다. 이유는 데이터베이스에 변경사항을 저장하기 위해 연결을 종료하기 전에 connection.commit()을 수행해야 하기 때문이다.\n```{python}\nimport sqlite3\n\ndef add_name(database_file, new_person):\n    query = \"INSERT INTO Person (id, personal, family) VALUES (?, ?, ?);\"\n\n    connection = sqlite3.connect(database_file)\n    cursor = connection.cursor()\n    cursor.execute(query, list(new_person))\n    cursor.close()\n    connection.commit()\n    connection.close()\n\n\ndef get_name(database_file, person_id):\n    query = \"SELECT personal || ' ' || family FROM Person WHERE id=?;\"\n\n    connection = sqlite3.connect(database_file)\n    cursor = connection.cursor()\n    cursor.execute(query, [person_id])\n    results = cursor.fetchall()\n    cursor.close()\n    connection.close()\n\n    return results[0][0]\n\n# Insert a new name\nadd_name('survey.db', ('barrett', 'Mary', 'Barrett'))\n# Check it exists\nprint(\"Full name for barrett:\", get_name('survey.db', 'barrett'))\n```\nFull name for barrett: Mary Barrett",
    "crumbs": [
      "SQL 쿼리",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>데이터베이스 프로그래밍</span>"
    ]
  },
  {
    "objectID": "prog.html#r-언어",
    "href": "prog.html#r-언어",
    "title": "10  데이터베이스 프로그래밍",
    "section": "10.3 R 언어",
    "text": "10.3 R 언어\n마무리 하면서, R 같은 범용 프로그래밍 언어에서 데이터베이스를 어떻게 접근하는지 살펴보자. 다른 언어도 거의 같은 모델을 사용한다. 라이브러리와 함수 이름이 다를지 모르지만, 개념은 동일한다.\nsurvey.db라는 이름의 파일에 저장된 SQLite 데이터베이스에서 위도와 경도를 선택하는 짧은 R 프로그램이 다음에 있다.\n```{r}\nlibrary(RSQLite)\nconnection &lt;- dbConnect(SQLite(), \"data/survey.db\")\nresults &lt;- dbGetQuery(connection, \"SELECT Site.lat, Site.long FROM Site;\")\nprint(results)\ndbDisconnect(connection)\n```\n     lat    long\n1 -49.85 -128.57\n2 -47.15 -126.72\n3 -48.87 -123.40\nRSQLite 라이브러리를 가져오면서 프로그램이 시작된다. MySQL, DB2 또는 다른 데이터베이스에 접속한다면 다른 라이브러리를 사용할 것이지만, 기능은 동일하다. 따라서 데이터베이스 A에서 데이터베이스 B로 변경한다면 프로그램의 다른 부분은 많이 변경할 필요가 없다.\n두 번째 줄은 데이터베이스 연결을 설정한다. SQLite를 사용하므로 필요한 것은 데이터베이스 파일 이름뿐이다. 일반적인 다른 데이터베이스 시스템은 사용자 이름과 비밀번호를 전달해야 한다.\n세 번째 줄은 연결을 통해 커서를 생성한다. 편집기 커서처럼, 데이터베이스의 어느 위치에 있는지를 추적하는 것이 커서의 역할이다.\n네 번째 줄에서 커서를 사용하여 사용자 대신 데이터베이스에 쿼리를 실행하도록 요청한다. 쿼리는 SQL로 작성되며 문자열 형태로 cursor.execute에 전달된다. SQL이 제대로 작성되었는지 확인하는 것은 사용자 귀책이다. 제대로 작성되지 않았거나 실행 중 문제가 발생하면 데이터베이스가 오류를 출력한다.\n다섯 번째 줄에서 cursor.fetchall 호출에 응답하여 데이터베이스가 쿼리 결과를 반환한다. 결과는 각 레코드마다 하나의 항목을 가진 결과집합(result set) 리스트다. 리스트를 반복하여 각 항목을 출력하면(여섯 번째, 일곱 번째 줄), 각각이 각 필드에 하나의 요소를 갖는 튜플(tuple)임을 알 수 있다.\n마지막으로, 여덟 번째와 아홉 번째 줄에서 커서와 데이터베이스 연결을 종료한다. 데이터베이스는 한 번에 열 수 있는 연결과 커서의 수가 제한되어 있기 때문이다. 연결을 설정하는 데 시간이 걸리기 때문에, 연결을 열고 하나의 작업을 수행한 후 연결을 닫았다가 몇 마이크로초 후에 다른 작업을 위해 다시 연결하는 것은 피해야 한다. 대신, 프로그램이 수행되는 동안 하나의 연결을 생성하고 유지하는 것이 일반적이다.\n실제 응용 프로그램에서 쿼리는 사용자가 제공하는 값에 따라 달라진다. 예를 들어, 다음 함수는 사용자 ID를 매개변수로 받아 이름을 반환한다.\n```{r}\nlibrary(RSQLite)\n\nconnection &lt;- dbConnect(SQLite(), \"data/survey.db\")\n\ngetName &lt;- function(personID) {\n  query &lt;- paste0(\"SELECT personal || ' ' || family FROM Person WHERE id =='\",\n                  personID, \"';\")\n  return(dbGetQuery(connection, query))\n}\n\nprint(paste(\"full name for dyer:\", getName('dyer')))\n\ndbDisconnect(connection)\n```\nfull name for dyer: William Dyer\n함수의 첫번째 행에 문자열 결함을 사용해서 사용자가 넘겨준 사용자 ID를 포함하는 쿼리를 완성한다. 단순하게 보일지 모르지만, 만약 누군가 다음 문자열을 입력값으로 준다면 무슨 일이 일어날까?\ndyer'; DROP TABLE Survey; SELECT '\n프로젝트 이름 뒤에는 쓰레기(garbage)처럼 보이지만, 매우 주의깊게 고른 쓰레기다. 만약 이 문자열을 쿼리에 삽입하면, 결과는 다음과 같다.\nSELECT personal || ' ' || family FROM Person WHERE id='dyer'; DROP TABLE Survey; SELECT '';\n만약 쿼리를 실행하게 된다면, 데이터베이스에 있는 테이블 중의 하나를 삭제한다.\n이것을 SQL 주입 공격(SQL injection attack)이라고 하며, 수년에 걸쳐 수천 개의 프로그램을 공격하는 데 사용되었다. 특히, 사용자로부터 데이터를 받는 많은 웹사이트들이 값을 쿼리에 직접 삽입하는데, 이를 신중하게 검사하지 않는 경우가 많다.\n악의를 가진 사용자가 다양한 많은 방식으로 쿼리에 명령어를 몰래 밀어넣으려고 한다. 이러한 위협을 다루는 가장 안전한 방식은 인용부호 같은 문자를 대체 상응값으로 대체하는 것이다. 그렇게 해서 안전하게 문자열 내부에 사용자가 입력한 무엇이든지 넣을 수 있다. 문자열로 문장을 작성하는 대신에 준비된 문장(prepared statement)를 사용해서 작업할 수 있다. 만약에 준비된 문장을 사용한다면, 예제 프로그램은 다음과 같다.번\n```{r}\nlibrary(RSQLite)\nconnection &lt;- dbConnect(SQLite(), \"data/survey.db\")\n\ngetName &lt;- function(personID) {\n  query &lt;- \"SELECT personal || ' ' || family FROM Person WHERE id == ?\"\n  return(dbGetPreparedQuery(connection, query, data.frame(personID)))\n}\n\nprint(paste(\"full name for dyer:\", getName('dyer')))\n\ndbDisconnect(connection)\n```\nfull name for dyer: William Dyer\nThe key changes are in the query string and the dbGetQuery call (we use dbGetPreparedQuery instead). Instead of formatting the query ourselves, we put question marks in the query template where we want to insert values. When we call dbGetPreparedQuery, we provide a dataframe that contains as many values as there are question marks in the query. The library matches values to question marks in order, and translates any special characters in the values into their escaped equivalents so that they are safe to use.\n주요 변경사항은 쿼리 문자열과 dbGetQuery 호출에 있다. (대신 dbGetPreparedQuery을 사용 ) 직접 쿼리를 포맷하는 대신, 값을 삽입하고 싶은 쿼리 템플릿에 물음표를 넣는다. dbGetPreparedQuery를 호출할 때, 쿼리에 있는 물음표 수만큼의 값을 포함하는 데이터프레임을 제공한다. 라이브러리는 순서대로 값과 물음표를 매칭하고, 값의 특수 문자를 이스케이프된 대응물로 변환하여 사용하기 안전하게 만든다.\n\n10.3.1 R 도우미 함수\nR의 데이터베이스 인터페이스 패키지들(예: RSQLite)은 데이터베이스를 탐색하고 전체 테이블을 한 번에 읽거나 쓰는 데 유용한 공통된 도우미 함수(helper function)들을 공유한다.\n데이터베이스의 모든 테이블을 보기 위해서는 dbListTables()를 사용한다.\n```{r}\nconnection &lt;- dbConnect(SQLite(), \"survey.db\")\ndbListTables(connection)\n```\n\"Person\"  \"Site\"    \"Survey\"  \"Visited\"\n테이블 모든 칼럼 이름을 보려면 dbListFields()를 사용한다.\n```{r}\ndbListFields(connection, \"Survey\")\n```\n\"taken\"   \"person\"  \"quant\"   \"reading\"\n전체 테이블을 데이터프레임으로 읽으려면 dbReadTable()을 사용한다.\n```{r}\ndbReadTable(connection, \"Person\")\n```\n        id  personal   family\n1     dyer   William     Dyer\n2       pb     Frank  Pabodie\n3     lake  Anderson     Lake\n4      roe Valentina  Roerich\n5 danforth     Frank Danforth\n데이터베이스에 전체 테이블을 쓰기 위해 dbWriteTable()을 사용한다. R이 행 이름을 별도 열로 쓰는 것을 방지하고자 할 경우, row.names = FALSE 인수를 설정한다. 예시에서 R에 내장된 iris 데이터셋을 survey.db 데이터베이스에 테이블로 쓰는 방법은 다음과 같다.\n```{r}\ndbWriteTable(connection, \"iris\", iris, row.names = FALSE)\nhead(dbReadTable(connection, \"iris\"))\n```\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n그리고 항상 그렇듯이 완료되면 데이터베이스 연결을 닫는 것을 잊지 말자.\n```{r}\ndbDisconnect(connection)\n```",
    "crumbs": [
      "SQL 쿼리",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>데이터베이스 프로그래밍</span>"
    ]
  },
  {
    "objectID": "prog.html#연습문제-1",
    "href": "prog.html#연습문제-1",
    "title": "10  데이터베이스 프로그래밍",
    "section": "연습문제",
    "text": "연습문제\n\n테이블 채우기 vs. 값 출력하기\nPressure라는 테이블 하나로 구성된 original.db라는 파일에 새로운 데이터베이스를 생성하는 R 프로그램을 작성해보자. 테이블에는 reading이라는 단일 필드가 있으며, 10.0에서 25.0 사이의 100,000개의 무작위 숫자를 삽입한다. 이 프로그램을 실행하는 데 얼마나 걸리는가? 단순히 이 무작위 숫자를 파일에 쓰는 프로그램을 실행하는 데는 얼마나 걸리는가?\n\n\nSQL 필터링 vs. R 필터링\noriginal.db와 동일한 구조를 가진 새 데이터베이스 backup.db를 생성하는 R 프로그램을 작성하고, original.db에서 20.0보다 큰 모든 값을 backup.db로 복사한다. 어느 것이 더 빠른가? 쿼리에서 값을 필터링하는 것, 아니면 모든 것을 메모리에 읽어 R에서 필터링하는 것.",
    "crumbs": [
      "SQL 쿼리",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>데이터베이스 프로그래밍</span>"
    ]
  },
  {
    "objectID": "calc-agg.html#합집합",
    "href": "calc-agg.html#합집합",
    "title": "6  새로운 필드와 집계",
    "section": "6.1 합집합",
    "text": "6.1 합집합\nUNION 연산자는 두 개의 쿼리 결과를 결합한다.\nSELECT * FROM Person WHERE id = 'dyer' UNION SELECT * FROM Person WHERE id = 'roe';\n\n\n\nid\npersonal\nfamily\n\n\n\n\ndyer\nWilliam\nDyer\n\n\nroe\nValentina\nRoerich\n\n\n\nUNION ALL 명령은 UNION 연산자와 동일하지만, UNION ALL은 모든 값을 선택한다는 점에서 차이가 있다. 차이점은 UNION ALL이 중복 행을 제거하지 않는다는 것이다. 대신, UNION ALL은 쿼리의 모든 행을 가져와서 하나의 테이블로 결합한다. UNION 명령은 결과 세트에 대해 SELECT DISTINCT를 수행한다. 만약 합병할 모든 레코드가 고유하다면, DISTINCT 단계를 건너뛰므로 더 빠른 결과를 얻기 위해 UNION ALL을 사용한다.",
    "crumbs": [
      "SQL 쿼리",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>새로운 필드와 집계</span>"
    ]
  },
  {
    "objectID": "calc-agg.html#집계",
    "href": "calc-agg.html#집계",
    "title": "6  새로운 필드와 집계",
    "section": "6.2 집계",
    "text": "6.2 집계\n이제 데이터의 평균과 범위를 계산하고자 한다. Visited 테이블에서 모든 날짜 정보를 어떻게 선택하는지 알고 있다.\nSELECT dated FROM Visited;\n\n\n\ndated\n\n\n\n\n1927-02-08\n\n\n1927-02-10\n\n\n1930-01-07\n\n\n1930-01-12\n\n\n1930-02-26\n\n\n-null-\n\n\n1932-01-14\n\n\n1932-03-22\n\n\n\n하지만 조합하기 위해서는 min 혹은 max 같은 집계 함수(aggregation function)를 사용해야만 한다. 각 함수는 입력으로 레코드 집합을 받고 출력으로 단일 레코드를 만든다.\nSELECT min(dated) FROM Visited;\n\n\n\nmin(dated)\n\n\n\n\n1927-02-08\n\n\n\n\n\n\n\n\nSELECT max(dated) FROM Visited;\n\n\n\nmax(dated)\n\n\n\n\n1932-03-22\n\n\n\nmin과 max는 SQL에 내장된 단지 두개의 집계 함수다. 다른 함수로 많이 사용되는 avg, count, sum을 들 수 있다.\nSELECT avg(reading) FROM Survey WHERE quant = 'sal';\n\n\n\navg(reading)\n\n\n\n\n7.20333333333333\n\n\n\nSELECT count(reading) FROM Survey WHERE quant = 'sal';\n\n\n\ncount(reading)\n\n\n\n\n9\n\n\n\nSELECT sum(reading) FROM Survey WHERE quant = 'sal';\n\n\n\nsum(reading)\n\n\n\n\n64.83\n\n\n\n여기서 count(reading)을 사용했다. 테이블에서 quant나 다른 필드를 세는 것도 마찬가지로 쉽게 할 수 있다. 심지어 count(*)를 사용할 수도 있다. 이 함수는 값 자체에 대해서는 관심이 없고, 값이 몇 개 있는지만을 고려하기 때문이다.\nSQL에서 한 번에 여러 집계작업을 수행할 수도 있다. 예를 들어, 적절한 염분측정치의 범위를 찾을 수도 있다.\nSELECT min(reading), max(reading) FROM Survey WHERE quant = 'sal' AND reading &lt;= 1.0;\n\n\n\nmin(reading)\nmax(reading)\n\n\n\n\n0.05\n0.21\n\n\n\n출력결과가 놀라움을 줄 수도 있지만, 원 결과값과 집계 결과를 조합할 수도 있다.\nSELECT person, count(*) FROM Survey WHERE quant = 'sal' AND reading &lt;= 1.0;\n\n\n\nperson\ncount(*)\n\n\n\n\nlake\n7\n\n\n\nLake의 이름이 Roerich나 Dyer 대신 나타나는 이유는 무엇일까? 이는 데이터베이스 관리자가 특정 필드를 집계해야 하지만, 집계 방법에 대한 지시가 없을 때 실제 입력 세트에서 하나의 값을 선택하기 때문이다. 이는 처리된 첫 번째 값일 수도 있고, 마지막 값이나 전혀 다른 어떤 값일 수도 있다.\n이 쿼리의 경우, person 필드가 그룹화되지 않았기 때문에, 데이터베이스 관리자는 person 칼럼에 대해 어떤 특정 값을 선택한다. 그 결과로 ’lake’가 출력된 것일 수 있는데, 이는 ’lake’가 데이터베이스에서 처리된 첫 번째 또는 마지막 값일 가능성이 있다. 데이터베이스의 내부 작동 방식에 따라, 이러한 선택은 일관성이 없을 수도 있다.\n또 다른 중요한 사실은 집계할 값이 없을 때 — 예를 들어, WHERE 절을 만족하는 행이 없는 경우 — 집계의 결과는 “모른다”로 처리되며, 이는 0이나 다른 임의의 값이 아니라는 점이다.\n예를 들어, 특정 조건을 만족하는 행이 하나도 없다면, count() 함수와 같은 집계 함수는 0을 반환하지만, sum()이나 avg()와 같은 다른 집계 함수들은 NULL을 반환할 수 있다. 이는 해당 집계 함수가 적용될 데이터가 전혀 없기 때문에, 그 결과가 “알 수 없는 값”이 됨을 의미한다. 이러한 방식은 데이터의 부재가 잘못된 결과로 이어지는 것을 방지하는 데 도움이 된다.\n\n\n\nperson\nmax(reading)\nsum(reading)\n\n\n\n\n-null-\n-null-\n-null-\n\n\n\n집계 함수의 또 다른 중요한 특징은 SQL 나머지 부분과는 매우 유용한 방식으로 일관성이 없다는 것이다. 이러한 동작은 다음과 같은 쿼리를 작성할 수 있게 해준다:\n집합 함수의 마지막 중요한 한가지 기능은 매우 유용한 방식으로 나머지 SQL과는 일관되지 않다는 점이다. 두 값을 더할 때 하나가 null이면 결과는 null이 된다. 이를 확장하면, sum을 사용하여 집합의 모든 값을 더하고 그 값들 중 하나라도 null이라면 결과도 null이 되어야 한다. 그러나 집계 함수가 null 값을 무시하고 null이 아닌 값들만 결합하는 것이 훨씬 더 유용하다. 명시적으로 항상 필터해야하는 대신에 이것의 결과 쿼리를 다음과 같이 작성할 수 있게 한다.\nSELECT min(dated) FROM Visited;\n\n\n\nmin(dated)\n\n\n\n\n1927-02-08\n\n\n\n명시적으로 항상 다음과 같이 필터하는 쿼리를 작성할 필요가 없다.\nSELECT min(dated) FROM Visited WHERE dated IS NOT NULL;\n\n\n\nmin(dated)\n\n\n\n\n1927-02-08\n\n\n\n모든 레코드를 한꺼번에 집계하는 것이 항상 의미 있는 것은 아니다. 예를 들어, 데이터에 체계적인 편향(bias)이 있을 것이라고 의심하고, 일부 과학자들의 방사능 측정값이 다른 사람들보다 높다고 생각한다고 가정해보자. 이런 경우에 다음과 같은 방법은 효과적이지 않다:\nSELECT person, count(reading), round(avg(reading), 2)\nFROM  Survey\nWHERE quant = 'rad';\n\n\n\nperson\ncount(reading)\nround(avg(reading), 2)\n\n\n\n\nroe\n8\n6.56\n\n\n\n데이터베이스 관리자가 각 과학자별로 별도로 집계하는 대신 임의로 한 과학자의 이름을 선택하기 때문에 이 방법은 효과적이지 않다. 과학자가 단지 다섯 명뿐이므로, 다음 형식의 다섯 개의 쿼리를 작성할 수 있다.\nSELECT person, count(reading), round(avg(reading), 2)\nFROM  Survey\nWHERE quant = 'rad'\nAND   person = 'dyer';\n\n\n\nperson\ncount(reading)\nround(avg(reading), 2)\n\n\n\n\ndyer\n2\n8.81\n\n\n\n하지만, 이러한 접근법은 성가시고, 만약 50명 혹은 500명의 과학자를 가진 데이터셋을 분석한다면, 모든 쿼리를 올바르게 작성할 가능성은 작다.\n필요한 것은 데이터베이스 관리자에게 각 과학자별로 시간을 별도로 집계하도록 GROUP BY 절을 사용하여 지시하는 것이다.\nSELECT   person, count(reading), round(avg(reading), 2)\nFROM     Survey\nWHERE    quant = 'rad'\nGROUP BY person;\n\n\n\nperson\ncount(reading)\nround(avg(reading), 2)\n\n\n\n\ndyer\n2\n8.81\n\n\nlake\n2\n1.82\n\n\npb\n3\n6.66\n\n\nroe\n1\n11.25\n\n\n\nGROUP BY는 그 이름이 암시하는 것처럼 정확히 다음과 같이 동작한다. 지정된 필드의 같은 값을 가진 모든 레코드를 그룹화하여 집계가 각 배치를 별도로 처리할 수 있도록 한다. 각 배치에 모든 레코드는 person에 대해 동일한 값을 가지기 때문에, 데이터베이스 관리자가 임의의 값을 잡아서 집합된 reading 값과 함께 표시하는 것은 더 이상 문제가 되지 않는다.\n우리가 한 번에 여러 기준에 따라 정렬할 수 있는 것처럼, 다중 기준에 따라 그룹화할 수도 있다. 예를 들어, 과학자별 및 측정된 양별로 평균 측정값을 얻기 위해서 GROUP BY 절에 다른 필드를 추가하기만 하면 된다.\nSELECT   person, quant, count(reading), round(avg(reading), 2)\nFROM     Survey\nGROUP BY person, quant;\n\n\n\nperson\nquant\ncount(reading)\nround(avg(reading), 2)\n\n\n\n\n-null-\nsal\n1\n0.06\n\n\n-null-\ntemp\n1\n-26.0\n\n\ndyer\nrad\n2\n8.81\n\n\ndyer\nsal\n2\n0.11\n\n\nlake\nrad\n2\n1.82\n\n\nlake\nsal\n4\n0.11\n\n\nlake\ntemp\n1\n-16.0\n\n\npb\nrad\n3\n6.66\n\n\npb\ntemp\n2\n-20.0\n\n\nroe\nrad\n1\n11.25\n\n\nroe\nsal\n2\n32.05\n\n\n\nquant를 표시되는 필드 목록에 추가한 것에 주목하자. 그렇지 않으면 결과가 큰 의미를 가지지 않을 것이다.\n한 단계 더 나아가 측정을 수행한 사람을 알 수 없는 모든 항목을 제거해보자.\nSELECT   person, quant, count(reading), round(avg(reading), 2)\nFROM     Survey\nWHERE    person IS NOT NULL\nGROUP BY person, quant\nORDER BY person, quant;\n\n\n\nperson\nquant\ncount(reading)\nround(avg(reading), 2)\n\n\n\n\ndyer\nrad\n2\n8.81\n\n\ndyer\nsal\n2\n0.11\n\n\nlake\nrad\n2\n1.82\n\n\nlake\nsal\n4\n0.11\n\n\nlake\ntemp\n1\n-16.0\n\n\npb\nrad\n3\n6.66\n\n\npb\ntemp\n2\n-20.0\n\n\nroe\nrad\n1\n11.25\n\n\nroe\nsal\n2\n32.05\n\n\n\n좀더 면밀하게 살펴보면, 이 쿼리는,\n\nSurvey테이블에서 person 필드가 null이 아닌 레코드를 선택한다.\n상기 레코드를 부분집합으로 그룹지어서 각 부분집합의 person과 quant의 값은 같다.\n먼저 person으로 부분집합을 정렬하고나서 quant로 각 하위 그룹내에서도 정렬한다.\n각 부분집합의 레코드 숫자를 세고, 각각 reading 평균을 계산하고, 각각 person과 quant 값을 선택한다. (모두 동등하기 때문에 어느 것인지는 문제가 되지 않는다.)",
    "crumbs": [
      "SQL 쿼리",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>새로운 필드와 집계</span>"
    ]
  },
  {
    "objectID": "calc-agg.html#연습문제",
    "href": "calc-agg.html#연습문제",
    "title": "6  새로운 필드와 집계",
    "section": "6.3 연습문제",
    "text": "6.3 연습문제\n\n6.3.1 염도 측정치 수정\n추가로 정보를 살펴본 결과, 발렌티나 로에리히(Valentina Roerich)가 염도를 백분율로 보고했다는 것을 알게 되었다. Survey 테이블에서 모든 염도 측정치를 100으로 나눈 값으로 반환하는 쿼리를 작성하시오.\nSELECT taken, reading / 100 FROM Survey WHERE person = 'roe' AND quant = 'sal';\n\n\n\ntaken\nreading / 100\n\n\n\n\n752\n0.416\n\n\n837\n0.225\n\n\n\n\n\n6.3.2 통합 측정목록\nUNION을 사용하여 발렌티나 로에리히(Roerich가)의 염도 측정치를 앞선 도전과제에서 설명한 대로 수정하고, 발렌티나 로에리히만의 측정치로 통합된 염도 측정치 목록을 만든다. 출력 결과는 다음과 같아야 한다.\n\n\n\ntaken\nreading\n\n\n\n\n619\n0.13\n\n\n622\n0.09\n\n\n734\n0.05\n\n\n751\n0.1\n\n\n752\n0.09\n\n\n752\n0.416\n\n\n837\n0.21\n\n\n837\n0.225\n\n\n\nSELECT taken, reading FROM Survey WHERE person != 'roe' AND quant = 'sal' UNION SELECT taken, reading / 100 FROM Survey WHERE person = 'roe' AND quant = 'sal' ORDER BY taken ASC;\n\n\n6.3.3 주요 사이트 식별자\nVisited 테이블의 사이트 식별자는 ’-’로 구분된 두 부분으로 이루어져 있다.\nSELECT DISTINCT site FROM Visited;\n\n\n\nsite\n\n\n\n\nDR-1\n\n\nDR-3\n\n\nMSK-4\n\n\n\n일부 주요 사이트 식별자(즉, 문자 코드)는 두 글자 길이이고 일부는 세 글자 길이이다. “문자열 내” 함수인 instr(X, Y)는 문자열 X 내에서 문자열 Y가 처음 나타나는 1-기반 인덱스를 반환하며, X 안에 Y가 존재하지 않으면 0을 반환한다. 부분 문자열 함수 substr(X, I, [L])는 X의 I 인덱스에서 시작하는 부분 문자열을 반환하며, 선택적으로 길이 L을 지정할 수 있다. 이 두 함수를 사용하여 고유한 주요 사이트 식별자 목록을 생성한다. (이 데이터의 경우, 목록은 “DR”과 “MSK”만을 포함해야 한다).\nSELECT DISTINCT substr(site, 1, instr(site, '-') - 1) AS MajorSite FROM Visited;\n\n\n6.3.4 온도 측정횟수 세기\n프랭크 파보디(Frank Pabodie)가 기록한 온도 측정횟수는 몇 번이며, 그 평균 값은 얼마인가?\nSELECT count(reading), avg(reading) FROM Survey WHERE quant = 'temp' AND person = 'pb';\n\n\n\ncount(reading)\navg(reading)\n\n\n\n\n2\n-20.0\n\n\n\n\n\n6.3.5 NULL 포함 평균 계산\n집합 값의 평균은 값들의 합을 값들의 개수로 나눈 것이다. 이는 avg 함수가 1.0, null, 5.0이라는 값들이 주어졌을 때 2.0 또는 3.0을 반환한다는 것을 의미하는가?\n\n정답은 3.0이다. NULL은 값이 아니라 값이 없음을 나타낸다. 따라서 계산에 포함되지 않는다. SQL avg 함수는 null 값을 무시하고, null이 아닌 값들만을 사용하여 평균을 계산한다. 따라서 주어진 값이 1.0, null, 5.0일 때, avg 함수는 null을 제외한 1.0과 5.0의 평균을 계산한다. 이는 (1.0 + 5.0) / 2 = 3.0 이므로, 함수는 3.0을 반환한다.\n다음 코드를 실행하여 이를 확인할 수 있다:\nSELECT AVG(a) FROM (\n    SELECT 1 AS a\n    UNION ALL SELECT NULL\n    UNION ALL SELECT 5);\n\n\n6.3.6 쿼리가 의미하는 바는?\n각 개별 방사능 측정값과 모든 방사능 측정값의 평균 사이의 차이를 계산하고자 한다. 이를 위해 다음과 같은 쿼리를 작성했다.\nSELECT avg(reading) FROM Survey WHERE quant='rad';\n쿼리가 실제로 어떤 결과를 생성하며, 그 이유는 무엇일까?\n\n쿼리는 각 측정값에 대한 결과 대신 단 하나의 결과 행만을 생성한다. avg() 함수는 단일 값을 생성하며, 먼저 실행되기 때문에 테이블이 단일 행으로 축소된다. reading 값은 단순히 임의의 값일 뿐이다.\n원하는 결과를 얻기 위해서는 두 개의 쿼리를 실행해야 한다:\nSELECT avg(reading) FROM Survey WHERE quant='rad';\n이는 평균값(6.5625)을 생성하는데, 이 값을 다음과 같은 두 번째 쿼리에 삽입할 수 있다.\nSELECT reading - 6.5625 FROM Survey WHERE quant = 'rad';\n이 쿼리는 우리가 원하는 결과를 생성하지만, 하위 쿼리(subquery)를 사용하여 이를 단일 쿼리로 결합할 수 있다.\nSELECT reading - (SELECT avg(reading) FROM Survey WHERE quant='rad') FROM Survey WHERE quant = 'rad';\n이 방법을 사용하면 두 개의 쿼리를 실행할 필요가 없다.\n요약하자면, 원래 쿼리에서 avg(reading)을 (SELECT avg(reading) FROM Survey WHERE quant='rad')로 대체한 것이다.\n\n\n6.3.7 group_concat 함수 사용\ngroup_concat(field, separator) 함수는 지정된 구분자 문자(또는 구분자가 지정되지 않은 경우 ‘,’)를 사용하여 필드의 모든 값을 연결한다. 이를 사용하여 과학자들의 이름을 한 줄 목록으로 생성하면 출력결과는 다음과 같다.\nWilliam Dyer, Frank Pabodie, Anderson Lake, Valentina Roerich, Frank Danforth\n쉼표로 구분된 모든 과학자들의 성을 나열하는 쿼리를 작성하세요. 쉼표로 구분된 모든 과학자들의 개인 이름과 성을 나열하는 쿼리를 작성하세요.\n\n쉼표로 구분된 모든 성을 나열하는 쿼리는 다음과 같다.\nSELECT group_concat(family, ',') FROM Person;\n쉼표로 구분된 모든 전체 이름을 나열하는 쿼리는 다음과 같다.\nSELECT group_concat(personal || ' ' || family, ',') FROM Person;",
    "crumbs": [
      "SQL 쿼리",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>새로운 필드와 집계</span>"
    ]
  },
  {
    "objectID": "spreadsheet.html#스프레드쉬트-복잡성",
    "href": "spreadsheet.html#스프레드쉬트-복잡성",
    "title": "11  엑셀의 역설",
    "section": "11.3 스프레드쉬트 복잡성",
    "text": "11.3 스프레드쉬트 복잡성\n데이터 분석을 위해, 사람과 컴퓨터 모두 읽을 수 있는 프로그램들이 많이 존재한다. 컴퓨터 프로그램을 열어보면, 컴퓨터가 실행하는 코드와 함께, 컴퓨터에서는 무시되지만 사람에게 매우 중요한 주석이 포함되어 있다. 따라서 컴퓨터와 사람 모두가 읽을 수 있는 코드는 데이터 과학에 필수적이다.\n코드는 사람과 컴퓨터 모두에게 가독성을 제공해야 한다. 그러나 데이터 역시 사람과 컴퓨터 모두에게 가독성을 제공해야 한다는 것이 문제다. 특히 스프레드시트는 사람과 컴퓨터 양쪽 모두에게 가독성을 제공하지 않는 경우가 많다. 스프레드시트는 처음에는 쉽게 접근할 수 있지만, 1주일 지난 후 해독하기 어려운 복잡성을 종종 경험하게 된다.\n“ALGORITHMS BY COMPLEXITY”(그림 11.1) 제목 아래에 다양한 알고리즘과 시스템이 복잡도에 따라 나열되어 있으며, 왼쪽에서 오른쪽으로 갈수록 복잡도가 증가한다. 가장 왼쪽에는 “LEFTPAD”와 “QUICKSORT”가 있고, 그 오른쪽에는 “GIT MERGE”가 있다. 그 다음에는 “SELF-DRIVING CAR”과 “GOOGLE SEARCH BACKEND”가 위치하고 있으며 오른쪽 끝에 “SPRAWLING EXCEL SPREADSHEET BUILT UP OVER 20 YEARS BY A CHURCH GROUP IN NEBRASKA TO COORDINATE THEIR SCHEDULING”이라는 문구가 있고, 20년 동안 네브래스카의 한 교회 그룹이 그들의 스케줄링을 조정하기 위해 만들어온 방대한 엑셀 스프레드시트 복잡성을 강조하고 있다.\n\n\n\n\n\n\n그림 11.1: 엑셀 알고리즘 복잡성\n\n\n\n스프레드쉬트는 데이터, 서식, 수식으로 구성된다. 숫자 데이터를 엑셀로 가져오게 되면 엑셀 내장 함수를 통해 수식 계산을 수행하고, 엑셀 사용자 본인 혹은 외부 사람을 위해 서식을 입히는 과정을 거쳐 비로소 완성된 스프레드쉬트가 된다.\n\n\n\n\n\n\n그림 11.2: 엑셀 구성요소\n\n\n\n\n\n\n\n표 11.3: 데이터, 서식, 수식 엑셀 구성요소별 R/파이썬 비교\n\n\n\n\n\n\n\n\n\n스프레드쉬트와 R/파이썬 비교\n\n\n스프레드쉬트\nR\nPython\n\n\n\n\n데이터 저장 및 관리\n데이터프레임, tibble\n판다스, 넘파이\n\n\n데이터 처리 및 변환\ndplyr, tidyr\npandas, NumPy\n\n\n고급 수식 및 계산\ndplyr\nNumPy, SciPy\n\n\n데이터 시각화\nggplot2, plotly\nmatplotlib, seaborn, plotly\n\n\n인터랙티브 대시보드\nShiny 앱\nDash, Streamlit apps\n\n\n복잡한 데이터 분석\nRMarkdown/Pandoc\n쥬피터 노트북, IPython\n\n\n\n\n\n\n\n\n\n\n스프레드쉬트과 R/파이썬이 비교된 표 11.3 를 보면, 기능이 일대일로 대응되는 것을 확인할 수 있다.",
    "crumbs": [
      "활용사례",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>엑셀의 한계</span>"
    ]
  },
  {
    "objectID": "spreadsheet.html#실무-사례",
    "href": "spreadsheet.html#실무-사례",
    "title": "11  엑셀의 역설",
    "section": "11.4 실무 사례",
    "text": "11.4 실무 사례\n데이터 복잡성이 일반적으로 처리할 수 있는 것을 넘어서면 “추출(Extract), 변환(Transform), 로드(Load)”라고 하는 과정을 수행해야 한다. 예를 들어 서지학 데이터에서 다음과 같은 질문에 답을 하고자 한다고 가정해 보자.\n\n각 사람이 기여한 논문의 수는 얼마인가?\n누가 누구와 협업하는가?\n\n불행히도, “다중 값 필드(multi-valued field)”가 있는 필드 때문에 바로 스프레드시트/CSV 형식 서지학 데이터를 데이터베이스에 넣을 수 없다. 이러한 상황에서는 데이터를 먼저 적절한 형식으로 변환하는 작업이 필요하다.\n저자 한명에만 관심이 있다면, 첫번째 질문에 답하기 위해 스프레드시트에서 저자명을 검색한 다음, 그 행을 선택하고 수동으로 그녀의 공동 저자를 집계하여 두 번째 질문에 답할 수 있다. 그러나 모든 저자에 대해서 동일한 작업을 한땀한땀 수행하는 데는 며칠이 걸릴 것이며, 거의 확실하게 실수(휴먼 에러)가 있을 것이며, 그러면 누군가가 또 다른 더 큰 스프레드시트를 건네주고 처음부터 다시 시작해야 할 것이다. 하지만, 모든 저자에 대해 하나씩 이런 작업을 수행하는 것은 몇일이 소요된다. 거의 확실히 실수도 할 것이다.\n두가지 질문에 답하기 위해 많은 작업처럼 보일 수 있지만, 수십줄 이상되는 데이터에 대해서는 많은 시간을 절약할 수 있다.\n\n데이터가 데이터베이스에 존재한다면 다른 질문들도 묻고 답하기 쉬워진다.\n향후 또다른 형태 스프레드시트에 개발한 도구를 재사용할 수 있다.\n지금까지 수행한 일에 대한 기록을 가질 수 있다(스프레드시트에서 클릭하는 것으로는 얻을 수 없는 것).\n정확할 가능성이 훨씬 더 높고 빠르다.\n\n이 접근 방식을 통해 데이터를 보다 체계적이고 효율적으로 관리할 수 있으며, 데이터 분석을 위한 기반을 마련할 수 있다. 데이터베이스에 데이터를 저장함으로써, 데이터의 일관성을 유지하고, 복잡한 쿼리를 쉽게 실행할 수 있으며, 나중에 데이터를 검토하거나 업데이트할 때 시간과 노력을 절약할 수 있다. 전체적인 작업흐름은 다음과 같다.\n\n모든 논문에 모든 기여자에 대한 (키값, 저자명) 짝을 출력하는 작은 파이썬 프로그램을 작성한다. 예를 들어, 작성한 프로그램이 스프레드쉬트 첫 세줄을 다음과 같이 변환한다:\n\n8SW85SQM McClelland, James L\n85QV9X5F McClelland, J. L.\n85QV9X5F McNaughton, B. L.\n85QV9X5F O'Reilly, R. C.\nZ4X6DT6N Ratcliff, R.\n\n프로그램을 변경해서 데이터베이스에 키값과 저자를 삽입하는 SQL insert 문장을 생성한다.\nSQL 쿼리를 사용해서 최초 질문에 답한다.\n\n\n\n\n\n\n\n바흐라이(Bahlai) 법칙\n\n\n\n“다른 사람의 데이터는 항상 일관성이 없고 잘못된 형식으로 되어 있다. (”Other people’s data is always inconsistent and in the wrong format.”)\n\n\n다음 간단한 예제를 통해서 데이터(스프레드쉬트에 내장된 참고문헌정보)를 어떻게 받아서 유용한 것으로 변경할지 살펴보자. 출발점은 다음과 같은 2,937행을 갖는 bibliography.csv 라는 스프레드쉬트(엑셀) 파일이다.\n\n\n\n\n표 11.4: 참고문헌정보 .csv 파일\n\n\n\n\n\n\n\n\n\n정형 참고문헌정보 일부\n\n\nkey\ntype\nyear\nauthors\ntitle\njournal\n\n\n\n\n8SW85SQM\njournalArticle\n2013\nMcClelland, James L\nIncorporating Rapid Neocortical Learning of New Schema-Consistent Information Into Complementary Learning Systems Theory.\nJ Exp Psychol Gen\n\n\n85QV9X5F\njournalArticle\n1995\nMcClelland, J. L.; McNaughton, B. L.; O'Reilly, R. C.\nWhy There are Complementary Learning Systems in the Hippocampus and Neocortex: Insights from the Successes and Failures of Connectionist Models of Learning and Memory\nPsychological Review\n\n\nZ4X6DT6N\njournalArticle\n1990\nRatcliff, R.\nConnectionist models of recognition memory: constraints imposed by learning and forgetting functions.\nPsychological review\n\n\n\n\n\n\n\n\n\n\n본격적인 개발에 들어가기 전에 프로그램을 개발하는 것이 시간을 얼마나 절약할 수 있고 정확도를 높이는지 살펴보자.\n\n11.4.1 확률이 얼마나 될까?\n스프레드쉬트는 2,937행을 담고 있다. 전체 분석작업의 99%를 틀리지 않게 하는데, 손으로하는 전사작업은 얼마나 정확성이 있을까? 즉, 행당 오류율이 얼마나 되어야 전체 작업을 올바르게 완수하는데 0.99 확률이 될까?\n\n\n# 전체 행의 수\ntotal_rows &lt;- 2937\n\n# 전체 작업의 99%가 정확해야 함\ndesired_accuracy &lt;- 0.99\n\n# 행당 오류율을 찾기\n# 정확도 = (1 - 오류율) ^ 전체 행의 수\n# 따라서 오류율 = 1 - (정확도의 1/전체 행의 수제곱)\nrow_error_rate &lt;- 1 - (desired_accuracy ^ (1/total_rows))\nscales::percent(row_error_rate, accuracy = 0.000001)\n#&gt; [1] \"0.000342%\"\n\n\"0.000342%\"\n\n\n11.4.2 손익분기점\n수작업으로 5분만 소요되는 작업을 (전산화해서) 10분 걸려 프로그램 작성한다면, 해당 작업을 두번 이상 수행한다면, 프로그램으로 작성할 가치가 있다. 유사하게, 특정한 저자와 공저자가 누구인지만 알아내려고 하고, 다른 질문은 전혀 없을 것이거나, 반복작업을 할 필요가 없다면, 수작업으로 스프레드쉬트를 검색하는 것이 데이터를 데이터베이스로 옮기는 프로그램을 작성하는 것보다 아마도 더 빠를 것이다.\n현재 수작업으로 하고 있는 작업을 선택하라. 매번 얼마의 시간이 소요되고, 얼마나 자주 수행하는지 추정하고, 대신에 작업을 프로그램으로 만드는데 얼마나 소요되는지 추정하라. 프로그래밍이 실질적으로 얼마나 시간을 절약해줄까? 얼마나 확신이 되나요?\n\n이 문제를 해결하기 위해, 현재 수작업으로 진행 중인 작업을 선정하고, 그 작업에 대한 다음 정보들을 추정해야 한다.\n\n작업에 소요되는 시간: 각 작업 수행에 걸리는 평균 시간을 추정한다.\n작업의 빈도: 이 작업이 얼마나 자주 수행되는지 추정한다. 예를 들어, 일주일에 몇 번 또는 한 달에 몇번 등이 된다.\n프로그램 작성에 소요되는 시간: 동일한 작업을 자동화하는 프로그램을 작성하는 데 필요한 시간을 추정한다.\n\n상기 정보를 바탕으로 프로그래밍이 실질적으로 시간을 절약해주는지를 평가할 수 있다. 시간 절약의 계산은 다음과 같은 간단한 공식으로 이루어집니다:\n\n총 절약시간 = (수작업 시간 * 작업 빈도 * 기간) - 프로그램 작성 시간\n\n여기서, “기간”은 프로그램이 사용될 예상 기간이 된다.\n예를 들어, 매주 2시간 걸리는 작업이 있고, 이를 자동화하는 프로그램을 작성하는 데 10시간이 걸린다고 가정해 보자. 프로그램이 1년 동안 사용될 것이라고 가정하면, 총 절약 시간은 다음과 같습니다:\n\n총 절약시간 = (2시간/주 * 52주) - 10시간 = 94시간\n\n프로그램 작성에 들인 시간을 고려하더라도 연간 94시간을 절약할 수 있음을 의미한다.\n유념할 점은 이러한 추정은 작업의 복잡성, 작업 빈도 및 프로그래밍 능력에 따라 달라질 수 있으므로, 여러가지 요소들을 고려하여 신중하게 추정해야 한다.\n\n\n\n\nHermans, Felienne, 와/과 Emerson Murphy-Hill. 2015. “Enron’s Spreadsheets and Related Emails: A Dataset and Analysis”. In 37th International Conference on Software Engineering, ICSE ’15.",
    "crumbs": [
      "활용사례",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>엑셀의 한계</span>"
    ]
  },
  {
    "objectID": "spreadsheet.html#엑셀-참사",
    "href": "spreadsheet.html#엑셀-참사",
    "title": "11  엑셀의 역설",
    "section": "11.2 엑셀 참사",
    "text": "11.2 엑셀 참사\n자율주행차, 로봇 등의 제어에는 C/C++ 언어가 많이 사용되고 있다. C/C++ 언어는 메모리를 직접 관리하는 저수준 언어로, 메모리 관리에 대한 이해도 많이 필요한데 제대로 작성되지 않은 코드는 운전자의 목숨을 위협할 수 있다. 마찬가지로 엑셀은 작은 데이터 분석과 계산에 최적화되어 있지만, 데이터베이스로 사용하는 것에는 제약이 있다. 많은 기업과 조직이 중요한 데이터를 엑셀 스프레드시트에 보관하고 작업하는데, 이는 사소한 실수로 중요한 의사결정이 왜곡될 위험이 있다.\n엑셀을 사용하는 주된 이유는 엑셀 사용 습관으로 데이터를 엑셀로 저장하고 분석하는 습관이 있으며, 데이터 내보내기가 쉽기 때문이다. 또한, 데이터가 작아서 데이터베이스가 필요 없다고 생각하지만, 사업이 커지고 업무량이 늘면서 엑셀의 작업량과 복잡성이 증가한다.\n엑셀 스프레드시트의 단점으로 다음이 많이 언급된다.\n\n한 번에 한 사람만 작업 가능: 다른 사람이 작업 중이면 읽기 전용으로만 접근 가능.\n데이터 감사의 부재: 한 사람이 주로 관리하므로, 그 사람이 떠나면 지식과 정보가 손실될 수 있다.\n정형화된 작업흐름 부족: 엑셀로 정의된 업무 프로세스는 수작업으로 취합과 정리가 필요하다.\n모형 지원 부족: 엑셀은 크기가 커지면 오류에 취약해진다.\n보고서 생성의 어려움: 데이터베이스에서는 쿼리를 통해 보고서를 더 쉽게 생성할 수 있다.\n보안과 규제의 어려움: 스프레드시트는 보안과 규제를 가하기 어렵다.\n\n여러 기업이 엑셀 사용을 중요한 업무에서 단순히 피하는 것을 넘어 전면 중단시킨 사례가 존재한다. 표 11.2에 2015년 전후까지 기록된 대표적인 스프레드시트 참사를 기록한 사례들인데, 최근 들어 이런 사례들은 크게 줄었지만 코로나 전후를 틈타 다시 엑셀 참사 사례가 다양하게 언론에 보도되고 있다.\n\n\n\n\n표 11.2: 대표적인 엑셀 참사 기록\n\n\n\n\n\n\n\n\n\n2015년 이전 역대 엑셀 참사 사례\n\n\n회사\n참사 비용\n발생일\n영향\n참사 개요\n\n\n\n\n옥스포드 대학\n미확인\n'11.12월\n학생 인터뷰 일정 지연\n엑셀이 수식이 꼬여 인터뷰 일정이 뒤죽박죽\n\n\nMI5\n미확인\n'11년\n잘못된 전화번호 작업\n엑셀 서식 수식이 꼬여 엉뚱한 전화번호 작업\n\n\n'12년 런던 올림픽\n£ 0.5백만\n'12.01월\n티켓 환불 소동\n수영장 10,000 티켓이 초과 판매 (엑셀 입력 오류)\n\n\nMouchel\n£ 4.3백만\n'10.11월\nCEO 사임, 주가폭락\n연금펀드평가 £ 4.3백만 엑셀 오류\n\n\nC&C Group\n£ 9 백만\n'09.7월\n주가 15% 하락 등\n매출 3% 상승이 아니고 5% 하락, 엑셀 오류\n\n\nUK 교통부\n최소 £ 50 백만\n'12.10월\n영국민 추가 세금 부담\n영국 철도 입찰 오류\n\n\nKing 펀드\n£ 130 백만\n'11.05월\n브래드 이미지 하락\n웨일즈 지방 NHS 지출 엑셀 오류\n\n\nAXA Rosenberg\n£ 150 백만\n'11.02월\n은폐, 벌금, 브래드 이미지 하락\n엑셀 오류를 감춰서 $242 백만 벌금\n\n\nJP Morgan Chase\n£ 250 백만\n'13.01월\n명성, 고객 신뢰도 저하\n바젤 II VaR 위험 평가 엑셀 오류\n\n\nFidelity Magellan 펀드\n£ 1.6 십억\n'95.01월\n투자자에게 약속한 배당금 지급 못함\n음수 부호 누락으로 자본이득 과대계상\n\n\n미연방준비위원회\n£ 2.5 십억\n'10.10월\n명확하지 않음\n리볼빙 카드 신용액 산출 과정에 엑셀 오류\n\n\n하버드 대학\n평가 불능\n'13.04월\n유럽 정부 긴축예산 편성 근거\nGDP 대비 정부 부채 영향도 분석 엑셀 오류\n\n\n\n출처: the dirty dozen 12 modelling horror stories & spreadsheet disasters\n\n\n\n\n\n\n\n\n\n\n\n반면, 데이터베이스는 사용자 활동 기록, 정형화된 작업흐름 지원, 오류 감소, 효율적인 보고서 생성 및 강력한 보안과 규제 기능을 제공한다. 따라서 엑셀 스프레드시트 대신 데이터베이스를 사용하는 것이 여러 면에서 이점을 제공한다.\n\n\n\n\n\n\n도널드 럼스펠트와 제니 브라이언\n\n\n\n“전쟁은 현재 가용한 군대와 함께 가야지, 나중에 원하거나 바라는 군대와 함께 가는 것은 아니다” – 도널드 럼스펠트\n“데이터 분석은 알고 있는 도구와 함께 시작하지, 필요한 도구와 함께 시작하는 것이 아니다” – 제니 브라이언",
    "crumbs": [
      "활용사례",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>엑셀의 한계</span>"
    ]
  },
  {
    "objectID": "spreadsheet.html#엔론사-엑셀-파일",
    "href": "spreadsheet.html#엔론사-엑셀-파일",
    "title": "11  엑셀의 역설",
    "section": "11.1 엔론사 엑셀 파일",
    "text": "11.1 엔론사 엑셀 파일\n회계부정으로 파산한 엔론(Enron)은 많은 유산(Hermans 와/과 Murphy-Hill 2015)을 남겼다. 유산중에 대기업에서 스프레드쉬트 엑셀을 어떻게 사용했는지에 대한 다양한 사례를 파악할 수 있다. 엑셀 코퍼스 분석 결과에 따르면, 엔론 스프레드시트 중 24%에서 엑셀 오류가 발견되었다.\n\n\n\n\n표 11.1: 회계부정 파산한 엔론사 스프레드시트 고빈도 함수\n\n\n\n\n\n\n\n\n\n엔론(Enron) 엑셀 빈도수 높은 함수\n\n\n순위\n함수\n스프레드쉬트 갯수\n누적 백분율(%)\n\n\n\n\n1\nSUM\n578\n6.4%\n\n\n2\n+\n1,259\n14.0%\n\n\n3\n-\n2,262\n25.1%\n\n\n4\n/\n2,625\n29.1%\n\n\n5\n*\n3,959\n43.9%\n\n\n6\nIF\n4,260\n47.3%\n\n\n7\nNOW\n5,322\n59.1%\n\n\n8\nAVERAGE\n5,664\n62.8%\n\n\n9\nVLOOKUP\n5,733\n63.6%\n\n\n10\nROUND\n5,990\n66.5%\n\n\n11\nTODAY\n6,182\n68.6%\n\n\n12\nSUBTOTAL\n6,480\n71.9%\n\n\n13\nMONTH\n6,520\n72.3%\n\n\n14\nCELL\n6,774\n75.2%\n\n\n15\nYEAR\n6,812\n75.6%\n\n\n\n출처: Enron's spreadsheets and related emails: A dataset and analysis\n\n\n\n\n\n\n\n\n\n\n\n표 11.1 에 사용된 함수들 중 핵심적인 15개의 함수가 전체 스프레드시트의 76%를 차지했고, 매일 100개의 스프레드시트가 이메일에 첨부되어 유통되었으며, 전체 전자주편 중 10%에서 스프레드시트가 첨부되거나 주제로 전달되었다.",
    "crumbs": [
      "활용사례",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>엑셀의 한계</span>"
    ]
  }
]